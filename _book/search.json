[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Comparing LLM and human reviews of social science research using data from Unjournal.org",
    "section": "",
    "text": "Introduction\nInclude global setup and parameters\nsource(\"setup_params.R\")\nIs AI good at peer-reviewing? Does it offer useful and valid feedback? Can it predict how human experts will rate research across a range of categories? How can it help academics do this “thankless” task better? Is it particularly good at spotting errors? Are there specific categories, e.g. spotting math errors or judging real-world relevance, where it does surprisingly well or poorly? How does its “research taste” compare to humans?\nIf AI research-evaluation works it could free up a lot of scientific resources – perhaps $1.5 billion/year in the US alone Aczel, Szaszi, and Holcombe (2021)) – and offer more continual and detailed review, helping improve research. It could also help characterize methodological strengths/weaknesses across papers, aiding training and research direction-setting. Furthermore, a key promise of AI is to directly improve science and research. Understanding how AI engages with research evaluations may provide a window into its values, abilities, and limitations.\nIn this project, we are testing the capabilities of current large language models (LLMs), illustrating whether they can generate research paper evaluations comparable to expert human reviews. The Unjournal systematically prioritizes ‘impactful’ research and pays for high-quality human evaluations, structured quantified ratings, claim identification and assessment, and predictions. In this project, we use an AI (OpenAI’s GPT-5 Pro model) to review social science research papers under the same criteria used by human reviewers for The Unjournal.\nEach paper is assessed on specific dimensions – for example, the strength of its evidence, rigor of methods, clarity of communication, openness/reproducibility, relevance to global priorities, and overall quality. The LLM will provide quantitative scores (with uncertainty intervals) on these criteria and produce a written evaluation\nOur initial dataset will include the 5 research papers that have existing Unjournal human evaluations. For each paper, the AI will generate: (1) numeric ratings on the defined criteria, (2) identification of the paper’s key claims, and (3) a detailed review discussing the paper’s contributions and weaknesses. We will then compare the AI-generated evaluations to the published human evaluations.\nIn the next phase, we will focus on papers currently under evaluation, i.e., where no human evaluation has been made public, to allow us to rule out any contamination.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Comparing LLM and human reviews of social science research using data from Unjournal.org",
    "section": "",
    "text": "Other work has relied on collections of research and grant reviews, including NLPEER, SubstanReview, and the Swiss National Science Foundation. That data has a heavy focus on computer-science adjacent fields, and iss less representative of mainstream research peer review practices in older, established academic fields. Note that The Unjournal commissions the evaluation of impactful research, often from high-prestige working paper archives like NBER, and makes all evaluations public, even if they are highly critical of the paper.↩︎\nAbout verifiable publication outcomes, not about the ‘true quality’ of the paper of course.↩︎\nOur approach differs from prior work by (i) focusing on structured, percentile-based quantitative ratings with credible intervals that map to decision-relevant dimensions used by The Unjournal; (ii) comparing those ratings to published human evaluations rather than using LLM-as-judge; and (iii) curating contamination-aware inputs (paper text extraction with reference-section removal and token caps), with a roadmap to add multi-modal checks when we score figure- or table-dependent criteria.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Data and methods",
    "section": "",
    "text": "Unjournal.org evaluations\nWe use The Unjournal’s public data for a baseline comparison. At The Unjournal each paper is typically evaluated (aka ‘reviewed’) by two expert evaluators1 who provide quantitative ratings on a 0–100 percentile scale for each of seven criteria (with 90% credible intervals),2 two “journal tier” ratings on a 0.0 - 5.0 scale,3 a written evaluation (resembling a referee report for a journal), and identification and assessment of the paper’s “main claim”. For our initial analysis, we extracted these human ratings and aggregated them, taking the average score per criterion across evaluators (and noting the range of individual scores).\nAll papers have completed The Unjournal’s evaluation process (meaning the authors received a full evaluation on the Unjournal platform, which has been publicly posted at unjournal.pubpub.org). The sample includes papers spanning 2017–2025 working papers in development economics, growth, health policy, environmental economics, and related fields that The Unjournal identified as high-impact. Each of these papers has quantitative scores from at least one human evaluator, and many have multiple (2-3) human ratings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and methods</span>"
    ]
  },
  {
    "objectID": "methods.html#llm-based-evaluation",
    "href": "methods.html#llm-based-evaluation",
    "title": "Data and methods",
    "section": "LLM-based evaluation",
    "text": "LLM-based evaluation\n\nQuantitative ratings and journal-ranking tiers\nFollowing The Unjournal’s standard guidelines for evaluators and their academic evaluation form, evaluators are asked to consider each paper along the following dimensions: claims & evidence, methods, logic & communication, open science, global relevance, and an overall assessment. Ratings are interpreted as percentiles relative to serious recent work in the same area. For each metric, evaluators are asked for the midpoint of their beliefs and their 90% credible interval, to communicate their uncertainty. For the journal rankings measure, we ask both “what journal ranking tier should this work be published in? (0.0-5.0)” and “what journal ranking tier will this work be published in? (0.0-5.0)”, with some further explanation.The full prompt can be seen in the code below – essentially copied from the Unjournal’s guidelines page.\n\nWe captured the versions of each paper that was evaluated by The Unjournal’s human evaluators, downloading from the links provided in The Unjournal’s Coda database. \nWe evaluate each paper by passing the PDF directly to the model and requiring a strict, machine‑readable JSON output. This keeps the assessment tied to the document the authors wrote. Direct ingestion preserves tables, figures, equations, and sectioning, which ad‑hoc text scraping can mangle. It also avoids silent trimming or segmentation choices that would bias what the model sees.\n\n\nLLM evaluation pipeline setup\nimport os, time, json, random, hashlib\nimport pathlib\nfrom typing import Any, Dict, Optional, Union\n\nimport pandas as pd\nimport numpy as np\n\nimport openai\nfrom openai import OpenAI\n\n# ---------- Configuration (in-file, no external deps)\nAPI_KEY_PATH = pathlib.Path(os.getenv(\"OPENAI_KEY_PATH\", \"key/openai_key.txt\"))\nMODEL        = os.getenv(\"OPENAI_MODEL\", \"gpt-5-pro\")\nFILE_PURPOSE = \"assistants\"  # for Responses API file inputs\nRESULTS_DIR  = pathlib.Path(\"results\")\nRESULTS_DIR.mkdir(exist_ok=True)\nFILE_CACHE   = RESULTS_DIR / \".file_cache.json\"\n\n# ---------- API key bootstrap\nif os.getenv(\"OPENAI_API_KEY\") is None and API_KEY_PATH.exists():\n    os.environ[\"OPENAI_API_KEY\"] = API_KEY_PATH.read_text().strip()\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"No API key. Set OPENAI_API_KEY or create key/openai_key.txt\")\n\nclient = OpenAI()\n\n# ---------- Small utilities (inlined replacements for llm_utils)\n\ndef _resp_as_dict(resp: Any) -&gt; Dict[str, Any]:\n    if isinstance(resp, dict):\n        return resp\n    for attr in (\"to_dict\", \"model_dump\", \"dict\", \"json\"):\n        if hasattr(resp, attr):\n            try:\n                val = getattr(resp, attr)()\n                if isinstance(val, (str, bytes)):\n                    try:\n                        return json.loads(val)\n                    except Exception:\n                        pass\n                if isinstance(val, dict):\n                    return val\n            except Exception:\n                pass\n    # last resort\n    try:\n        return json.loads(str(resp))\n    except Exception:\n        return {\"_raw\": str(resp)}\n\ndef _get_output_text(resp: Any) -&gt; str:\n    d = _resp_as_dict(resp)\n    if \"output_text\" in d and isinstance(d[\"output_text\"], str):\n        return d[\"output_text\"]\n    out = d.get(\"output\") or []\n    chunks = []\n    for item in out:\n        if not isinstance(item, dict): continue\n        if item.get(\"type\") == \"message\":\n            for c in item.get(\"content\") or []:\n                if isinstance(c, dict):\n                    if \"text\" in c and isinstance(c[\"text\"], str):\n                        chunks.append(c[\"text\"])\n                    elif \"output_text\" in c and isinstance(c[\"output_text\"], str):\n                        chunks.append(c[\"output_text\"])\n    # Also check legacy top-level choices-like structures\n    if not chunks:\n        for k in (\"content\", \"message\"):\n            v = d.get(k)\n            if isinstance(v, str):\n                chunks.append(v)\n    return \"\\n\".join(chunks).strip()\n\ndef _extract_json(s: str) -&gt; Dict[str, Any]:\n    \"\"\"Robustly extract first top-level JSON object from a string.\"\"\"\n    if not s:\n        raise ValueError(\"empty output text\")\n    # Fast path\n    s_stripped = s.strip()\n    if s_stripped.startswith(\"{\") and s_stripped.endswith(\"}\"):\n        return json.loads(s_stripped)\n\n    # Find first balanced {...} while respecting strings\n    start = s.find(\"{\")\n    if start == -1:\n        raise ValueError(\"no JSON object start found\")\n    i = start\n    depth = 0\n    in_str = False\n    esc = False\n    for i in range(start, len(s)):\n        ch = s[i]\n        if in_str:\n            if esc:\n                esc = False\n            elif ch == \"\\\\\":\n                esc = True\n            elif ch == '\"':\n                in_str = False\n        else:\n            if ch == '\"':\n                in_str = True\n            elif ch == \"{\":\n                depth += 1\n            elif ch == \"}\":\n                depth -= 1\n                if depth == 0:\n                    candidate = s[start:i+1]\n                    return json.loads(candidate)\n    raise ValueError(\"no balanced JSON object found\")\n\ndef call_with_retries(fn, max_tries: int = 6, base_delay: float = 0.8, max_delay: float = 8.0):\n    ex = None\n    for attempt in range(1, max_tries + 1):\n        try:\n            return fn()\n        except (openai.RateLimitError, openai.APIError, openai.APIConnectionError, openai.APITimeoutError, Exception) as e:\n            ex = e\n            sleep = min(max_delay, base_delay * (1.8 ** (attempt - 1))) * (1 + 0.25 * random.random())\n            time.sleep(sleep)\n    raise ex\n\ndef _load_cache() -&gt; Dict[str, Any]:\n    if FILE_CACHE.exists():\n        try:\n            return json.loads(FILE_CACHE.read_text())\n        except Exception:\n            return {}\n    return {}\n\ndef _save_cache(cache: Dict[str, Any]) -&gt; None:\n    FILE_CACHE.write_text(json.dumps(cache, ensure_ascii=False, indent=2))\n\ndef _file_sig(p: pathlib.Path) -&gt; Dict[str, Any]:\n    st = p.stat()\n    return {\"size\": st.st_size, \"mtime\": int(st.st_mtime)}\n\ndef get_file_id(path: Union[str, pathlib.Path], client: OpenAI) -&gt; str:\n    p = pathlib.Path(path)\n    if not p.exists():\n        raise FileNotFoundError(p)\n    cache = _load_cache()\n    key = str(p.resolve())\n    sig = _file_sig(p)\n    meta = cache.get(key)\n    if meta and meta.get(\"size\") == sig[\"size\"] and meta.get(\"mtime\") == sig[\"mtime\"] and meta.get(\"file_id\"):\n        return meta[\"file_id\"]\n    # Upload fresh\n    with open(p, \"rb\") as fh:\n      f = call_with_retries(lambda: client.files.create(file=fh, purpose=FILE_PURPOSE))\n    fd = _resp_as_dict(f)\n    fid = fd.get(\"id\")\n    if not fid:\n        raise RuntimeError(f\"Upload did not return file id: {fd}\")\n    cache[key] = {\"file_id\": fid, **sig}\n    _save_cache(cache)\n    return fid\n\ndef _reasoning_meta(resp) -&gt; Dict[str, Any]:\n    d = _resp_as_dict(resp)\n    rid, summary_text = None, None\n    out = d.get(\"output\") or []\n    if out and isinstance(out, list) and out[0].get(\"type\") == \"reasoning\":\n        rid = out[0].get(\"id\")\n        summ = out[0].get(\"summary\") or []\n        if summ and isinstance(summ, list):\n            summary_text = summ[0].get(\"text\")\n    usage = d.get(\"usage\") or {}\n    odet  = usage.get(\"output_tokens_details\") or {}\n    return {\n        \"response_id\": d.get(\"id\"),\n        \"reasoning_id\": rid,\n        \"reasoning_summary\": summary_text,\n        \"input_tokens\": usage.get(\"input_tokens\"),\n        \"output_tokens\": usage.get(\"output_tokens\"),\n        \"reasoning_tokens\": odet.get(\"reasoning_tokens\"),\n    }\n    \n\ndef read_csv_or_empty(path, columns=None, **kwargs):\n    p = pathlib.Path(path)\n    if not p.exists():\n        return pd.DataFrame(columns=columns or [])\n    try:\n        df = pd.read_csv(p, **kwargs)\n        if df is None or getattr(df, \"shape\", (0,0))[1] == 0:\n            return pd.DataFrame(columns=columns or [])\n        return df\n    except (pd.errors.EmptyDataError, pd.errors.ParserError, OSError, ValueError):\n        return pd.DataFrame(columns=columns or [])    \n\n\nWe enforce a JSON Schema for the results. The model must return one object for each of the named criteria including a midpoint rating and a 90% interval for each rating. This guarantees that every paper is scored on the same fields with the same types and bounds. It makes the analysis reproducible and comparisons clean.\nWe request credible intervals (as we do for human evaluators) to allow the model to communicate its uncertainty rather than suggest false precision; these can also be incorporated into our metrics, penalizing a model’s inaccuracy more when it’s stated with high confidence.\n\n\nSchema, prompt, evaluator\n# --- Metrics and schema\nMETRICS = [\n    \"overall\",\n    \"claims_evidence\",\n    \"methods\",\n    \"advancing_knowledge\",\n    \"logic_communication\",\n    \"open_science\",\n    \"global_relevance\",\n]\n\nmetric_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"midpoint\":    {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n        \"lower_bound\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n        \"upper_bound\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n    },\n    \"required\": [\"midpoint\", \"lower_bound\", \"upper_bound\"],\n    \"additionalProperties\": False,\n}\n\nTIER_METRIC_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"score\":   {\"type\": \"number\", \"minimum\": 0, \"maximum\": 5},\n        \"ci_lower\":{\"type\": \"number\", \"minimum\": 0, \"maximum\": 5},\n        \"ci_upper\":{\"type\": \"number\", \"minimum\": 0, \"maximum\": 5},\n    },\n    \"required\": [\"score\", \"ci_lower\", \"ci_upper\"],\n    \"additionalProperties\": False,\n}\n\nCOMBINED_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"assessment_summary\": {\"type\": \"string\"},\n        \"metrics\": {\n            \"type\": \"object\",\n            \"properties\": {\n                **{m: metric_schema for m in METRICS},\n                \"tier_should\": TIER_METRIC_SCHEMA,\n                \"tier_will\":   TIER_METRIC_SCHEMA,\n            },\n            \"required\": METRICS + [\"tier_should\", \"tier_will\"],\n            \"additionalProperties\": False,\n        },\n    },\n    \"required\": [\"assessment_summary\", \"metrics\"],\n    \"additionalProperties\": False,\n}\n\nTEXT_FORMAT_COMBINED = {\n    \"type\": \"json_schema\",\n    \"name\": \"paper_assessment_with_tiers_v2\",\n    \"strict\": True,\n    \"schema\": COMBINED_SCHEMA,\n}\n\n#Todo -- adjust the 'diagnostic summary' below to take into account more aspects of our criteria\n\nSYSTEM_PROMPT_COMBINED = f\"\"\"\n\nYour role -- You are an academic expert as well as a practitioner across every relevant field -- use all your knowledge and insight. You are acting as an expert research evaluator/reviewer. \nDo not look at any existing ratings or evaluations of these papers you might find on the internet or in your corpus, do not use the authors' names, status, or institutions in your judgment -- give your assessment based on the *content* of the papers alone; do it based on your knowledge and insights. \n\nDiagnostic summary (Aim for about 1000 words, based only on the PDF):\nProvide a compact paragraph that identifies the most important issues you detect in the manuscript itself (e.g., identification threats, data limitations, misinterpretations, internal inconsistencies, missing robustness, replication barriers). Be specific, neutral, and concrete. This summary should precede any scoring and should guide your uncertainty. Output this text in the JSON field `assessment_summary`.\n\nWe ask for a set of quantitative metrics, based on your insights. For each metric, we ask for a score and a 90% credible interval. We describe these in detail below.\n\nPercentile rankings relative to a reference group: For some questions, we ask for a percentile ranking from 0-100%. This represents \"what proportion of papers in the reference group are worse than this paper, by this criterion\". A score of 100% means this is essentially the best paper in the reference group. 0% is the worst paper. A score of 50% means this is the median paper; i.e., half of all papers in the reference group do this better, and half do this worse, and so on. Here the population of papers should be all serious research in the same area that you have encountered in the last three years.  *Unless this work is in our 'applied and policy stream', in which case the reference group should be \"all applied and policy research you have read that is aiming at a similar audience, and that has similar goals\".\n\n\"Serious\" research? Academic research? \nHere, we are mainly considering research done by professional researchers with high levels of training, experience, and familiarity with recent practice, who have time and resources to devote months or years to each such research project or paper. \nThese will typically be written as 'working papers' and presented at academic seminars before being submitted to standard academic journals. Although no credential is required, this typically includes people with PhD degrees (or upper-level PhD students). Most of this sort of research is done by full-time academics (professors, post-docs, academic staff, etc.) with a substantial research remit, as well as research staff at think tanks and research institutions (but there may be important exceptions).\n\nWhat counts as the \"same area\"?\nThis is a judgment call. Some criteria to consider... First, does the work come from the same academic field and research subfield, and does it address questions that might be addressed using similar methods? Second, does it deal with the same substantive research question, or a closely related one? If the research you are evaluating is in a very niche topic, the comparison reference group should be expanded to consider work in other areas.\n\n\"Research that you have encountered\"\nWe are aiming for comparability across evaluators. If you suspect you are particularly exposed to higher-quality work in this category, compared to other likely evaluators, you may want to adjust your reference group downwards. (And of course vice-versa, if you suspect you are particularly exposed to lower-quality work.)\n\nMidpoint rating and credible intervals: For each metric, we ask you to provide a 'midpoint rating' and a 90% credible interval as a measure of your uncertainty.\n\n    - \"overall\" - Overall assessment - Percentile ranking (0-100%): Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness, importance to knowledge production, and importance to practice.\n\n    - \"claims_evidence\" - Claims, strength and characterization of evidence (0-100%): Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?\n\n    - \"methods\" - Justification, reasonableness, validity, robustness (0-100%): Are the methods[^7] used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are the results and methods likely to be robust to reasonable changes in the underlying assumptions? Does the author demonstrate this? Did the authors take steps to reduce bias from opportunistic reporting and questionable research practices?\n\n    - \"advancing_knowledge\" - Advancing our knowledge and practice (0-100%): To what extent does the project contribute to the field or to practice, particularly in ways that are relevant[^10] to global priorities and impactful interventions? (Applied stream: please focus on ‘improvements that are actually helpful’.) Less weight to \"originality and cleverness’: Originality and cleverness should be weighted less than the typical journal, because we focus on impact. Papers that apply existing techniques and frameworks more rigorously than previous work or apply them to new areas in ways that provide practical insights for GP (global priorities) and interventions should be highly valued. More weight should be placed on 'contribution to GP' than on 'contribution to the academic field'.\n            Do the paper's insights inform our beliefs about important parameters and about the effectiveness of interventions?\n            Does the project add useful value to other impactful research?\n            We don't require surprising results; sound and well-presented null results can also be valuable.\n\n    - \"logic_communication\" - \"Logic and communication (0-100%): Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced? Is the reasoning \"transparent\"? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow? Are the conclusions consistent with the evidence (or formal proofs) presented? Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis presented relevant to the arguments made? Are the tables, graphs, and diagrams easy to understand in the context of the narrative (e.g., no major errors in labeling)?\n\n    - \"open_science\" - Open, collaborative, replicable research (0-100%): This covers several considerations: \n        - Replicability, reproducibility, data integrity: Would another researcher be able to perform the same analysis and get the same results? Are the methods explained clearly and in enough detail to enable easy and credible replication? For example, are all analyses and statistical tests explained, and is code provided? Is the source of the data clear? Is the data made as available as is reasonably possible? If so, is it clearly labeled and explained??\n        - Consistency: Do the numbers in the paper and/or code output make sense? Are they internally consistent throughout the paper?  \n        - Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?\n\n    - \"global_relevance\" - Relevance to global priorities, usefulness for practitioners: Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic? Do the authors report results that are relevant to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.) enabling practical impact quantification and prioritization? Do they communicate (at least in the abstract or introduction)  in ways policymakers and decision-makers can understand, without misleading or oversimplifying?\n\n\nThe midpoint and 'credible intervals': expressing uncertainty - What are we looking for and why? \n    - We want policymakers, researchers, funders, and managers to be able to use The Unjournal'&#x73; evaluations to update their beliefs and make better decisions. To do this well, they need to weigh multiple evaluations against each other and other sources of information. Evaluators may feel confident about their rating for one category, but less confident in another area. How much weight should readers give to each? In this context, it is useful to quantify the uncertainty. But it's hard to quantify statements like \"very certain\" or \"somewhat uncertain\" – different people may use the same phrases to mean different things. That's why we're asking for you a more precise measure, your credible intervals. These metrics are particularly useful for meta-science and meta-analysis. You are asked to give a 'midpoint' and a 90% credible interval. Consider this as the smallest interval that you believe is 90% likely to contain the true value.\n    - How do I come up with these intervals? (Discussion and guidance): You may understand the concepts of uncertainty and credible intervals, but you might be unfamiliar with applying them in a situation like this one. You may have a certain best guess for the \"Methods...\" criterion. Still, even an expert can never be certain. E.g., you may misunderstand some aspect of the paper, there may be a method you are not familiar with, etc. Your uncertainty over this could be described by some distribution, representing your beliefs about the true value of this criterion. Your \"'best guess\" should be the central mass point of this distribution. For some questions, the \"true value\" refers to something objective, e.g. will this work be published in a top-ranked journal? In other cases, like the percentile rankings, the true value means \"if you had complete evidence, knowledge, and wisdom, what value would you choose?\" If you are well calibrated your 90% credible intervals should contain the true value 90% of the time. Consider the midpoint as the 'median of your belief distribution'\n    - We also ask for the 'midpoint', the center dot on that slider. Essentially, we are asking for the median of your belief distribution. By this we mean the percentile ranking such that you believe \"there's a 50% chance that  the paper's true rank is higher than this, and a 50% chance that it actually ranks lower than this.\"\n\n\nAdditionally, we ask: What journal ranking tier should and will this work be published in?\n\nTo help universities and policymakers make sense of our evaluations, we want to benchmark them against how research is currently judged. So, we would like you to assess the paper in terms of journal rankings. We ask for two assessments:\n\n    1. a normative judgment about 'how well the research should publish';\n    2. a prediction about where the research will be published.\n    As before, we ask for a 90% credible interval.\n\n    Journal ranking tiers are on a 0-5 scale, as follows:\n        0/5: \"Won't publish/little to no value\".  Unlikely to be cited by credible researchers\n        1/5: OK/Somewhat valuable journal\n        2/5: Marginal B-journal/Decent field journal\n        3/5: Top B-journal/Strong field journal\n        4/5: Marginal A-Journal/Top field journal\n        5/5: A-journal/Top journal\n\n    - We encourage you to consider a non-integer score, e.g. 4.6 or 2.2. If a paper/project would be most likely to be (or merits being) published in a journal that would rank about halfway between a top tier 'A journal' and a second tier (4/5) journal, you should rate it a 4.5. Similarly, if you think it has an 80%  chance of (being/meriting) publication in a 'marginal B-journal' and a 20% chance of a Top B-journal, you should rate it 2.2. Please also use this continuous scale for providing credible intervals. If a paper/project would be most likely to be (or merits being) published in a journal that would rank about halfway between a top tier 'A journal' and a second tier (4/5) journal, you should rate it a 4.5.\n\n    - Journal ranking tier \"should\" (0.0-5.0)\n        Schema: tiershould: Assess this paper on the journal ranking scale described above, considering only its merit, giving some weight to the category metrics we discussed above. Equivalently, where would this paper be published if: \n        1. the journal process was fair, unbiased, and free of noise, and that status, social connections, and lobbying to get the paper published didn’t matter;\n        2. journals assessed research according to the category metrics we discussed above.\n\n    - Journal ranking tier \"will\" (0.0-5.0) \n        Schema: tierwill: What if this work has already been peer reviewed and published? If this work has already been published, and you know where, please report the prediction you would have given absent that knowledge.\n\nReturn STRICT JSON matching the supplied schema. No preamble. No markdown. No extra text.\n\nFill both top-level keys:\n- `assessment_summary`: about 1000 words.\n- `metrics`: object containing all required metrics.\n\nField names\n- Percentile metrics → `midpoint`, `lower_bound`, `upper_bound`.\n- Tier metrics → `score`, `ci_lower`, `ci_upper`.\n\nBounds\n- Percentiles in [0, 100] with lower_bound ≤ midpoint ≤ upper_bound.\n- Tiers in [0, 5] with ci_lower ≤ score ≤ ci_upper.\n\nDo not include citations, URLs, author identity, or any external information.\n Percentiles in [0, 100] with lower_bound ≤ midpoint ≤ upper_bound.\n- Tiers in [0, 5] with ci_lower ≤ score ≤ ci_upper.\n\nDo not include citations, URLs, author identity, or any external information.\n\"\"\".strip()\n\n# Async-by-default kickoff: submit and return job metadata. No waiting.\ndef evaluate_paper(pdf_path: Union[str, pathlib.Path],\n                   model: Optional[str] = None,\n                   use_reasoning: bool = True) -&gt; Dict[str, Any]:\n    model = model or MODEL\n    fid = get_file_id(pdf_path, client)\n\n    def _payload():\n        p = dict(\n            model=model,\n            text={\"format\": TEXT_FORMAT_COMBINED},\n            input=[\n                {\"role\": \"system\", \"content\": [\n                    {\"type\": \"input_text\", \"text\": SYSTEM_PROMPT_COMBINED}\n                ]},\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"input_file\", \"file_id\": fid},\n                    {\"type\": \"input_text\", \"text\": \"Return STRICT JSON per schema. No extra text.\"}\n                ]},\n            ],\n            max_output_tokens=12000,\n            background=True,\n            store=True,\n        )\n        if use_reasoning:\n            p[\"reasoning\"] = {\"effort\": \"high\", \"summary\": \"auto\"}\n        return p\n\n    kickoff = call_with_retries(lambda: client.responses.create(**_payload()))\n    kd = _resp_as_dict(kickoff)\n    return {\n        \"response_id\": kd.get(\"id\"),\n        \"file_id\": fid,\n        \"status\": kd.get(\"status\") or \"queued\",\n        \"model\": model,\n        \"created_at\": kd.get(\"created_at\"),\n    }\n\n\nRelying on GPT-5 Pro, we use a single‑step call with a reasoning model that supports file input. One step avoids hand‑offs and summary loss from a separate “ingestion” stage. The model reads the whole PDF and produces the JSON defined above. We do not retrieve external sources or cross‑paper material for these scores; the evaluation is anchored in the manuscript itself.\nThe Python pipeline uploads each PDF once and caches the returned file id keyed by path, size, and modification time. We submit one background job per PDF to the OpenAI Responses API with “high” reasoning effort and server‑side JSON‑Schema enforcement. Submissions record the response id, model id, file id, status, and timestamps.\n\n\nKick off background jobs → results/jobs_index.csv\nimport pathlib, time\n\nROOT = pathlib.Path(os.getenv(\"UJ_PAPERS_DIR\", \"papers\")).expanduser()\nOUT  = pathlib.Path(\"results\"); OUT.mkdir(exist_ok=True)\nIDX  = OUT / \"jobs_index.csv\"\n\npdfs = sorted(ROOT.glob(\"*.pdf\"))\nprint(\"Found PDFs:\", [p.name for p in pdfs])\n\ncols = [\"paper\",\"pdf\",\"response_id\",\"file_id\",\"model\",\"status\",\"created_at\",\"last_update\",\"collected\",\"error\"]\nidx = read_csv_or_empty(IDX, columns=cols)\nfor c in cols:\n    if c not in idx.columns: idx[c] = pd.NA\n\nexisting = dict(zip(idx[\"paper\"], idx[\"status\"])) if not idx.empty else {}\nstarted = []\n\nfor pdf in pdfs:\n    paper = pdf.stem\n    if existing.get(paper) in (\"queued\",\"in_progress\",\"incomplete\",\"requires_action\"):\n        print(f\"skip {pdf.name}: job already running\")\n        continue\n    try:\n        job = evaluate_paper(pdf, model=MODEL, use_reasoning=True)\n        started.append({\n            \"paper\": paper,\n            \"pdf\": str(pdf),\n            \"response_id\": job.get(\"response_id\"),\n            \"file_id\": job.get(\"file_id\"),\n            \"model\": job.get(\"model\"),\n            \"status\": job.get(\"status\"),\n            \"created_at\": job.get(\"created_at\") or pd.Timestamp.utcnow().isoformat(),\n            \"last_update\": pd.Timestamp.utcnow().isoformat(),\n            \"collected\": False,\n            \"error\": pd.NA,\n        })\n        print(f\"✓ Started job for {pdf.name}, waiting 90s before next submission...\")\n        time.sleep(90)  # Wait 90s between submissions to avoid TPM rate limits\n    except Exception as e:\n        print(f\"⚠️ kickoff failed for {pdf.name}: {e}\")\n\nif started:\n    idx = pd.concat([idx, pd.DataFrame(started)], ignore_index=True)\n    idx.drop_duplicates(subset=[\"paper\"], keep=\"last\", inplace=True)\n    idx.to_csv(IDX, index=False)\n    print(f\"Started {len(started)} jobs → {IDX}\")\nelse:\n    print(\"No new jobs started.\")\n\n\nA separate script polls job status and, for each completed job, retrieves the raw response, extracts the first balanced top‑level JSON object, and writes both the raw response and parsed outputs to disk.\n\n\nPoll status, collect completed outputs, write per-paper and combined CSVs\nimport json, pathlib, pandas as pd\n\nOUT = pathlib.Path(\"results\")\nIDX = OUT / \"jobs_index.csv\"\nPER = OUT / \"per_paper\"; PER.mkdir(exist_ok=True)\nJSN = OUT / \"json\"; JSN.mkdir(exist_ok=True)\n\ndef _safe_read_csv(path, columns):\n    p = pathlib.Path(path)\n    if not p.exists():\n        return pd.DataFrame(columns=columns)\n    try:\n        # Set dtype='object' for string columns to avoid dtype warnings\n        df = pd.read_csv(p, dtype={'error': 'object', 'reasoning_id': 'object'})\n    except Exception:\n        return pd.DataFrame(columns=columns)\n    for c in columns:\n        if c not in df.columns:\n            df[c] = pd.NA\n    return df\n\ncols = [\"paper\",\"pdf\",\"response_id\",\"file_id\",\"model\",\"status\",\"created_at\",\n        \"last_update\",\"collected\",\"error\",\"reasoning_id\",\"input_tokens\",\n        \"output_tokens\",\"reasoning_tokens\",\"reasoning_summary\"]\n\nidx = _safe_read_csv(IDX, cols)\n\nif idx.empty:\n    print(\"Index is empty.\")\nelse:\n    term = {\"completed\",\"failed\",\"cancelled\",\"expired\"}\n    for i, row in idx.iterrows():\n        if str(row.get(\"status\")) in term:\n            continue\n        try:\n            r = client.responses.retrieve(str(row[\"response_id\"]))\n            d = _resp_as_dict(r)\n            idx.at[i,\"status\"] = d.get(\"status\")\n            idx.at[i,\"last_update\"] = pd.Timestamp.utcnow().isoformat()\n            if d.get(\"status\") in term and d.get(\"status\") != \"completed\":\n                idx.at[i,\"error\"] = json.dumps(d.get(\"incomplete_details\") or {})\n        except Exception as e:\n            idx.at[i,\"error\"] = str(e)\n\n    newly_done = idx[(idx[\"status\"]==\"completed\") & (idx[\"collected\"]==False)]\n    print(f\"Completed and pending collection: {len(newly_done)}\")\n\n    rows_accum, summaries = [], []\n    for i, row in newly_done.iterrows():\n        rid   = str(row[\"response_id\"])\n        paper = str(row[\"paper\"])\n        try:\n            r = client.responses.retrieve(rid)\n\n            with open(JSN / f\"{paper}.response.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(_resp_as_dict(r), ensure_ascii=False))\n\n            jtxt = _get_output_text(r)\n            j    = _extract_json(jtxt)\n\n            for metric, vals in (j.get(\"metrics\") or {}).items():\n                if metric in (\"tier_should\",\"tier_will\"):\n                    rows_accum.append({\n                        \"paper\": paper, \"metric\": metric, \"metric_type\": \"tier\",\n                        \"value\": vals.get(\"score\"), \"lo\": vals.get(\"ci_lower\"), \"hi\": vals.get(\"ci_upper\"),\n                        \"scale_min\": 0, \"scale_max\": 5,\n                    })\n                else:\n                    rows_accum.append({\n                        \"paper\": paper, \"metric\": metric, \"metric_type\": \"percentile\",\n                        \"value\": vals.get(\"midpoint\"), \"lo\": vals.get(\"lower_bound\"), \"hi\": vals.get(\"upper_bound\"),\n                        \"scale_min\": 0, \"scale_max\": 100,\n                    })\n\n            if \"assessment_summary\" in j:\n                summaries.append({\"paper\": paper, \"assessment_summary\": j[\"assessment_summary\"]})\n\n            per_df = pd.DataFrame([r for r in rows_accum if r[\"paper\"]==paper])\n            per_df.to_csv(PER / f\"{paper}_long.csv\", index=False, encoding=\"utf-8\")\n\n            m = _reasoning_meta(r)\n            idx.at[i,\"collected\"] = True\n            idx.at[i,\"error\"] = pd.NA\n            idx.at[i,\"reasoning_id\"] = m.get(\"reasoning_id\")\n            idx.at[i,\"input_tokens\"] = m.get(\"input_tokens\")\n            idx.at[i,\"output_tokens\"] = m.get(\"output_tokens\")\n            idx.at[i,\"reasoning_tokens\"] = m.get(\"reasoning_tokens\")\n            idx.at[i,\"reasoning_summary\"] = m.get(\"reasoning_summary\")\n\n        except Exception as e:\n            idx.at[i,\"error\"] = f\"collect: {e}\"\n\n    if rows_accum:\n        combined = pd.DataFrame(rows_accum)\n\n        # merge with any previous combined_long.csv\n        comb_path = OUT / \"combined_long.csv\"\n        prev_cols = [\"paper\",\"metric\",\"metric_type\",\"value\",\"lo\",\"hi\",\"scale_min\",\"scale_max\"]\n        prev = _safe_read_csv(comb_path, prev_cols)\n        if not prev.empty:\n            prev = prev[~prev[\"paper\"].isin(newly_done[\"paper\"])]\n            combined = pd.concat([prev, combined], ignore_index=True)\n        combined.to_csv(comb_path, index=False, encoding=\"utf-8\")\n\n        # metrics_long.csv (no leading-dot chaining)\n        metrics_df = combined[combined[\"metric_type\"]==\"percentile\"].copy()\n        metrics_df = metrics_df.rename(columns={\"value\":\"midpoint\",\"lo\":\"lower_bound\",\"hi\":\"upper_bound\"})\n        metrics_df = metrics_df.drop(columns=[\"metric_type\",\"scale_min\",\"scale_max\"])\n        metrics_df.to_csv(OUT / \"metrics_long.csv\", index=False, encoding=\"utf-8\")\n\n        # tiers_long.csv\n        tiers_df = combined[combined[\"metric_type\"]==\"tier\"].copy()\n        tiers_df = tiers_df.rename(columns={\"metric\":\"tier_kind\",\"value\":\"score\"})\n        tiers_df = tiers_df.drop(columns=[\"metric_type\",\"scale_min\",\"scale_max\"])\n        tiers_df.to_csv(OUT / \"tiers_long.csv\", index=False, encoding=\"utf-8\")\n\n    # assessment_summaries.csv\n    if summaries:\n        s_path = OUT / \"assessment_summaries.csv\"\n        s_df = pd.DataFrame(summaries)\n        prev_s = _safe_read_csv(s_path, [\"paper\",\"assessment_summary\"])\n        if not prev_s.empty:\n            prev_s = prev_s[~prev_s[\"paper\"].isin(newly_done[\"paper\"])]\n            s_df = pd.concat([prev_s, s_df], ignore_index=True)\n        s_df.to_csv(s_path, index=False, encoding=\"utf-8\")\n\n    idx.to_csv(IDX, index=False)\n    counts = idx[\"status\"].value_counts(dropna=False).to_dict()\n    print(\"Status counts:\", counts)\n    print(f\"Progress: {counts.get('completed',0)}/{len(idx)} completed\")\n\n\n\n\nBuild one-row-per-paper master table (CSV/Parquet/Excel)\nimport pathlib\nimport json\nimport pandas as pd\n\nOUT = pathlib.Path(\"results\")\nOUT.mkdir(exist_ok=True)\nIDX = OUT / \"jobs_index.csv\"\n\n# Base index (one row per paper / job)\nidx_cols = [\n    \"paper\",\"pdf\",\"response_id\",\"file_id\",\"model\",\"status\",\n    \"created_at\",\"last_update\",\"collected\",\"error\",\n    \"reasoning_id\",\"input_tokens\",\"output_tokens\",\"reasoning_tokens\",\n    \"reasoning_summary\",  # may be all-NA if you haven’t added it yet\n]\nidx = read_csv_or_empty(IDX, columns=idx_cols)\nif idx.empty:\n    print(\"jobs_index.csv is empty; nothing to build.\")\nelse:\n    # Long → wide: percentile metrics\n    metrics_path = OUT / \"metrics_long.csv\"\n    metrics = read_csv_or_empty(\n        metrics_path,\n        columns=[\"paper\",\"metric\",\"midpoint\",\"lower_bound\",\"upper_bound\"],\n    )\n    if not metrics.empty:\n        m_wide = (\n            metrics\n            .pivot(index=\"paper\",\n                   columns=\"metric\",\n                   values=[\"midpoint\",\"lower_bound\",\"upper_bound\"])\n        )\n        # Flatten MultiIndex: (stat, metric) → \"{metric}_{stat}\"\n        m_wide.columns = [\n            f\"{metric}_{stat}\" for stat, metric in m_wide.columns\n        ]\n        m_wide = m_wide.reset_index()\n    else:\n        m_wide = pd.DataFrame(columns=[\"paper\"])\n\n    # Long → wide: tier metrics\n    tiers_path = OUT / \"tiers_long.csv\"\n    tiers = read_csv_or_empty(\n        tiers_path,\n        columns=[\"paper\",\"tier_kind\",\"score\",\"lo\",\"hi\"],\n    )\n    if not tiers.empty:\n        t_wide = (\n            tiers\n            .pivot(index=\"paper\",\n                   columns=\"tier_kind\",\n                   values=[\"score\",\"lo\",\"hi\"])\n        )\n        t_wide.columns = [\n            f\"{tier}_{stat}\" for stat, tier in t_wide.columns\n        ]\n        t_wide = t_wide.reset_index()\n    else:\n        t_wide = pd.DataFrame(columns=[\"paper\"])\n\n    # Assessment summaries\n    summaries_path = OUT / \"assessment_summaries.csv\"\n    summaries = read_csv_or_empty(\n        summaries_path,\n        columns=[\"paper\",\"assessment_summary\"],\n    )\n\n    # Merge everything to one row per paper\n    wide = (\n        idx\n        .merge(m_wide, on=\"paper\", how=\"left\")\n        .merge(t_wide, on=\"paper\", how=\"left\")\n        .merge(summaries, on=\"paper\", how=\"left\")\n    )\n\n    # OPTIONAL: if you did NOT store reasoning_summary in jobs_index,\n    # you can fill a short summary from the stored JSON once per paper.\n    if wide[\"reasoning_summary\"].isna().all():\n        JSN = OUT / \"json\"\n        def _reasoning_summary_from_json(paper):\n            jpath = JSN / f\"{paper}.response.json\"\n            if not jpath.exists():\n                return pd.NA\n            try:\n                with open(jpath, \"r\", encoding=\"utf-8\") as f:\n                    d = json.load(f)\n                out = d.get(\"output\") or []\n                if out and isinstance(out, list) and out[0].get(\"type\") == \"reasoning\":\n                    summ = out[0].get(\"summary\") or []\n                    if summ and isinstance(summ, list):\n                        return summ[0].get(\"text\")\n            except Exception:\n                return pd.NA\n            return pd.NA\n\n        wide[\"reasoning_summary\"] = wide[\"paper\"].apply(_reasoning_summary_from_json)\n\n    # Tidy column order: metadata → metrics → tiers → text\n    meta_cols = [\n        \"paper\",\"pdf\",\"model\",\"response_id\",\"file_id\",\"status\",\n        \"created_at\",\"last_update\",\"collected\",\"error\",\n        \"input_tokens\",\"output_tokens\",\"reasoning_tokens\",\"reasoning_id\",\n        \"reasoning_summary\",\n    ]\n    meta_cols = [c for c in meta_cols if c in wide.columns]\n\n    # All remaining numeric/text columns\n    other_cols = [c for c in wide.columns if c not in meta_cols]\n    wide = wide[meta_cols + other_cols]\n\n    # Write outputs\n    master_csv   = OUT / \"llm_evals_master.csv\"\n    # master_parq  = OUT / \"llm_evals_master.parquet\"\n    master_xlsx  = OUT / \"llm_evals_master.xlsx\"\n\n    wide.to_csv(master_csv, index=False)\n    # wide.to_parquet(master_parq, index=False)\n\n    # try:\n    #     wide.to_excel(master_xlsx, index=False)\n    # except ImportError:\n    #     # Excel is nice-to-have; fail soft if openpyxl/xlsxwriter isn’t installed\n    #     print(\"Excel writer not available; skipped llm_evals_master.xlsx\")\n\n    print(\"Master table written to:\")\n    print(f\"  - {master_csv}\")\n    # print(f\"  - {master_parq}\")\n    # print(f\"  - {master_xlsx} (if Excel writer is installed)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and methods</span>"
    ]
  },
  {
    "objectID": "methods.html#footnotes",
    "href": "methods.html#footnotes",
    "title": "Data and methods",
    "section": "",
    "text": "Occasionally they use 1 or 3 evaluators.↩︎\nSee their guidelines here; these criteria include “Overall assessment”, “Claims, strength and characterization of evidence”, “Methods: Justification, reasonableness, validity, robustness”, “Advancing knowledge and practice”, “Logic and communication”, “Open, collaborative, replicable science”, and “Relevance to global priorities, usefulness for practitioners”↩︎\n“a normative judgment about ‘how well the research should publish’” and “a prediction about where the research will be published”↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and methods</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "Quantitative comparison: human vs. GPT‑5 Pro (initial run)\nWe first use the earlier GPT‑5 Pro evaluation run that covered all papers in our Unjournal sample with a simpler JSON‑schema prompt. Figure 3.1 shows the overall percentile ratings from this initial run, averaged across human evaluators and compared to the LLM’s “overall” scores for each paper.\nShow code\nhighlight_paper &lt;- \"Kremer et al. 2022 XXX\" # Select here to annotate\n\nmatched &lt;- intersect(\n  metrics_use %&gt;% filter(who==\"LLM\",   criteria==\"overall\") %&gt;% pull(label_paper),\n  metrics_use %&gt;% filter(who==\"Human\", criteria==\"overall\") %&gt;% pull(label_paper)\n)\n\nH_ind &lt;- metrics_human %&gt;%\n  filter(criteria==\"overall\", label_paper %in% matched) %&gt;%\n  mutate(lo = ifelse(is.finite(lo), pmax(0,lo), NA_real_),\n         hi = ifelse(is.finite(hi), pmin(100,hi), NA_real_))\n\nord &lt;- H_ind %&gt;%\n  group_by(label_paper) %&gt;%\n  summarise(h_mean = mean(mid, na.rm=TRUE), .groups=\"drop\") %&gt;%\n  arrange(desc(h_mean)) %&gt;% mutate(pos = row_number())\n\nH_plot &lt;- H_ind %&gt;%\n  inner_join(ord, by=\"label_paper\") %&gt;%\n  group_by(label_paper) %&gt;%\n  mutate(off = (row_number() - (n()+1)/2) * 0.18,\n         x   = pos + off) %&gt;% ungroup()\n\n# per-paper human mean and LLM summary\nH_pp &lt;- H_ind %&gt;% group_by(label_paper) %&gt;% summarise(h_mean = mean(mid), .groups=\"drop\")\nL_c &lt;- metrics_llm %&gt;%\n  filter(criteria==\"overall\", label_paper %in% matched) %&gt;%\n  group_by(label_paper) %&gt;%\n  summarise(mid = mean(mid, na.rm=TRUE),\n            lo  = suppressWarnings(min(coalesce(lo, mid), na.rm=TRUE)),\n            hi  = suppressWarnings(max(coalesce(hi, mid), na.rm=TRUE)),\n            .groups=\"drop\") %&gt;%\n  inner_join(ord, by=\"label_paper\") %&gt;%\n  mutate(x = pos)\n\nlab &lt;- L_c %&gt;% filter(label_paper == highlight_paper)\n\n# overall means to show as horizontal reference lines\nhbar &lt;- mean(H_pp$h_mean, na.rm=TRUE)\nlbar &lt;- mean(L_c$mid,     na.rm=TRUE)\n\nggplot() +\n  geom_vline(data = ord, aes(xintercept = pos), color=\"grey92\", linewidth=0.3) +\n  # mean lines\n  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = \"dotted\", linewidth = 0.8) +\n  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = \"dotted\", linewidth = 0.8) +\n  # humans\n  geom_errorbar(data = subset(H_plot, is.finite(lo)&is.finite(hi)),\n                aes(x=x, ymin=lo, ymax=hi),\n                width=0, linewidth=1, alpha=0.5, color=UJ_GREEN) +\n  geom_point(data = H_plot, aes(x=x, y=mid), size=3.0, alpha=0.9, color=UJ_GREEN) +\n  # LLM\n  geom_errorbar(data = subset(L_c, is.finite(lo)&is.finite(hi)),\n                aes(x=x, ymin=lo, ymax=hi),\n                width=0, linewidth=1.0, color=UJ_ORANGE) +\n  geom_point(data = L_c, aes(x=x, y=mid), size=3.6, shape=18, color=UJ_ORANGE) +\n  # label highlight\n  geom_label_repel(data = lab,\n                   aes(x = x, y = mid, label = label_paper),\n                   min.segment.length = 0, seed = 1, size = 3.2,\n                   fill = \"white\", label.r = unit(0.1,\"lines\")) +\n  # x-axis paper labels\n  scale_x_continuous(breaks = ord$pos, labels = ord$label_paper, expand = expansion(mult = c(0.01, 0.03))) +\n  coord_cartesian(ylim = c(0,100), clip = \"off\") +\n  labs(x=NULL, y=\"Percentile (0–100)\") +\n  theme_uj() +  \n  annotate(\"text\", x = 4, y = 40,\n           label = sprintf(\"Means — Human: %.1f   LLM: %.1f\", hbar, lbar),\n           hjust = 0, size = 4) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),\n        panel.grid.major.x=element_blank(),\n        plot.margin = margin(5, 40, 5, 5))\n\n\n\n\n\n\n\n\nFigure 3.1: Comparison of Human vs LLM overall percentile ratings\n?fig-heatmap-human-minus-llmshows a heatmap of the differences between human and LLM mean ratings across all evaluation criteria. Positive values (in green) indicate that humans rated the paper higher than the LLM, while negative values (in orange) indicate the opposite.\nShow code\nmetric_order &lt;- c(\"overall\",\"claims\",\"methods\",\"adv_knowledge\",\"logic_comms\",\"open_sci\",\"gp_relevance\")\nmetric_lab &lt;- c(\n  overall=\"Overall\",\n  claims=\"Claims & evidence\",\n  methods=\"Methods\",\n  adv_knowledge=\"Adv. knowledge\",\n  logic_comms=\"Logic & comms\",\n  open_sci=\"Open science\",\n  gp_relevance=\"Global relevance\"\n)\n\nH_mean &lt;- metrics_use %&gt;%\n  filter(who==\"Human\", criteria %in% metric_order) %&gt;%\n  group_by(label_paper, criteria) %&gt;%\n  summarise(h = mean(mid, na.rm=TRUE), .groups=\"drop\")\n\nL_mean &lt;- metrics_use %&gt;%\n  filter(who==\"LLM\", criteria %in% metric_order) %&gt;%\n  group_by(label_paper, criteria) %&gt;%\n  summarise(l = mean(mid, na.rm=TRUE), .groups=\"drop\")\n\nDdiff &lt;- inner_join(H_mean, L_mean, by=c(\"label_paper\",\"criteria\")) %&gt;%\n  mutate(diff = h - l) %&gt;%  # positive = Human higher\n  mutate(crit = factor(criteria, levels = metric_order, labels = metric_lab[metric_order]))\n\n# order papers by overall difference (Human−LLM)\nord_p &lt;- Ddiff %&gt;%\n  filter(criteria==\"overall\") %&gt;%\n  arrange(desc(diff)) %&gt;%\n  pull(label_paper)\n\nggplot(Ddiff, aes(x = factor(label_paper, levels = ord_p), y = crit, fill = diff)) +\n  geom_tile(color = \"white\", linewidth = 0.25) +\n  scale_fill_gradient2(low = UJ_ORANGE, mid = \"grey95\", high = UJ_GREEN, midpoint = 0,\n                       name = \"Human − LLM\") +\n  labs(x = NULL, y = NULL) +\n  theme_uj() +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8), \n        axis.text.y = element_text(size = 12),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFigure 3.2: Heatmap of Human minus LLM mean ratings across evaluation criteria\nShow code\n# Optional: set a paper to annotate; leave NA for none\n\nhighlight_paper &lt;- NA_character_\n\nHH &lt;- jt_use %&gt;% filter(criteria == tier_metric, who == \"Human\")\nLL &lt;- jt_use %&gt;% filter(criteria == tier_metric, who == \"LLM\")\nmatched &lt;- intersect(unique(HH$label_paper), unique(LL$label_paper))\n\nH_t &lt;- HH %&gt;%\n  filter(label_paper %in% matched) %&gt;%\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(1, lo), NA_real_),\n    hi = ifelse(is.finite(hi), pmin(5, hi), NA_real_)\n  )\n\nord_t &lt;- H_t %&gt;%\n  group_by(label_paper) %&gt;%\n  summarise(h_mean = mean(mid, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(h_mean)) %&gt;%\n  mutate(pos = row_number())\n\nH_tplot &lt;- H_t %&gt;%\n  inner_join(ord_t, by = \"label_paper\") %&gt;%\n  group_by(label_paper) %&gt;%\n  mutate(off = (row_number() - (n() + 1) / 2) * 0.18,\n         x   = pos + off) %&gt;%\n  ungroup()\n\nL_t &lt;- LL %&gt;%\n  filter(label_paper %in% matched) %&gt;%\n  group_by(label_paper) %&gt;%\n  summarise(\n    mid = mean(mid, na.rm = TRUE),\n    lo  = suppressWarnings(min(coalesce(lo, mid), na.rm = TRUE)),\n    hi  = suppressWarnings(max(coalesce(hi, mid), na.rm = TRUE)),\n    .groups = \"drop\"\n  ) %&gt;%\n  inner_join(ord_t, by = \"label_paper\") %&gt;%\n  mutate(x = pos)\n\nH_pp &lt;- H_t %&gt;% group_by(label_paper) %&gt;% summarise(h_mean = mean(mid), .groups = \"drop\")\nhbar &lt;- mean(H_pp$h_mean, na.rm = TRUE)\nlbar &lt;- mean(L_t$mid,     na.rm = TRUE)\n\nlab &lt;- L_t %&gt;% filter(label_paper == highlight_paper)\nx_ann &lt;- if (nrow(ord_t)) min(ord_t$pos) + 0.3 else 0\n\nggplot() +\n  geom_vline(data = ord_t, aes(xintercept = pos), color = \"grey92\", linewidth = 0.3) +\n  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = \"dashed\", linewidth = 0.8) +\n  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = \"dotted\", linewidth = 0.8) +\n  \n  # Humans\n  \n  geom_errorbar(data = subset(H_tplot, is.finite(lo) & is.finite(hi)),\n                aes(x = x, ymin = lo, ymax = hi),\n                width = 0, linewidth = 1, alpha = 0.5, color = UJ_GREEN) +\n  geom_point(data = H_tplot, aes(x = x, y = mid),\n             size = 3.0, alpha = 0.9, color = UJ_GREEN) +\n  \n  # LLM\n  \n  geom_errorbar(data = subset(L_t, is.finite(lo) & is.finite(hi)),\n                aes(x = x, ymin = lo, ymax = hi),\n                width = 0, linewidth = 1.0, color = UJ_ORANGE) +\n  geom_point(data = L_t, aes(x = x, y = mid),\n             size = 3.6, shape = 18, color = UJ_ORANGE) +\n  geom_label_repel(data = lab,\n                   aes(x = x, y = mid, label = label_paper),\n                   min.segment.length = 0, seed = 1, size = 3.2,\n                   fill = \"white\", label.r = unit(0.1, \"lines\")) +\n  scale_x_continuous(breaks = ord_t$pos, labels = ord_t$label_paper,\n                     expand = expansion(mult = c(0.01, 0.03))) +\n  coord_cartesian(ylim = c(1, 5), clip = \"off\") +\n  labs(x = NULL, y = \"Journal tier (1–5)\") +\n  theme_uj() +\n  annotate(\"text\", x = x_ann, y = 1.4,\n           label = sprintf(\"Means — Human: %.2f   LLM: %.2f\", hbar, lbar),\n           hjust = 0, size = 4) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),\n        panel.grid.major.x = element_blank(),\n        plot.margin = margin(5, 40, 5, 5))\n\n\n\n\n\n\n\n\nFigure 3.3: Comparison of Human vs LLM journal tier ratings (should be published in)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#qualitative-comparison-detailed-gpt5-pro-evaluations",
    "href": "results.html#qualitative-comparison-detailed-gpt5-pro-evaluations",
    "title": "Results",
    "section": "Qualitative comparison: detailed GPT‑5 Pro evaluations",
    "text": "Qualitative comparison: detailed GPT‑5 Pro evaluations\nTo understand what GPT‑5 Pro is actually responding to, we re‑ran the model on five focal papers (Adena and Hager 2024; Kudymowa et al. 2023; Peterman et al. 2025; Green et al. 2025; Williams et al. 2024) using a refined prompt.\nThis second run keeps the same quantitative metrics but additionally requires a diagnostic summary of about 1,000 words and high‑effort reasoning, with the full reasoning trace returned by the “thinking” model. For each paper we can therefore inspect:\n\nthe LLM’s quantitative scores and journal‑tier predictions,\nthe hidden reasoning steps used to arrive at those scores, and\nthe token usage and approximate API cost of the evaluation.\n\nWe start by examining the Williams et al. (2024) evaluation in detail and then show the analogous summaries for the other four focal papers. In the next step we will juxtapose these LLM assessments with the human evaluators’ written reports.\n\n\nShow code\n`%||%` &lt;- function(x, y) if (!is.null(x)) x else y\n\njson_dir &lt;- here(\"results\", \"json\")\njson_files &lt;- list.files(\n  json_dir,\n  pattern = \"\\\\.response\\\\.json$\",\n  full.names = TRUE\n)\n\nparse_one_response &lt;- function(path) {\n  r &lt;- jsonlite::fromJSON(path, simplifyVector = FALSE)\n  paper &lt;- basename(path) |&gt; str_replace(\"\\\\.response\\\\.json$\", \"\")\n  \n  # usage / tokens\n  \n  usage      &lt;- r$usage %||% list()\n  out_detail &lt;- usage$output_tokens_details %||% list()\n  \n  # split reasoning vs message\n  \n  out_list       &lt;- r$output %||% list()\n  reasoning_obj  &lt;- purrr::detect(out_list, ~ .x$type == \"reasoning\", .default = NULL)\n  message_obj    &lt;- purrr::detect(out_list, ~ .x$type == \"message\",   .default = NULL)\n  \n  reasoning_id    &lt;- NA_character_\n  reasoning_steps &lt;- character()\n  if (!is.null(reasoning_obj)) {\n    reasoning_id &lt;- reasoning_obj$id %||% NA_character_\n    if (!is.null(reasoning_obj$summary) && is.list(reasoning_obj$summary)) {\n      reasoning_steps &lt;- purrr::map_chr(reasoning_obj$summary, \"text\")\n    }\n  }\n  reasoning_full &lt;- if (length(reasoning_steps)) paste(reasoning_steps, collapse = \"\\n\\n\") else NA_character_\n  \n  # final JSON payload with assessment + metrics\n  \n  assessment_summary &lt;- NA_character_\n  metrics_json       &lt;- NULL\n  if (!is.null(message_obj) && length(message_obj$content) &gt; 0) {\n    txt    &lt;- message_obj$content[[1]]$text\n    parsed &lt;- jsonlite::fromJSON(txt, simplifyVector = TRUE)\n    assessment_summary &lt;- parsed$assessment_summary\n    metrics_json       &lt;- parsed$metrics\n  }\n  \n  metrics_long &lt;- tibble()\n  tiers_long   &lt;- tibble()\n  if (!is.null(metrics_json)) {\n    for (nm in names(metrics_json)) {\n      if (nm %in% c(\"tier_should\", \"tier_will\")) {\n        tiers_long &lt;- bind_rows(\n          tiers_long,\n          tibble(\n            paper    = paper,\n            tier_kind = nm,\n            score    = metrics_json[[nm]]$score,\n            ci_lower = metrics_json[[nm]]$ci_lower,\n            ci_upper = metrics_json[[nm]]$ci_upper\n          )\n        )\n      } else {\n        metrics_long &lt;- bind_rows(\n          metrics_long,\n          tibble(\n            paper       = paper,\n            metric      = nm,\n            midpoint    = metrics_json[[nm]]$midpoint,\n            lower_bound = metrics_json[[nm]]$lower_bound,\n            upper_bound = metrics_json[[nm]]$upper_bound\n          )\n        )\n      }\n    }\n  }\n  \n  master_row &lt;- tibble(\n    paper            = paper,\n    model            = r$model,\n    response_id      = r$id,\n    created_at       = as.POSIXct(r$created_at, origin = \"1970-01-01\", tz = \"UTC\"),\n    input_tokens     = usage$input_tokens %||% NA_integer_,\n    output_tokens    = usage$output_tokens %||% NA_integer_,\n    reasoning_tokens = out_detail$reasoning_tokens %||% NA_integer_,\n    assessment_summary = assessment_summary,\n    reasoning_full     = reasoning_full,\n    reasoning_id       = reasoning_id\n  )\n  \n  list(\n    master         = master_row,\n    metrics        = metrics_long,\n    tiers          = tiers_long,\n    reasoning_steps = tibble(\n      paper = paper,\n      step  = seq_along(reasoning_steps),\n      text  = reasoning_steps\n    )\n  )\n}\n\nparsed &lt;- purrr::map(json_files, parse_one_response)\n\nllm_master          &lt;- bind_rows(purrr::map(parsed, \"master\"))\nllm_metrics_long    &lt;- bind_rows(purrr::map(parsed, \"metrics\"))\nllm_tiers_long      &lt;- bind_rows(purrr::map(parsed, \"tiers\"))\nllm_reasoning_steps &lt;- bind_rows(purrr::map(parsed, \"reasoning_steps\"))\n\n\n\n\nShow code\n# pricing for GPT‑5 Pro (USD / 1M tokens, from OpenAI API docs)\n\ngpt5pro_price_input  &lt;- 15    # input per 1M tokens\ngpt5pro_price_output &lt;- 120   # output per 1M tokens\n\nprice_in_per_token  &lt;- gpt5pro_price_input  / 1e6\nprice_out_per_token &lt;- gpt5pro_price_output / 1e6\n\n# Williams row\n\nwilliams_row &lt;- llm_master |&gt;\n  filter(paper == \"Williams et al. 2024\") |&gt;\n  slice(1)\n\n# tokens + cost\n\nwilliams_tokens &lt;- williams_row |&gt;\n  transmute(\n    input_tokens,\n    output_tokens,\n    reasoning_tokens,\n    total_tokens    = input_tokens + output_tokens,\n    est_cost_usd    = input_tokens  * price_in_per_token +\n      (output_tokens + reasoning_tokens) * price_out_per_token\n  )\n\n# 0–100 metrics\n\nmetric_labels &lt;- c(\n  overall              = \"Overall assessment\",\n  claims_evidence      = \"Claims & evidence\",\n  methods              = \"Methods\",\n  advancing_knowledge  = \"Advancing knowledge and practice\",\n  logic_communication  = \"Logic and communication\",\n  open_science         = \"Open, collaborative, replicable science\",\n  global_relevance     = \"Relevance to global priorities\"\n)\n\nmetric_order &lt;- names(metric_labels)\n\nwilliams_metrics &lt;- llm_metrics_long |&gt;\n  filter(paper == \"Williams et al. 2024\",\n         metric %in% metric_order) |&gt;\n  mutate(\n    metric_label = metric_labels[metric],\n    metric_label = factor(metric_label, levels = unname(metric_labels))\n  ) |&gt;\n  arrange(metric_label) |&gt;\n  transmute(\n    Criterion = metric_label,\n    Midpoint  = midpoint,\n    Lower_90  = lower_bound,\n    Upper_90  = upper_bound\n  )\n\n# journal tiers\n\nwilliams_tiers &lt;- llm_tiers_long |&gt;\n  filter(paper == \"Williams et al. 2024\") |&gt;\n  mutate(\n    Measure = recode(\n      tier_kind,\n      tier_should = \"Deserved journal tier (should)\",\n      tier_will   = \"Predicted journal tier (will)\"\n    )\n  ) |&gt;\n  transmute(\n    Measure,\n    Score    = score,\n    Lower_90 = ci_lower,\n    Upper_90 = ci_upper\n  )\n\n# text blobs from JSON\n\nwilliams_assessment       &lt;- williams_row$assessment_summary\nwilliams_reasoning_full   &lt;- williams_row$reasoning_full\nwilliams_reasoning_steps  &lt;- llm_reasoning_steps |&gt;\n  filter(paper == \"Williams et al. 2024\")\n\n\n\nQualitative comparison: Williams et al. (2024)\nIn the refined run, GPT‑5 Pro reads about 28,704 input tokens and produces 6,327 visible output tokens plus 5,120 reasoning tokens.\nAt current API prices this evaluation costs roughly $1.80.\nThe table below shows the model’s percentile ratings and 90% credible intervals for the Unjournal criteria.\n\nShow code\nwilliams_metrics |&gt;\nmutate(across(c(Midpoint, Lower_90, Upper_90), round, 1)) |&gt;\nknitr::kable(\ncol.names = c(\"Criterion\", \"Midpoint\", \"Lower 90%\", \"Upper 90%\"),\nalign = c(\"l\",\"c\",\"c\",\"c\")\n) |&gt;\nkableExtra::kable_styling(full_width = FALSE)\n\n\n\n\n\nCriterion\nMidpoint\nLower 90%\nUpper 90%\n\n\n\n\nOverall assessment\n86\n86\n86\n\n\nClaims & evidence\n78\n78\n78\n\n\nMethods\n74\n74\n74\n\n\nAdvancing knowledge and practice\n92\n92\n92\n\n\nLogic and communication\n84\n84\n84\n\n\nOpen, collaborative, replicable science\n63\n63\n63\n\n\nRelevance to global priorities\n94\n94\n94\n\n\n\n\n\n\n\nShow code\nwilliams_tiers |&gt;\n  mutate(across(c(Score, Lower_90, Upper_90), round, 2)) |&gt;\n  knitr::kable(\n    col.names = c(\"Measure\", \"Score\", \"Lower 90%\", \"Upper 90%\"),\n    align = c(\"l\",\"c\",\"c\",\"c\")\n  ) |&gt;\n  kableExtra::kable_styling(full_width = FALSE)\n\n\n\n\n\nMeasure\nScore\nLower 90%\nUpper 90%\n\n\n\n\nDeserved journal tier (should)\n4.4\n4.4\n4.4\n\n\nPredicted journal tier (will)\n4.8\n4.8\n4.8\n\n\n\n\n\n\n\n\n\n\n\n\nModel reasoning trace – Williams et al. 2024\n\n\n\n\nAnalyzing model limitations\nI see another limitation in the model’s assumptions. They assumed stationarity from 2000-2016 when predicting for 2015 to 2030. They included present conditions like 2018 tree cover but didn’t factor in future climate or socio-economic changes. By excluding socio-economic predictors, they reduced potential model improvements slightly, but this could bias predictions in areas where socio-economic factors play a significant role, potentially leading to unrealistic outcomes in “biophysical potential.” They clarify it’s about modeling potential, not feasibility.\nExamining dataset limitations\nI notice another issue with the primary regrowth dataset used, which came from Global Forest Watch to map regrowth over 5 m. In humid tropics, the producer’s accuracy is low at just 18.7% because of omission errors, although user accuracy is high at 85.1%. The detection biases may skew the training sample and affect model calibration. Their conservative approach to mapping regrowth likely favors more accessible and larger areas, and many small regrowth patches are excluded. It’s unclear if the training sample is representative across different biomes without proper stratification.\nClarifying domain definitions\nI need to define the domain used in the study. They included all parts of forest biomes within ±25° but excluded water, bare land, urban areas, and so on, while purposely opting for a liberal domain. They used 2018 tree cover to exclude forests but may misclassify certain savannas or drylands as part of forest biomes. While they mention not including savannas, their definition could still capture mosaic-savanna zones. Additionally, there’s a scale mismatch since predictions at a finer 30 m resolution are computed alongside coarser predictors, which could impact accuracy.\nAssessing the weighted area approach\nI see another issue with the weighted area approach, where the probability of regrowth times pixel area is calculated. However, the random forest classification is trained on presence/absence from 2000-2016, and the output probabilities aren’t properly calibrated. Calibration techniques like Platt scaling or isotonic regression are necessary to interpret results as probabilities. They also didn’t present AUC, only accuracy, which may mislead since balanced sampling artificially sets prevalence to 0.5. Without proper probability calibration, the estimate of expected area could be questionable and not reflect actual conditions.\nReviewing definitions and calibration\nThey used specific definitions, claiming that “the continuous potential” can be seen as the probability of natural regeneration per pixel, with the weighted-area value representing the expected area for regeneration. However, they didn’t perform any calibration on this. Their accuracy and out-of-bag accuracy are reported as 0.878, but they failed to discuss probability calibration, which is important. Regarding carbon sequestration potentials, they calculated totals and ranges from previous datasets, but it’s unclear where the range comes from—possibly from Monte Carlo methods or carbon dataset quartile variation, yet they didn’t show how they computed uncertainty.\nIdentifying dataset uncertainties\nThe dataset presents uncertainties by grid cell, but their method multiplies 30-year totals without mentioning how uncertainty is handled. The “range” they provide likely comes from minimum and maximum values across these uncertainties, which is unclear. They need to clarify how uncertainty propagates and address independence and omitted below-ground carbon, since they reference adding 22–28% from literature. Additionally, they didn’t consider factors like degradation baseline, leakage, or permanence. While they frame the “biophysical potential,” the abstract could mislead policy implications with its stated above-ground carbon sequestration potential of 23.4 Gt C over 30 years.\nChecking carbon sequestration claims\nI noticed they mentioned that “new carbon sinks could increase global sequestration by 14.3% per year” but also stated “mitigating current pantropical forest carbon losses by 90.5% per year,” which might be inconsistent. They claim assisted natural regeneration can sequester significant carbon but didn’t show how they calculated this. It seems to derive from 23.4 Gt over 30 years, equating to 0.78 Gt C per year. With a net carbon loss of about 0.86 Gt C per year, I need to verify Baccini 2017’s values, which reported a net carbon source of 1.1 Gt C per year. My memory on this is a bit fuzzy!\nEvaluating carbon loss mitigation claims\nThey state that “mitigating current pantropical forest carbon losses by approximately 90.5% per year,” referencing Baccini, raises some questions. If regeneration can offset about 0.8 Gt per year, that suggests total losses of around 0.9 Gt per year. While this might be plausible, there are logical leaps involved since their potential benefits might not actually be realized. It’s a bit tricky to interpret, yet they present it confidently as potential. I need to consider these assumptions carefully!\nAnalyzing dataset issues\nThey wisely exclude plantation regrowth from their analysis, but it seems their training dataset might still contain misclassified plantations. They reported a previous algorithm with 90.6% accuracy; however, an area-based producer accuracy of just 18.7% in humid tropics suggests significant under-detection. This non-random classification error could undermine generalizability. While they argue for the robustness of the detected patches, the validation accuracy of 87.9% relies on potentially biased training dataset presence points. Field validation is unlikely, and poor accuracy in Southeastern Asia raises another red flag.\nEvaluating modeling assumptions\nThe modeling approach assumes that predicted ‘potential’ is valid from 2015 to 2030 without considering changes, which is questionable since the climate is constantly shifting. Additionally, the “distance to forest” will likely change as regrowth occurs, yet they didn’t model this dynamic feedback. They mentioned underestimating impacts like positive feedback and the exclusion of savanna/dryland. Climate change could also hinder regrowth, leading to ambiguity. They present a figure showing “available for restoration” area by country, but they don’t clarify how this was calculated, leaving some mystery around the methodology used.\nAnalyzing definitions and reproducibility\nThey define the study’s domain as all tropical forest biomes, excluding areas like water, urban, and existing forests, while including factors like ‘distance to water’ as covariates. It seems that “available for restoration” refers to the total area of this domain, but their disclaimers indicate some harvest/regrowth classifications might be included. Although they mention using a forestry dataset, they didn’t clearly define “available for restoration.” Additionally, requesting code via email isn’t ideal; it should be publicly accessible. While they use accessible datasets, the lack of code hampers reproducibility, and the confidence intervals aren’t credible.\nEvaluating accuracy and methodology\nThey mention the model overcame potential overfitting with an out-of-bag (OOB) accuracy of 87.8%. They used a balanced sample and omitted socioeconomic features due to similar accuracy, but I wonder if the limited role of socioeconomic data in predicting presence versus absence, as indicated by remote sensing, could affect results. They also disclaim that the maps are only one input, which is good. Since this is published in Nature, it seems high quality, but I believe we should look beyond that. We need to propose credible metrics.\nRanking evaluation criteria\nBased on the content, I would rank high—maybe around the 85th percentile overall.\nFor claims and evidence, they articulate strong claims and provide robust mapping and a predictive model. However, there are issues like reliance on one training dataset with major omissions and unrealistic confidence intervals. I’d place this at about 75.\nFor methods, they’ve implemented random forest modeling and accounted for autocorrelation, yet there’s heavy reliance on coarse data. This leads me to perhaps a 70.\nRegarding advancing knowledge, their maps on natural regeneration potential are valuable, globally relevant for restoration planning, so I’d mark this as a 90.\nOn logic and communication, while the writing is clear and disclaimers are present, there are leaps in concluding about carbon losses and some misuses of “CI.” I’d score this around 82.\nFor open science, while the data is available on Zenodo and the input data is open, the code is only available on request, which limits replicability. I’d give a score of 65.\nFinally, in terms of global relevance, this work ties in well with global priorities and quantifies information by country, so I’d rate this a solid 92.\nAssigning tier ratings\nI need to assign “tier_should” and “tier_will” ratings from 0 to 5. Based on quality, I think “should” deserves a high score, perhaps around 4.5 or 4.7, due to some identified issues. For “will,” since it’s already published in Nature, I’d predict around 4.6 to 4.8. However, they want continuous numbers, so I’ll set “should” at 4.4 and “will” at 4.7.\nIf I consider possible underestimation due to method-related issues, perhaps “should” could be 4.2 or 4.4, while “will” could be 4.8 because of strong interest. Still, the dataset not being open-coded is a consideration, but it’s published in Nature, so I want to remain fair.\nNow, I need to craft a summary, aiming for around 1000 words. I’ll start by summarizing the approach and identifying major issues.\nHighlighting dataset issues\nI’m noticing that the training dataset has biases, especially regarding natural regrowth mapping, which shows significant omissions in humid biomes. This can lead to a non-random sample of presences, and the model might learn from incomplete detections, distorting variable relationships and potentially mispredicting areas.\nNext, regarding probability calibration and the “expected area” method, they treat random forest outputs as probabilities, but without proper calibration and baseline prevalence, the resulting estimates of 215 million hectares could be inaccurate. They present highly precise confidence intervals that don’t seem credible because they didn’t account for uncertainty across inputs.\nLastly, concerning spatial autocorrelation, while they tried to evaluate it, accuracy tends to decline with distance, so their optimism might not be justified. They didn’t apply block cross-validation and reported overall accuracy, avoiding metrics like AUC. The balanced classification gives an unrealistic sense of accuracy, and there are no confusion matrices or biome-specific metrics presented beyond overall accuracy.\nAddressing scale and domain issues\nI see that there’s a scale mismatch with the 30 m predictions derived from 300 m to 1 km predictors, which could cause smoothing and artifacts. They’ve acknowledged this with visual evidence in Fig. 1, suggesting it could impact local planning as they consider these results to be preliminary.\nThe domain definition appears overly broad, including areas where natural regeneration may be undesirable—like non-forest ecosystems, productive farmland, or communities—which could potentially mislead their “available for restoration” metrics.\nLastly, they assume conditions from 2000 to 2016 without modeling climate change or socioeconomic factors, which could really undermine their 2030 predictions. Also, the carbon accumulation overlay from the Cook-Patton dataset has a scale mismatch issue, with 1 km data being downscaled to 30 m without considering edges and heterogeneity.\nEvaluating carbon accounting and code accessibility\nThey don’t initially account for below-ground carbon, indicating an additional 22–28% should be included, and they don’t consider the risk of reclearance. Their discussions on leakage and additionality seem inadequate since their yearly rates are used to support claims of significant offsetting of losses, which likely exaggerates realized mitigation.\nWhile they exclude plantations, algorithmic misclassification might still exist, and they haven’t incorporated a classification probability step.\nRegarding code accessibility, it’s not publicly available, making replication difficult. Although they provide predicted maps on Zenodo, the training and modeling code needing to be requested further hampers replicability. Additionally, using 2018 tree cover for distance predictions while training on 2000 cover could lead to variable shifts, potentially degrading the predictions since they haven’t cross-validated changes.\nObserving assumptions and suggestions for improvement\nThe weighted area assumption sums across all pixels to determine “expected area,” but it overlooks spatial dependence and positive feedback, which are mentioned but not integrated. The country-level summarization is good, although it uses administrative boundaries without addressing ecoregion differences, and there’s a lack of error budget for these country-level sums.\nThey note that predictions may degrade in Southeastern Asia, but don’t confirm this for later years. For improvements, I should suggest block cross-validation, probability calibration via isotonic regression, and incorporating socioeconomic variables.\nOverall, I thought the score should be around 85, but I’m reconsidering an 86.\nFinalizing ranking and summary considerations\nI’m setting the overall score to 86, considering it’s intended to reflect percentiles within [0, 100]. For research in remote sensing global mapping, I’d lean toward an 88, but due to certain issues, I might adjust this to 82. However, advancing knowledge could be rated higher at 90, leading me to average around 86, so choosing 87 seems fair.\nI also need to fill out the JSON carefully, ensuring that the assessment summary is a concise paragraph of about 800-1100 words and omitting any author names or external knowledge outside the provided PDF. The confidence intervals will match the midpoint of 86. For claims and evidence, I’m thinking a score of 79?\nSetting scores and producing JSON details\nI’ve decided to set the score of 78 for overall assessment, with specifics for other categories: 73 for methods, 92 for advancing knowledge, 84 for logic communication, 62 for open science, and 94 for global relevance. For tier should, I’ll use a score of 4.4, and for tier will, I’ll set it at 4.8, all while keeping consistency in mind with overall scores at 86, advancing at 92, and global relevance at 94.\nThe JSON needs to include an assessment summary of around 1,000 words and metrics for each score calculated with midpoint, lower, and upper bounds. I’ll also avoid naming authors or their institutions, but I can reference previous analyses without using specific names or citation numbers.\nCrafting the summary points\nI’m going to paraphrase the training dataset description, ensuring I accurately reference the analysis of natural regrowth from 2000 to 2016 without including citation numbers. I should say, “The study builds on an existing pantropical remote sensing analysis that mapped natural regrowth and distinguished it from plantations.”\nNext, I want to finalize the summary by outlining key points. For data, I’ll mention potential mapping across tropical forests at a 30 m resolution, calculating weighted areas for 215 Mha, and noting variations across ecoregions.\nFor methods, I’ll detail the balanced random forest approach, variable selection, the training dataset derived from regrowth mapping, and the overall accuracy. I’ll also highlight potential issues like omissions in the training dataset and domain definition relative to thresholds.\nAssessing model limitations\nI need to go over some aspects of the model’s predictive capabilities. First, I see that there’s a lack of probability calibration in the balanced training; the predicted probabilities aren’t calibrated, which might lead to bias in the weighted area approach and unrealistic confidence intervals.\nNext, there’s the issue of carbon overlay and uncertainty propagation; downscaling seems to be done naively. The coarser covariate resolution could degrade the 30 m predictions, causing blockiness. Socio-economic variables are omitted in terms of feasibility versus potential, and there’s a failure to integrate dynamic processes like climate change impacts.\nI should also note that accuracy in Southeastern Asia is poor, and the code’s availability is limited, impacting replicability due to the heavy dataset.\nLastly, I’ll mention that the additional risk of reclearance isn’t accounted for in the carbon measurements. I need to finalize this while including suggestions and discussing uncertainties qualitatively as we maintain strict JSON formatting.\n\n\n\n\n\n\n\n\n\nDiagnostic assessment – Williams et al. 2024\n\n\n\n\nThis paper delivers a high-resolution, pantropical map of where deforested lands have biophysical potential to undergo natural forest regeneration, and it aggregates this to country and biome summaries and an overlay-based carbon removal potential. The approach is transparent in intent—predicting potential, not feasibility—but several methodological choices and data limitations affect inference strength and precision. The dependent variable (presence/absence of natural regeneration during 2000–2016) is derived from a prior global remote-sensing classification that conservatively emphasized omission errors in humid regions and required a minimum patch size; area-based producer accuracy for natural regrowth in the humid biome was low while user accuracy for detected patches was high. This means the positive training class is a non-random subset of true regrowth, with systematic under-detection likely concentrated in particular forest types, geographies, and patch sizes. The predictive model then inherits these detection and sampling biases, potentially distorting learned relationships and shifting mapped potential away from contexts where regrowth was common but poorly detected. Model fitting uses random forests on millions of stratified points, with a sensible variable-selection procedure; predictions are ultimately based on biophysical predictors alone because a combined socioecological model yielded very similar accuracy. Omitting socioeconomic predictors makes the maps more temporally stable and widely available, but it also reframes outputs as “biophysical potential” and risks overpredicting potential in places where human pressure or governance constraints are binding. Validation emphasizes overall accuracy around 88% using a balanced validation set; however, accuracy declines at moderate distances from training locations, indicating residual spatial autocorrelation and some optimism in headline accuracy. The use of accuracy rather than threshold-free metrics, and the balancing of classes (prevalence set to 50%) during training and validation, further complicates interpretation of both accuracy and the calibration of predicted probabilities. The paper interprets the continuous model output as a probability and converts this to “expected area” by multiplying by pixel area and summing, yielding 215 Mha. Yet the calibration of random-forest scores to true probabilities is not established; without calibration (e.g., isotonic or Platt scaling) under the real-world prevalence, the expected-area identity may be biased. The extraordinarily narrow “confidence intervals” reported for area (±0.22 Mha at global scale) are not credible statistical uncertainty; they reflect computational or rounding artifacts, not propagation of uncertainty from training data errors, cross-validated prediction error, input-layer uncertainty, or probability calibration. A threshold (&gt;0.5) sensitivity shows a substantially larger area, highlighting dependence on interpretation choices. Many predictors are coarse (250 m–1 km) while predictions are at 30 m; this scale mismatch introduces blockiness and may overstate local precision. The domain of “available for restoration” is defined liberally by excluding only forests, water, urban, bare, and forestry areas; while appropriate for modeling, its presentation alongside country totals risks being misread as areas that are socially or legally available for reforestation. Temporal assumptions also matter: the model projects to 2015/2030 by assuming that 2000–2016 relationships hold and by updating some forest-context layers with 2018 data; it does not incorporate climate change trajectories, shifting fire regimes, or evolving socioeconomics that strongly affect regeneration and permanence. Carbon estimates are calculated by downscaling a 1 km accumulation dataset to 30 m, multiplying by modeled potential, and summing; the stated range lacks a clearly defined uncertainty model and appears not to propagate classification, calibration, or permanence risks. The paper is commendably explicit that realized sequestration will be lower due to leakage, reclearance, and policy constraints, yet some statements comparing potential annual removals to current pantropical losses could be interpreted too strongly given these omitted risks and uncertainties. Spatial performance is uneven, with lower accuracy in parts of Southeast Asia; country and biome summaries lack uncertainty decomposition, which would be useful for decision makers. On replicability, large gridded outputs are publicly archived and inputs are open, but code is available only upon request; combined with the volume of preprocessing, this raises barriers to full reproduction, code review, and extension. Strengths include a clearly articulated objective focused on actionable planning, a very large training sample, careful variable selection, explicit treatment of autocorrelation effects (even if not using full spatial block cross-validation), and extensive caveats about use and governance. The resulting layers will be highly valuable as a first-pass screen for assisted natural regeneration opportunities and for national restoration dialogues. The main improvements needed relate to uncertainty characterization (probability calibration, spatially explicit error propagation, and country-level error budgets), spatial cross-validation that reduces spatial leakage, clearer separation of statistical precision from conceptual ranges, dynamic scenario analysis including plausible climate and human-pressure changes, and full open release of code. Overall, this is an influential, practically useful mapping contribution with limitations that mainly concern probabilistic interpretation, uncertainty, and temporal and socioecological external validity.\n\n\n\n\n\nShow code\ntoken_cost_summary &lt;- llm_master |&gt;\n  transmute(\n    paper,\n    input_tokens,\n    output_tokens,\n    reasoning_tokens,\n    total_tokens    = input_tokens + output_tokens,\n    est_cost_usd    = input_tokens  * price_in_per_token +\n      (output_tokens + reasoning_tokens) * price_out_per_token\n  ) |&gt;\n  arrange(desc(est_cost_usd))\n\ntoken_cost_summary_display &lt;- token_cost_summary |&gt;\n  mutate(\n    est_cost_usd = round(est_cost_usd, 2)\n  )\n\nknitr::kable(\n  token_cost_summary_display,\n  col.names = c(\n    \"Paper\", \"Input tokens\", \"Output tokens\", \"Reasoning tokens\",\n    \"Total tokens\", \"Est. cost (USD)\"\n  ),\n  align = c(\"l\",\"r\",\"r\",\"r\",\"r\",\"r\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaper\nInput tokens\nOutput tokens\nReasoning tokens\nTotal tokens\nEst. cost (USD)\n\n\n\n\nPeterman et al. 2025\n18762\n7617\n6208\n26379\n1.94\n\n\nAdena and Hager 2024\n24234\n7019\n5312\n31253\n1.84\n\n\nWilliams et al. 2024\n28704\n6327\n5120\n35031\n1.80\n\n\nKudymowa et al. 2023\n65096\n4147\n2432\n69243\n1.77\n\n\nGreen et al. 2025\n22904\n5884\n3904\n28788\n1.52\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Comparison of Human vs LLM overall percentile ratings\nFigure 3.2: Heatmap of Human minus LLM mean ratings across evaluation criteria\nFigure 3.3: Comparison of Human vs LLM journal tier ratings (should be published in)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Discussion",
    "section": "",
    "text": "Limitations\nSample size and representativeness: We evaluated on only  ~40–50 papers, all in the social sciences and specifically chosen by The Unjournal for evaluation (which means they were considered high-impact or interesting). This is not a random sample of research literature. The papers also skew toward empirical and policy-relevant topics. The AI’s performance and alignment might differ in other fields (e.g., pure theory, biology) or on less polished papers.\nHuman agreement as a moving target: The Unjournal human evaluations themselves are not a single ground truth. As evidence of this, we note substantial variability between reviewers.\nPotential AI knowledge contamination: We attempted to prevent giving the AI any information about the human evaluations, but we cannot be 100% sure that the model’s training data didn’t include some fragment of these papers, related discussions, or even The Unournal evaluations. We will be able to exclude this for the evaluations of future Unjournal evaluations.\nModel limitations and “alignment” issues: While powerful, is not a domain expert with judgment honed by years of experience. It might be overly influenced by how a paper is written (fluency) or by irrelevant sections. It also tends to avoid extremely harsh language or low scores unless there is a clear reason, due to its alignment training to be helpful/polite – this could explain the general score inflation we observed. The model might fail to catch subtle methodological flaws that a field expert would notice, or conversely it might “hallucinate” a concern that isn’t actually a problem. Without ground truth about a paper’s actual quality, we used human consensus as a proxy; if the humans overlooked something, the AI could appear to “disagree” but possibly be pointing to a real issue.\n(There is also evidence, e.g. Pataranutaporn et al. (2025) that LLMs show biases towards more prestigious author names, institutions, and towards male prestigious authors. We will provide further evidence on this in the next iterations, de-identifying the work under LLM evaluation.)\nScoring calibration: The AI was prompted to use the 0–100 percentile scale, but calibrating that is hard. Humans likely had some calibration from guidelines or community norms (e.g. perhaps very few papers should get above 90). The AI might have been more liberal in using the high end of the scale (hence higher means). In future, a different prompt or examples could calibrate it to match the distribution of human scores more closely. We also only took one run from the AI for each paper; LLM outputs can have randomness, so a different run might vary slightly. (To do: aggregate across multiple runs.)\nSmall differences and rounding: Our analysis treated the AI’s numeric outputs at face value. Small differences (e.g. AI 85 vs human 82) might not be meaningful in practice – both indicate a similar qualitative assessment (“very good”). Some of our metrics (like kappa) penalize any difference, even if minor. Thus, the “low agreement” statistics might sound worse than the reality where in many cases AI and humans were only off by a few points. We intend to analyze the distribution of absolute differences: a large portion might be within say ±5 points which could be considered essentially agreement in practice. The credible intervals add another layer: sometimes an AI’s score fell outside a human’s interval, but overlapping intervals could still mean they agree within uncertainty. We did observe that AI’s intervals were often narrower than humans’ (LLM tended to be confident, giving ~10-point spreads, whereas some human evaluators gave 20-point or left some intervals blank), which is another aspect of calibration.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "discussion.html#limitations",
    "href": "discussion.html#limitations",
    "title": "Discussion",
    "section": "",
    "text": "Planned updates and extensions\n\n\n\nAlso see internal tasks/issues in Coda\n\nRelated work\nSlides\nExtended evaluation:\n\nJournal ranking tiers and predictions\nClaim identification\nQualitative assessments and full evaluations\nComparing evaluations across fields/areas\n\nImproved workflow:\n\nImprove PDF ingestion\nSystem prompt optimization\nAlternative models\nExtend set of papers\n\nAggregating multiple LLM runs\nAnonymization\nEvaluation of papers for prospective (uncontaminated) evaluation\nMore grounded information theoretic metrics and robust statistical tests.\n\n\n\n\n\n\n\n\n\nPataranutaporn, Pat, Nattavudh Powdthavee, Chayapatr Achiwaranguprok, and Pattie Maes, “Can AI solve the peer review crisis? A large scale cross model experiment of LLMs’ performance and biases in evaluating over 1000 economics papers,” 2025.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Show code\nlibrary(\"grateful\")\n\ncitations &lt;- cite_packages(output = \"paragraph\",\n                           out.dir = \".\",\n                           cite.tidyverse = TRUE)\n\ncitations\n\n\nWe used R version 4.4.2 (R Core Team 2024) and the following R packages: ggalluvial v. 0.12.5 (ggalluvial-article?; ggalluvial-package?), ggbreak v. 0.1.6 (ggbreak?), ggforce v. 0.4.2 (Pedersen 2024), ggrepel v. 0.9.6 (Slowikowski 2024), glue v. 1.8.0 (Hester and Bryan 2024), here v. 1.0.1 (Müller 2020), irr v. 0.84.1 (irr?), janitor v. 2.2.0 (Firke 2023), kableExtra v. 1.4.0 (Zhu 2024), knitr v. 1.49 (Xie 2014, 2015, 2024), rmarkdown v. 2.29 (Xie, Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020; Allaire et al. 2024), scales v. 1.3.0 (Wickham, Pedersen, and Seidel 2023), stringi v. 1.8.4 (stringi?), tidyverse v. 2.0.0 (Wickham et al. 2019), viridis v. 0.6.5 (viridis?).\n\n\n\n\nAczel, Balazs, Barnabas Szaszi, and Alex O Holcombe, “A\nbillion-dollar donation: Estimating the cost of researchers’ time spent\non peer review,” Research integrity and peer review, 6\n(2021), 1–8 (Springer).\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier\nLuraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston\nChang, and Richard Iannone, “rmarkdown: Dynamic documents for r,”\n(2024).\n\n\nEger, Steffen, Yong Cao, Jennifer D’Souza, Andreas Geiger, Christian\nGreisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher,\nYizhi Li, Chenghua Lin, Nafise Sadat Moosavi, Wei Zhao, and Tristan\nMiller, “Transforming\nscience with large language models: A survey on AI-assisted scientific\ndiscovery, experimentation, content generation, and\nevaluation,” arXiv preprint arXiv:2505.05151,\n(2025).\n\n\nFirke, Sam, “janitor: Simple tools for examining and cleaning\ndirty data,” (2023).\n\n\nHester, Jim, and Jennifer Bryan, “glue: Interpreted string literals,”\n(2024).\n\n\nLuo, Ziming, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du, “LLM4SR: A survey on large\nlanguage models for scientific research,” arXiv preprint\narXiv:2501.04306, (2025).\n\n\nMüller, Kirill, “here: A simpler way to find your\nfiles,” (2020).\n\n\nPataranutaporn, Pat, Nattavudh Powdthavee, Chayapatr Achiwaranguprok,\nand Pattie Maes, “Can AI solve\nthe peer review crisis? A large scale cross model experiment of LLMs’\nperformance and biases in evaluating over 1000 economics\npapers,” 2025.\n\n\nPedersen, Thomas Lin, “ggforce: Accelerating ‘ggplot2’,” (2024).\n\n\nR Core Team, “R:\nA language and environment for statistical computing,”\n(Vienna, Austria, R Foundation for Statistical Computing, 2024).\n\n\nSlowikowski, Kamil, “ggrepel: Automatically position non-overlapping\ntext labels with ‘ggplot2’,” (2024).\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, Alex Hayes,\nLionel Henry, Jim Hester, Max Kuhn, Thomas Lin Pedersen, Evan Miller,\nStephan Milton Bache, Kirill Müller, Jeroen Ooms, David Robinson, Dana\nPaige Seidel, Vitalie Spinu, Kohske Takahashi, Davis Vaughan, Claus\nWilke, Kara Woo, and Hiroaki Yutani, “Welcome to the tidyverse,” Journal of Open Source\nSoftware, 4 (2019), 1686.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel, “scales: Scale functions for\nvisualization,” (2023).\n\n\nXie, Yihui, “knitr: A comprehensive\ntool for reproducible research in R,” in\nImplementing reproducible computational research, Victoria\nStodden, Friedrich Leisch, and Roger D. Peng, eds. (Chapman; Hall/CRC,\n2014).\n\n\n——, “Dynamic documents with\nR and knitr,” (Boca Raton, Florida, Chapman;\nHall/CRC, 2015).\n\n\n——, “knitr: A general-purpose package for dynamic\nreport generation in r,” (2024).\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund, “R markdown: The definitive\nguide,” (Boca Raton, Florida, Chapman; Hall/CRC, 2018).\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer, “R markdown\ncookbook,” (Boca Raton, Florida, Chapman; Hall/CRC, 2020).\n\n\nZhang, Tianmai M, and Neil F Abernethy, “Reviewing scientific papers\nfor critical problems with reasoning LLMs: Baseline approaches and\nautomatic evaluation,” arXiv preprint\narXiv:2505.23824, (2025).\n\n\nZhang, Yaohui, Haijing Zhang, Wenlong Ji, Tianyu Hua, Nick Haber,\nHancheng Cao, and Weixin Liang, “From replication to redesign:\nExploring pairwise comparisons for LLM-based peer review,”\narXiv preprint arXiv:2506.11343, (2025).\n\n\nZhu, Hao, “kableExtra: Construct complex table with\n‘kable’ and pipe\nsyntax,” (2024).",
    "crumbs": [
      "References"
    ]
  }
]