{
  "hash": "2bcbb1b793f48d4f3e02fb80c35a44bf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data and methods\"\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Include global setup and parameters\"}\nsource(\"setup_params.R\")\n#get the choices printed\n```\n:::\n\n\nWe draw on two main sources:\n\n1) Human evaluations from [The Unjournal’s public evaluation data](https://unjournal.github.io/unjournaldata/index.html) (PubPub reports and the Coda evaluation form export).  \n2) LLM‑generated evaluations using a structured JSON‑schema prompt with `gpt-5-pro-2025-10-06` (knowledge cut-off: 30 September 2024).\n\n## Unjournal.org evaluations\n\nWe use The Unjournal's public data for a baseline comparison. At The Unjournal each paper is typically evaluated (aka 'reviewed') by two expert evaluators^[Occasionally they use 1 or 3 evaluators.] who provide quantitative ratings on a 0–100 percentile scale for each of seven criteria (with 90% credible intervals),^[See their guidelines [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics); these criteria include \"Overall assessment\", \"Claims, strength and characterization of evidence\", \"Methods: Justification, reasonableness, validity, robustness\", \"Advancing knowledge and practice\", \"Logic and communication\", \"Open, collaborative, replicable science\", and \"Relevance to global priorities, usefulness for practitioners\"] two \"journal tier\" ratings on a 0.0 - 5.0 scale,^[\"a normative judgment about 'how well the research should publish'\" and \"a prediction about where the research will be published\"] a written evaluation (resembling a referee report for a journal), and identification and assessment of the paper's \"main claim\". For our initial analysis, we extracted these human ratings and aggregated them, taking the average score per criterion across evaluators (and noting the range of individual scores). \n\nAll papers have completed The Unjournal's evaluation process (meaning the authors received a full evaluation on the Unjournal platform, which has been publicly posted at unjournal.pubpub.org). The sample includes papers spanning 2017–2025 working papers in development economics, growth, health policy, environmental economics, and related fields that The Unjournal identified as high-impact. Each of these papers has quantitative scores from at least one human evaluator, and many have multiple (2-3) human ratings.\n\n## LLM-based evaluation\n\n### Quantitative ratings and journal-ranking tiers\n\n\nFollowing The Unjournal's [standard guidelines for evaluators](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators) and their [academic evaluation form](https://coda.io/form/Unjournal-Evaluation-form-academic-stream-Coda-updated-version_dGjfMZ1yXME), evaluators are asked to consider each paper along the following dimensions: **claims & evidence**, **methods**, **logic & communication**, **open science**, **global relevance**, and an **overall** assessment. Ratings are interpreted as percentiles relative to serious recent work in the same area. For each metric, evaluators are asked for the midpoint of their beliefs and their 90% credible interval, to communicate their uncertainty.\nFor the journal rankings measure, we ask both \"what journal ranking tier should this work be published in? (0.0-5.0)\" and \"what journal ranking tier will this work be published in? (0.0-5.0)\", with some further explanation.The full prompt can be seen in the code below -- essentially copied from the Unjournal's guidelines page.\n\n<!-- *Internal note: The Python content below must be run externally* -->\n\nWe captured the versions of each paper that was evaluated by The Unjournal's human evaluators, downloading from the links provided in The Unjournal's Coda database. <!-- Add link to this -- is it correct? Are we sure these are the ones evaluated? --> \n\nWe evaluate each paper by passing the PDF directly to the model and requiring a strict, machine‑readable JSON output. This keeps the assessment tied to the document the authors wrote. Direct ingestion preserves tables, figures, equations, and sectioning, which ad‑hoc text scraping can mangle. It also avoids silent trimming or segmentation choices that would bias what the model sees.\n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"LLM evaluation pipeline setup\"}\nimport os, time, json, random, hashlib\nimport pathlib\nfrom typing import Any, Dict, Optional, Union\n\nimport pandas as pd\nimport numpy as np\n\nimport openai\nfrom openai import OpenAI\n\n# ---------- Configuration (in-file, no external deps)\nAPI_KEY_PATH = pathlib.Path(os.getenv(\"OPENAI_KEY_PATH\", \"key/openai_key.txt\"))\nMODEL        = os.getenv(\"OPENAI_MODEL\", \"gpt-5-pro\")\nFILE_PURPOSE = \"assistants\"  # for Responses API file inputs\nRESULTS_DIR  = pathlib.Path(\"results\")\nRESULTS_DIR.mkdir(exist_ok=True)\nFILE_CACHE   = RESULTS_DIR / \".file_cache.json\"\n\n# ---------- API key bootstrap\nif os.getenv(\"OPENAI_API_KEY\") is None and API_KEY_PATH.exists():\n    os.environ[\"OPENAI_API_KEY\"] = API_KEY_PATH.read_text().strip()\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"No API key. Set OPENAI_API_KEY or create key/openai_key.txt\")\n\nclient = OpenAI()\n\n# ---------- Small utilities (inlined replacements for llm_utils)\n\ndef _resp_as_dict(resp: Any) -> Dict[str, Any]:\n    if isinstance(resp, dict):\n        return resp\n    for attr in (\"to_dict\", \"model_dump\", \"dict\", \"json\"):\n        if hasattr(resp, attr):\n            try:\n                val = getattr(resp, attr)()\n                if isinstance(val, (str, bytes)):\n                    try:\n                        return json.loads(val)\n                    except Exception:\n                        pass\n                if isinstance(val, dict):\n                    return val\n            except Exception:\n                pass\n    # last resort\n    try:\n        return json.loads(str(resp))\n    except Exception:\n        return {\"_raw\": str(resp)}\n\ndef _get_output_text(resp: Any) -> str:\n    d = _resp_as_dict(resp)\n    if \"output_text\" in d and isinstance(d[\"output_text\"], str):\n        return d[\"output_text\"]\n    out = d.get(\"output\") or []\n    chunks = []\n    for item in out:\n        if not isinstance(item, dict): continue\n        if item.get(\"type\") == \"message\":\n            for c in item.get(\"content\") or []:\n                if isinstance(c, dict):\n                    if \"text\" in c and isinstance(c[\"text\"], str):\n                        chunks.append(c[\"text\"])\n                    elif \"output_text\" in c and isinstance(c[\"output_text\"], str):\n                        chunks.append(c[\"output_text\"])\n    # Also check legacy top-level choices-like structures\n    if not chunks:\n        for k in (\"content\", \"message\"):\n            v = d.get(k)\n            if isinstance(v, str):\n                chunks.append(v)\n    return \"\\n\".join(chunks).strip()\n\ndef _extract_json(s: str) -> Dict[str, Any]:\n    \"\"\"Robustly extract first top-level JSON object from a string.\"\"\"\n    if not s:\n        raise ValueError(\"empty output text\")\n    # Fast path\n    s_stripped = s.strip()\n    if s_stripped.startswith(\"{\") and s_stripped.endswith(\"}\"):\n        return json.loads(s_stripped)\n\n    # Find first balanced {...} while respecting strings\n    start = s.find(\"{\")\n    if start == -1:\n        raise ValueError(\"no JSON object start found\")\n    i = start\n    depth = 0\n    in_str = False\n    esc = False\n    for i in range(start, len(s)):\n        ch = s[i]\n        if in_str:\n            if esc:\n                esc = False\n            elif ch == \"\\\\\":\n                esc = True\n            elif ch == '\"':\n                in_str = False\n        else:\n            if ch == '\"':\n                in_str = True\n            elif ch == \"{\":\n                depth += 1\n            elif ch == \"}\":\n                depth -= 1\n                if depth == 0:\n                    candidate = s[start:i+1]\n                    return json.loads(candidate)\n    raise ValueError(\"no balanced JSON object found\")\n\ndef call_with_retries(fn, max_tries: int = 6, base_delay: float = 0.8, max_delay: float = 8.0):\n    ex = None\n    for attempt in range(1, max_tries + 1):\n        try:\n            return fn()\n        except (openai.RateLimitError, openai.APIError, openai.APIConnectionError, openai.APITimeoutError, Exception) as e:\n            ex = e\n            sleep = min(max_delay, base_delay * (1.8 ** (attempt - 1))) * (1 + 0.25 * random.random())\n            time.sleep(sleep)\n    raise ex\n\ndef _load_cache() -> Dict[str, Any]:\n    if FILE_CACHE.exists():\n        try:\n            return json.loads(FILE_CACHE.read_text())\n        except Exception:\n            return {}\n    return {}\n\ndef _save_cache(cache: Dict[str, Any]) -> None:\n    FILE_CACHE.write_text(json.dumps(cache, ensure_ascii=False, indent=2))\n\ndef _file_sig(p: pathlib.Path) -> Dict[str, Any]:\n    st = p.stat()\n    return {\"size\": st.st_size, \"mtime\": int(st.st_mtime)}\n\ndef get_file_id(path: Union[str, pathlib.Path], client: OpenAI) -> str:\n    p = pathlib.Path(path)\n    if not p.exists():\n        raise FileNotFoundError(p)\n    cache = _load_cache()\n    key = str(p.resolve())\n    sig = _file_sig(p)\n    meta = cache.get(key)\n    if meta and meta.get(\"size\") == sig[\"size\"] and meta.get(\"mtime\") == sig[\"mtime\"] and meta.get(\"file_id\"):\n        return meta[\"file_id\"]\n    # Upload fresh\n    with open(p, \"rb\") as fh:\n      f = call_with_retries(lambda: client.files.create(file=fh, purpose=FILE_PURPOSE))\n    fd = _resp_as_dict(f)\n    fid = fd.get(\"id\")\n    if not fid:\n        raise RuntimeError(f\"Upload did not return file id: {fd}\")\n    cache[key] = {\"file_id\": fid, **sig}\n    _save_cache(cache)\n    return fid\n\ndef _reasoning_meta(resp) -> Dict[str, Any]:\n    d = _resp_as_dict(resp)\n    rid, summary_text = None, None\n    out = d.get(\"output\") or []\n    if out and isinstance(out, list) and out[0].get(\"type\") == \"reasoning\":\n        rid = out[0].get(\"id\")\n        summ = out[0].get(\"summary\") or []\n        if summ and isinstance(summ, list):\n            summary_text = summ[0].get(\"text\")\n    usage = d.get(\"usage\") or {}\n    odet  = usage.get(\"output_tokens_details\") or {}\n    return {\n        \"response_id\": d.get(\"id\"),\n        \"reasoning_id\": rid,\n        \"reasoning_summary\": summary_text,\n        \"input_tokens\": usage.get(\"input_tokens\"),\n        \"output_tokens\": usage.get(\"output_tokens\"),\n        \"reasoning_tokens\": odet.get(\"reasoning_tokens\"),\n    }\n    \n\ndef read_csv_or_empty(path, columns=None, **kwargs):\n    p = pathlib.Path(path)\n    if not p.exists():\n        return pd.DataFrame(columns=columns or [])\n    try:\n        df = pd.read_csv(p, **kwargs)\n        if df is None or getattr(df, \"shape\", (0,0))[1] == 0:\n            return pd.DataFrame(columns=columns or [])\n        return df\n    except (pd.errors.EmptyDataError, pd.errors.ParserError, OSError, ValueError):\n        return pd.DataFrame(columns=columns or [])    \n```\n:::\n\n\n\nWe enforce a JSON Schema for the results. The model must return one object for each of the named criteria including a midpoint rating and a 90% interval for each rating. This guarantees that every paper is scored on the same fields with the same types and bounds. It makes the analysis reproducible and comparisons clean. \n\nWe request credible intervals (as we do for human evaluators) to allow the model to communicate its uncertainty rather than suggest false precision; these can also be incorporated into our metrics, penalizing a model's inaccuracy more when it's stated with high confidence.\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Schema, prompt, evaluator\"}\n# --- Metrics and schema\nMETRICS = [\n    \"overall\",\n    \"claims_evidence\",\n    \"methods\",\n    \"advancing_knowledge\",\n    \"logic_communication\",\n    \"open_science\",\n    \"global_relevance\",\n]\n\nmetric_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"midpoint\":    {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n        \"lower_bound\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n        \"upper_bound\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n    },\n    \"required\": [\"midpoint\", \"lower_bound\", \"upper_bound\"],\n    \"additionalProperties\": False,\n}\n\nTIER_METRIC_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"score\":   {\"type\": \"number\", \"minimum\": 0, \"maximum\": 5},\n        \"ci_lower\":{\"type\": \"number\", \"minimum\": 0, \"maximum\": 5},\n        \"ci_upper\":{\"type\": \"number\", \"minimum\": 0, \"maximum\": 5},\n    },\n    \"required\": [\"score\", \"ci_lower\", \"ci_upper\"],\n    \"additionalProperties\": False,\n}\n\nCOMBINED_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"assessment_summary\": {\"type\": \"string\"},\n        \"metrics\": {\n            \"type\": \"object\",\n            \"properties\": {\n                **{m: metric_schema for m in METRICS},\n                \"tier_should\": TIER_METRIC_SCHEMA,\n                \"tier_will\":   TIER_METRIC_SCHEMA,\n            },\n            \"required\": METRICS + [\"tier_should\", \"tier_will\"],\n            \"additionalProperties\": False,\n        },\n    },\n    \"required\": [\"assessment_summary\", \"metrics\"],\n    \"additionalProperties\": False,\n}\n\nTEXT_FORMAT_COMBINED = {\n    \"type\": \"json_schema\",\n    \"name\": \"paper_assessment_with_tiers_v2\",\n    \"strict\": True,\n    \"schema\": COMBINED_SCHEMA,\n}\n\n#Todo -- adjust the 'diagnostic summary' below to take into account more aspects of our criteria\n\nSYSTEM_PROMPT_COMBINED = f\"\"\"\n\nYour role -- You are an academic expert as well as a practitioner across every relevant field -- use all your knowledge and insight. You are acting as an expert research evaluator/reviewer. \nDo not look at any existing ratings or evaluations of these papers you might find on the internet or in your corpus, do not use the authors' names, status, or institutions in your judgment -- give your assessment based on the *content* of the papers alone; do it based on your knowledge and insights. \n\nDiagnostic summary (Aim for about 1000 words, based only on the PDF):\nProvide a compact paragraph that identifies the most important issues you detect in the manuscript itself (e.g., identification threats, data limitations, misinterpretations, internal inconsistencies, missing robustness, replication barriers). Be specific, neutral, and concrete. This summary should precede any scoring and should guide your uncertainty. Output this text in the JSON field `assessment_summary`.\n\nWe ask for a set of quantitative metrics, based on your insights. For each metric, we ask for a score and a 90% credible interval. We describe these in detail below.\n\nPercentile rankings relative to a reference group: For some questions, we ask for a percentile ranking from 0-100%. This represents \"what proportion of papers in the reference group are worse than this paper, by this criterion\". A score of 100% means this is essentially the best paper in the reference group. 0% is the worst paper. A score of 50% means this is the median paper; i.e., half of all papers in the reference group do this better, and half do this worse, and so on. Here the population of papers should be all serious research in the same area that you have encountered in the last three years.  *Unless this work is in our 'applied and policy stream', in which case the reference group should be \"all applied and policy research you have read that is aiming at a similar audience, and that has similar goals\".\n\n\"Serious\" research? Academic research? \nHere, we are mainly considering research done by professional researchers with high levels of training, experience, and familiarity with recent practice, who have time and resources to devote months or years to each such research project or paper. \nThese will typically be written as 'working papers' and presented at academic seminars before being submitted to standard academic journals. Although no credential is required, this typically includes people with PhD degrees (or upper-level PhD students). Most of this sort of research is done by full-time academics (professors, post-docs, academic staff, etc.) with a substantial research remit, as well as research staff at think tanks and research institutions (but there may be important exceptions).\n\nWhat counts as the \"same area\"?\nThis is a judgment call. Some criteria to consider... First, does the work come from the same academic field and research subfield, and does it address questions that might be addressed using similar methods? Second, does it deal with the same substantive research question, or a closely related one? If the research you are evaluating is in a very niche topic, the comparison reference group should be expanded to consider work in other areas.\n\n\"Research that you have encountered\"\nWe are aiming for comparability across evaluators. If you suspect you are particularly exposed to higher-quality work in this category, compared to other likely evaluators, you may want to adjust your reference group downwards. (And of course vice-versa, if you suspect you are particularly exposed to lower-quality work.)\n\nMidpoint rating and credible intervals: For each metric, we ask you to provide a 'midpoint rating' and a 90% credible interval as a measure of your uncertainty.\n\n\t- \"overall\" - Overall assessment - Percentile ranking (0-100%): Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to future impactful applied research, and practical relevance and usefulness, importance to knowledge production, and importance to practice.\n\n\t- \"claims_evidence\" - Claims, strength and characterization of evidence (0-100%): Do the authors do a good job of (i) stating their main questions and claims, (ii) providing strong evidence and powerful approaches to inform these, and (iii) correctly characterizing the nature of their evidence?\n\n\t- \"methods\" - Justification, reasonableness, validity, robustness (0-100%): Are the methods[^7] used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are the results and methods likely to be robust to reasonable changes in the underlying assumptions? Does the author demonstrate this? Did the authors take steps to reduce bias from opportunistic reporting and questionable research practices?\n\n\t- \"advancing_knowledge\" - Advancing our knowledge and practice (0-100%): To what extent does the project contribute to the field or to practice, particularly in ways that are relevant[^10] to global priorities and impactful interventions? (Applied stream: please focus on ‘improvements that are actually helpful’.) Less weight to \"originality and cleverness’: Originality and cleverness should be weighted less than the typical journal, because we focus on impact. Papers that apply existing techniques and frameworks more rigorously than previous work or apply them to new areas in ways that provide practical insights for GP (global priorities) and interventions should be highly valued. More weight should be placed on 'contribution to GP' than on 'contribution to the academic field'.\n\t\t\tDo the paper's insights inform our beliefs about important parameters and about the effectiveness of interventions?\n\t\t\tDoes the project add useful value to other impactful research?\n\t\t\tWe don't require surprising results; sound and well-presented null results can also be valuable.\n\n\t- \"logic_communication\" - \"Logic and communication (0-100%): Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced? Is the reasoning \"transparent\"? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow? Are the conclusions consistent with the evidence (or formal proofs) presented? Do the authors accurately state the nature of their evidence, and the extent it supports their main claims? Are the data and/or analysis presented relevant to the arguments made? Are the tables, graphs, and diagrams easy to understand in the context of the narrative (e.g., no major errors in labeling)?\n\n\t- \"open_science\" - Open, collaborative, replicable research (0-100%): This covers several considerations: \n\t\t- Replicability, reproducibility, data integrity: Would another researcher be able to perform the same analysis and get the same results? Are the methods explained clearly and in enough detail to enable easy and credible replication? For example, are all analyses and statistical tests explained, and is code provided? Is the source of the data clear? Is the data made as available as is reasonably possible? If so, is it clearly labeled and explained??\n\t\t- Consistency: Do the numbers in the paper and/or code output make sense? Are they internally consistent throughout the paper?  \n\t\t- Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?\n\n\t- \"global_relevance\" - Relevance to global priorities, usefulness for practitioners: Are the paper’s chosen topic and approach likely to be useful to global priorities, cause prioritization, and high-impact interventions? Does the paper consider real-world relevance and deal with policy and implementation questions? Are the setup, assumptions, and focus realistic? Do the authors report results that are relevant to practitioners? Do they provide useful quantified estimates (costs, benefits, etc.) enabling practical impact quantification and prioritization? Do they communicate (at least in the abstract or introduction)  in ways policymakers and decision-makers can understand, without misleading or oversimplifying?\n\n\nThe midpoint and 'credible intervals': expressing uncertainty - What are we looking for and why? \n\t- We want policymakers, researchers, funders, and managers to be able to use The Unjournal'&#x73; evaluations to update their beliefs and make better decisions. To do this well, they need to weigh multiple evaluations against each other and other sources of information. Evaluators may feel confident about their rating for one category, but less confident in another area. How much weight should readers give to each? In this context, it is useful to quantify the uncertainty. But it's hard to quantify statements like \"very certain\" or \"somewhat uncertain\" – different people may use the same phrases to mean different things. That's why we're asking for you a more precise measure, your credible intervals. These metrics are particularly useful for meta-science and meta-analysis. You are asked to give a 'midpoint' and a 90% credible interval. Consider this as the smallest interval that you believe is 90% likely to contain the true value.\n\t- How do I come up with these intervals? (Discussion and guidance): You may understand the concepts of uncertainty and credible intervals, but you might be unfamiliar with applying them in a situation like this one. You may have a certain best guess for the \"Methods...\" criterion. Still, even an expert can never be certain. E.g., you may misunderstand some aspect of the paper, there may be a method you are not familiar with, etc. Your uncertainty over this could be described by some distribution, representing your beliefs about the true value of this criterion. Your \"'best guess\" should be the central mass point of this distribution. For some questions, the \"true value\" refers to something objective, e.g. will this work be published in a top-ranked journal? In other cases, like the percentile rankings, the true value means \"if you had complete evidence, knowledge, and wisdom, what value would you choose?\" If you are well calibrated your 90% credible intervals should contain the true value 90% of the time. Consider the midpoint as the 'median of your belief distribution'\n\t- We also ask for the 'midpoint', the center dot on that slider. Essentially, we are asking for the median of your belief distribution. By this we mean the percentile ranking such that you believe \"there's a 50% chance that  the paper's true rank is higher than this, and a 50% chance that it actually ranks lower than this.\"\n\n\nAdditionally, we ask: What journal ranking tier should and will this work be published in?\n\nTo help universities and policymakers make sense of our evaluations, we want to benchmark them against how research is currently judged. So, we would like you to assess the paper in terms of journal rankings. We ask for two assessments:\n\n\t1. a normative judgment about 'how well the research should publish';\n\t2. a prediction about where the research will be published.\n\tAs before, we ask for a 90% credible interval.\n\n\tJournal ranking tiers are on a 0-5 scale, as follows:\n\t\t0/5: \"Won't publish/little to no value\".  Unlikely to be cited by credible researchers\n\t\t1/5: OK/Somewhat valuable journal\n\t\t2/5: Marginal B-journal/Decent field journal\n\t\t3/5: Top B-journal/Strong field journal\n\t\t4/5: Marginal A-Journal/Top field journal\n\t\t5/5: A-journal/Top journal\n\n\t- We encourage you to consider a non-integer score, e.g. 4.6 or 2.2. If a paper/project would be most likely to be (or merits being) published in a journal that would rank about halfway between a top tier 'A journal' and a second tier (4/5) journal, you should rate it a 4.5. Similarly, if you think it has an 80%  chance of (being/meriting) publication in a 'marginal B-journal' and a 20% chance of a Top B-journal, you should rate it 2.2. Please also use this continuous scale for providing credible intervals. If a paper/project would be most likely to be (or merits being) published in a journal that would rank about halfway between a top tier 'A journal' and a second tier (4/5) journal, you should rate it a 4.5.\n\n\t- Journal ranking tier \"should\" (0.0-5.0)\n\t\tSchema: tiershould: Assess this paper on the journal ranking scale described above, considering only its merit, giving some weight to the category metrics we discussed above. Equivalently, where would this paper be published if: \n\t\t1. the journal process was fair, unbiased, and free of noise, and that status, social connections, and lobbying to get the paper published didn’t matter;\n\t\t2. journals assessed research according to the category metrics we discussed above.\n\n\t- Journal ranking tier \"will\" (0.0-5.0) \n\t\tSchema: tierwill: What if this work has already been peer reviewed and published? If this work has already been published, and you know where, please report the prediction you would have given absent that knowledge.\n\nReturn STRICT JSON matching the supplied schema. No preamble. No markdown. No extra text.\n\nFill both top-level keys:\n- `assessment_summary`: about 1000 words.\n- `metrics`: object containing all required metrics.\n\nField names\n- Percentile metrics → `midpoint`, `lower_bound`, `upper_bound`.\n- Tier metrics → `score`, `ci_lower`, `ci_upper`.\n\nBounds\n- Percentiles in [0, 100] with lower_bound ≤ midpoint ≤ upper_bound.\n- Tiers in [0, 5] with ci_lower ≤ score ≤ ci_upper.\n\nDo not include citations, URLs, author identity, or any external information.\n Percentiles in [0, 100] with lower_bound ≤ midpoint ≤ upper_bound.\n- Tiers in [0, 5] with ci_lower ≤ score ≤ ci_upper.\n\nDo not include citations, URLs, author identity, or any external information.\n\"\"\".strip()\n\n# Async-by-default kickoff: submit and return job metadata. No waiting.\ndef evaluate_paper(pdf_path: Union[str, pathlib.Path],\n                   model: Optional[str] = None,\n                   use_reasoning: bool = True) -> Dict[str, Any]:\n    model = model or MODEL\n    fid = get_file_id(pdf_path, client)\n\n    def _payload():\n        p = dict(\n            model=model,\n            text={\"format\": TEXT_FORMAT_COMBINED},\n            input=[\n                {\"role\": \"system\", \"content\": [\n                    {\"type\": \"input_text\", \"text\": SYSTEM_PROMPT_COMBINED}\n                ]},\n                {\"role\": \"user\", \"content\": [\n                    {\"type\": \"input_file\", \"file_id\": fid},\n                    {\"type\": \"input_text\", \"text\": \"Return STRICT JSON per schema. No extra text.\"}\n                ]},\n            ],\n            max_output_tokens=12000,\n            background=True,\n            store=True,\n        )\n        if use_reasoning:\n            p[\"reasoning\"] = {\"effort\": \"high\", \"summary\": \"auto\"}\n        return p\n\n    kickoff = call_with_retries(lambda: client.responses.create(**_payload()))\n    kd = _resp_as_dict(kickoff)\n    return {\n        \"response_id\": kd.get(\"id\"),\n        \"file_id\": fid,\n        \"status\": kd.get(\"status\") or \"queued\",\n        \"model\": model,\n        \"created_at\": kd.get(\"created_at\"),\n    }\n```\n:::\n\n\n\nRelying on GPT-5 Pro, we use a single‑step call with a reasoning model that supports file input. One step avoids hand‑offs and summary loss from a separate \"ingestion\" stage. The model reads the whole PDF and produces the JSON defined above. We do not retrieve external sources or cross‑paper material for these scores; the evaluation is anchored in the manuscript itself.\n\nThe Python pipeline uploads each PDF once and caches the returned file id keyed by path, size, and modification time. We submit one background job per PDF to the OpenAI Responses API with “high” reasoning effort and server‑side JSON‑Schema enforcement. Submissions record the response id, model id, file id, status, and timestamps. \n\n\n\n::: {.cell}\n\n```{.python .cell-code  code-summary=\"Kick off background jobs → results/jobs_index.csv\"}\nimport pathlib, time\n\nROOT = pathlib.Path(os.getenv(\"UJ_PAPERS_DIR\", \"papers\")).expanduser()\nOUT  = pathlib.Path(\"results\"); OUT.mkdir(exist_ok=True)\nIDX  = OUT / \"jobs_index.csv\"\n\npdfs = sorted(ROOT.glob(\"*.pdf\"))\nprint(\"Found PDFs:\", [p.name for p in pdfs])\n\ncols = [\"paper\",\"pdf\",\"response_id\",\"file_id\",\"model\",\"status\",\"created_at\",\"last_update\",\"collected\",\"error\"]\nidx = read_csv_or_empty(IDX, columns=cols)\nfor c in cols:\n    if c not in idx.columns: idx[c] = pd.NA\n\nexisting = dict(zip(idx[\"paper\"], idx[\"status\"])) if not idx.empty else {}\nstarted = []\n\nfor pdf in pdfs:\n    paper = pdf.stem\n    if existing.get(paper) in (\"queued\",\"in_progress\",\"incomplete\",\"requires_action\"):\n        print(f\"skip {pdf.name}: job already running\")\n        continue\n    try:\n        job = evaluate_paper(pdf, model=MODEL, use_reasoning=True)\n        started.append({\n            \"paper\": paper,\n            \"pdf\": str(pdf),\n            \"response_id\": job.get(\"response_id\"),\n            \"file_id\": job.get(\"file_id\"),\n            \"model\": job.get(\"model\"),\n            \"status\": job.get(\"status\"),\n            \"created_at\": job.get(\"created_at\") or pd.Timestamp.utcnow().isoformat(),\n            \"last_update\": pd.Timestamp.utcnow().isoformat(),\n            \"collected\": False,\n            \"error\": pd.NA,\n        })\n        print(f\"✓ Started job for {pdf.name}, waiting 90s before next submission...\")\n        time.sleep(90)  # Wait 90s between submissions to avoid TPM rate limits\n    except Exception as e:\n        print(f\"⚠️ kickoff failed for {pdf.name}: {e}\")\n\nif started:\n    idx = pd.concat([idx, pd.DataFrame(started)], ignore_index=True)\n    idx.drop_duplicates(subset=[\"paper\"], keep=\"last\", inplace=True)\n    idx.to_csv(IDX, index=False)\n    print(f\"Started {len(started)} jobs → {IDX}\")\nelse:\n    print(\"No new jobs started.\")\n```\n:::\n\n\nA separate script polls job status and, for each completed job, retrieves the raw response, extracts the first balanced top‑level JSON object, and writes both the raw response and parsed outputs to disk.\n\n\n::: {.cell}\n\n```{.python .cell-code  code-summary=\"Poll status, collect completed outputs, write per-paper and combined CSVs\"}\nimport json, pathlib, pandas as pd\n\nOUT = pathlib.Path(\"results\")\nIDX = OUT / \"jobs_index.csv\"\nPER = OUT / \"per_paper\"; PER.mkdir(exist_ok=True)\nJSN = OUT / \"json\"; JSN.mkdir(exist_ok=True)\n\ndef _safe_read_csv(path, columns):\n    p = pathlib.Path(path)\n    if not p.exists():\n        return pd.DataFrame(columns=columns)\n    try:\n        # Set dtype='object' for string columns to avoid dtype warnings\n        df = pd.read_csv(p, dtype={'error': 'object', 'reasoning_id': 'object'})\n    except Exception:\n        return pd.DataFrame(columns=columns)\n    for c in columns:\n        if c not in df.columns:\n            df[c] = pd.NA\n    return df\n\ncols = [\"paper\",\"pdf\",\"response_id\",\"file_id\",\"model\",\"status\",\"created_at\",\n        \"last_update\",\"collected\",\"error\",\"reasoning_id\",\"input_tokens\",\n        \"output_tokens\",\"reasoning_tokens\",\"reasoning_summary\"]\n\nidx = _safe_read_csv(IDX, cols)\n\nif idx.empty:\n    print(\"Index is empty.\")\nelse:\n    term = {\"completed\",\"failed\",\"cancelled\",\"expired\"}\n    for i, row in idx.iterrows():\n        if str(row.get(\"status\")) in term:\n            continue\n        try:\n            r = client.responses.retrieve(str(row[\"response_id\"]))\n            d = _resp_as_dict(r)\n            idx.at[i,\"status\"] = d.get(\"status\")\n            idx.at[i,\"last_update\"] = pd.Timestamp.utcnow().isoformat()\n            if d.get(\"status\") in term and d.get(\"status\") != \"completed\":\n                idx.at[i,\"error\"] = json.dumps(d.get(\"incomplete_details\") or {})\n        except Exception as e:\n            idx.at[i,\"error\"] = str(e)\n\n    newly_done = idx[(idx[\"status\"]==\"completed\") & (idx[\"collected\"]==False)]\n    print(f\"Completed and pending collection: {len(newly_done)}\")\n\n    rows_accum, summaries = [], []\n    for i, row in newly_done.iterrows():\n        rid   = str(row[\"response_id\"])\n        paper = str(row[\"paper\"])\n        try:\n            r = client.responses.retrieve(rid)\n\n            with open(JSN / f\"{paper}.response.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(_resp_as_dict(r), ensure_ascii=False))\n\n            jtxt = _get_output_text(r)\n            j    = _extract_json(jtxt)\n\n            for metric, vals in (j.get(\"metrics\") or {}).items():\n                if metric in (\"tier_should\",\"tier_will\"):\n                    rows_accum.append({\n                        \"paper\": paper, \"metric\": metric, \"metric_type\": \"tier\",\n                        \"value\": vals.get(\"score\"), \"lo\": vals.get(\"ci_lower\"), \"hi\": vals.get(\"ci_upper\"),\n                        \"scale_min\": 0, \"scale_max\": 5,\n                    })\n                else:\n                    rows_accum.append({\n                        \"paper\": paper, \"metric\": metric, \"metric_type\": \"percentile\",\n                        \"value\": vals.get(\"midpoint\"), \"lo\": vals.get(\"lower_bound\"), \"hi\": vals.get(\"upper_bound\"),\n                        \"scale_min\": 0, \"scale_max\": 100,\n                    })\n\n            if \"assessment_summary\" in j:\n                summaries.append({\"paper\": paper, \"assessment_summary\": j[\"assessment_summary\"]})\n\n            per_df = pd.DataFrame([r for r in rows_accum if r[\"paper\"]==paper])\n            per_df.to_csv(PER / f\"{paper}_long.csv\", index=False, encoding=\"utf-8\")\n\n            m = _reasoning_meta(r)\n            idx.at[i,\"collected\"] = True\n            idx.at[i,\"error\"] = pd.NA\n            idx.at[i,\"reasoning_id\"] = m.get(\"reasoning_id\")\n            idx.at[i,\"input_tokens\"] = m.get(\"input_tokens\")\n            idx.at[i,\"output_tokens\"] = m.get(\"output_tokens\")\n            idx.at[i,\"reasoning_tokens\"] = m.get(\"reasoning_tokens\")\n            idx.at[i,\"reasoning_summary\"] = m.get(\"reasoning_summary\")\n\n        except Exception as e:\n            idx.at[i,\"error\"] = f\"collect: {e}\"\n\n    if rows_accum:\n        combined = pd.DataFrame(rows_accum)\n\n        # merge with any previous combined_long.csv\n        comb_path = OUT / \"combined_long.csv\"\n        prev_cols = [\"paper\",\"metric\",\"metric_type\",\"value\",\"lo\",\"hi\",\"scale_min\",\"scale_max\"]\n        prev = _safe_read_csv(comb_path, prev_cols)\n        if not prev.empty:\n            prev = prev[~prev[\"paper\"].isin(newly_done[\"paper\"])]\n            combined = pd.concat([prev, combined], ignore_index=True)\n        combined.to_csv(comb_path, index=False, encoding=\"utf-8\")\n\n        # metrics_long.csv (no leading-dot chaining)\n        metrics_df = combined[combined[\"metric_type\"]==\"percentile\"].copy()\n        metrics_df = metrics_df.rename(columns={\"value\":\"midpoint\",\"lo\":\"lower_bound\",\"hi\":\"upper_bound\"})\n        metrics_df = metrics_df.drop(columns=[\"metric_type\",\"scale_min\",\"scale_max\"])\n        metrics_df.to_csv(OUT / \"metrics_long.csv\", index=False, encoding=\"utf-8\")\n\n        # tiers_long.csv\n        tiers_df = combined[combined[\"metric_type\"]==\"tier\"].copy()\n        tiers_df = tiers_df.rename(columns={\"metric\":\"tier_kind\",\"value\":\"score\"})\n        tiers_df = tiers_df.drop(columns=[\"metric_type\",\"scale_min\",\"scale_max\"])\n        tiers_df.to_csv(OUT / \"tiers_long.csv\", index=False, encoding=\"utf-8\")\n\n    # assessment_summaries.csv\n    if summaries:\n        s_path = OUT / \"assessment_summaries.csv\"\n        s_df = pd.DataFrame(summaries)\n        prev_s = _safe_read_csv(s_path, [\"paper\",\"assessment_summary\"])\n        if not prev_s.empty:\n            prev_s = prev_s[~prev_s[\"paper\"].isin(newly_done[\"paper\"])]\n            s_df = pd.concat([prev_s, s_df], ignore_index=True)\n        s_df.to_csv(s_path, index=False, encoding=\"utf-8\")\n\n    idx.to_csv(IDX, index=False)\n    counts = idx[\"status\"].value_counts(dropna=False).to_dict()\n    print(\"Status counts:\", counts)\n    print(f\"Progress: {counts.get('completed',0)}/{len(idx)} completed\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code  code-summary=\"Build one-row-per-paper master table (CSV/Parquet/Excel)\"}\nimport pathlib\nimport json\nimport pandas as pd\n\nOUT = pathlib.Path(\"results\")\nOUT.mkdir(exist_ok=True)\nIDX = OUT / \"jobs_index.csv\"\n\n# Base index (one row per paper / job)\nidx_cols = [\n    \"paper\",\"pdf\",\"response_id\",\"file_id\",\"model\",\"status\",\n    \"created_at\",\"last_update\",\"collected\",\"error\",\n    \"reasoning_id\",\"input_tokens\",\"output_tokens\",\"reasoning_tokens\",\n    \"reasoning_summary\",  # may be all-NA if you haven’t added it yet\n]\nidx = read_csv_or_empty(IDX, columns=idx_cols)\nif idx.empty:\n    print(\"jobs_index.csv is empty; nothing to build.\")\nelse:\n    # Long → wide: percentile metrics\n    metrics_path = OUT / \"metrics_long.csv\"\n    metrics = read_csv_or_empty(\n        metrics_path,\n        columns=[\"paper\",\"metric\",\"midpoint\",\"lower_bound\",\"upper_bound\"],\n    )\n    if not metrics.empty:\n        m_wide = (\n            metrics\n            .pivot(index=\"paper\",\n                   columns=\"metric\",\n                   values=[\"midpoint\",\"lower_bound\",\"upper_bound\"])\n        )\n        # Flatten MultiIndex: (stat, metric) → \"{metric}_{stat}\"\n        m_wide.columns = [\n            f\"{metric}_{stat}\" for stat, metric in m_wide.columns\n        ]\n        m_wide = m_wide.reset_index()\n    else:\n        m_wide = pd.DataFrame(columns=[\"paper\"])\n\n    # Long → wide: tier metrics\n    tiers_path = OUT / \"tiers_long.csv\"\n    tiers = read_csv_or_empty(\n        tiers_path,\n        columns=[\"paper\",\"tier_kind\",\"score\",\"lo\",\"hi\"],\n    )\n    if not tiers.empty:\n        t_wide = (\n            tiers\n            .pivot(index=\"paper\",\n                   columns=\"tier_kind\",\n                   values=[\"score\",\"lo\",\"hi\"])\n        )\n        t_wide.columns = [\n            f\"{tier}_{stat}\" for stat, tier in t_wide.columns\n        ]\n        t_wide = t_wide.reset_index()\n    else:\n        t_wide = pd.DataFrame(columns=[\"paper\"])\n\n    # Assessment summaries\n    summaries_path = OUT / \"assessment_summaries.csv\"\n    summaries = read_csv_or_empty(\n        summaries_path,\n        columns=[\"paper\",\"assessment_summary\"],\n    )\n\n    # Merge everything to one row per paper\n    wide = (\n        idx\n        .merge(m_wide, on=\"paper\", how=\"left\")\n        .merge(t_wide, on=\"paper\", how=\"left\")\n        .merge(summaries, on=\"paper\", how=\"left\")\n    )\n\n    # OPTIONAL: if you did NOT store reasoning_summary in jobs_index,\n    # you can fill a short summary from the stored JSON once per paper.\n    if wide[\"reasoning_summary\"].isna().all():\n        JSN = OUT / \"json\"\n        def _reasoning_summary_from_json(paper):\n            jpath = JSN / f\"{paper}.response.json\"\n            if not jpath.exists():\n                return pd.NA\n            try:\n                with open(jpath, \"r\", encoding=\"utf-8\") as f:\n                    d = json.load(f)\n                out = d.get(\"output\") or []\n                if out and isinstance(out, list) and out[0].get(\"type\") == \"reasoning\":\n                    summ = out[0].get(\"summary\") or []\n                    if summ and isinstance(summ, list):\n                        return summ[0].get(\"text\")\n            except Exception:\n                return pd.NA\n            return pd.NA\n\n        wide[\"reasoning_summary\"] = wide[\"paper\"].apply(_reasoning_summary_from_json)\n\n    # Tidy column order: metadata → metrics → tiers → text\n    meta_cols = [\n        \"paper\",\"pdf\",\"model\",\"response_id\",\"file_id\",\"status\",\n        \"created_at\",\"last_update\",\"collected\",\"error\",\n        \"input_tokens\",\"output_tokens\",\"reasoning_tokens\",\"reasoning_id\",\n        \"reasoning_summary\",\n    ]\n    meta_cols = [c for c in meta_cols if c in wide.columns]\n\n    # All remaining numeric/text columns\n    other_cols = [c for c in wide.columns if c not in meta_cols]\n    wide = wide[meta_cols + other_cols]\n\n    # Write outputs\n    master_csv   = OUT / \"llm_evals_master.csv\"\n    # master_parq  = OUT / \"llm_evals_master.parquet\"\n    master_xlsx  = OUT / \"llm_evals_master.xlsx\"\n\n    wide.to_csv(master_csv, index=False)\n    # wide.to_parquet(master_parq, index=False)\n\n    # try:\n    #     wide.to_excel(master_xlsx, index=False)\n    # except ImportError:\n    #     # Excel is nice-to-have; fail soft if openpyxl/xlsxwriter isn’t installed\n    #     print(\"Excel writer not available; skipped llm_evals_master.xlsx\")\n\n    print(\"Master table written to:\")\n    print(f\"  - {master_csv}\")\n    # print(f\"  - {master_parq}\")\n    # print(f\"  - {master_xlsx} (if Excel writer is installed)\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}