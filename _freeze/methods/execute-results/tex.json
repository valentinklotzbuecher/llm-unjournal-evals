{
  "hash": "621e7d5089de8f65a6e7141840f7e82a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data and methods\"\n---\n\n::: {.cell}\n\n:::\n\n\n\nWe draw on two main sources:\n\n1) Human evaluations from [The Unjournal’s public evaluation data](https://unjournal.github.io/unjournaldata/index.html) (PubPub reports and the Coda evaluation form export).  \n2) LLM‑generated evaluations using a structured JSON‑schema prompt with `gpt-5-pro-2025-10-06` (knowledge cut-off: 30 September 2024).\n\n## Unjournal.org evaluations\n\nWe use The Unjournal's public data for a baseline comparison. At The Unjournal each paper is typically evaluated (aka 'reviewed') by two expert evaluators^[Occasionally they use 1 or 3 evaluators.] who provide quantitative ratings on a 0–100 percentile scale for each of seven criteria (with 90% credible intervals),^[See their guidelines [here](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics); these criteria include \"Overall assessment\", \"Claims, strength and characterization of evidence\", \"Methods: Justification, reasonableness, validity, robustness\", \"Advancing knowledge and practice\", \"Logic and communication\", \"Open, collaborative, replicable science\", and \"Relevance to global priorities, usefulness for practitioners\"] two \"journal tier\" ratings on a 0.0 - 5.0 scale,^[\"a normative judgment about 'how well the research should publish'\" and \"a prediction about where the research will be published\"] a written evaluation (resembling a referee report for a journal), and identification and assessment of the paper's \"main claim\". For our initial analysis, we extracted these human ratings and aggregated them, taking the average score per criterion across evaluators (and noting the range of individual scores). \n\nAll papers have completed The Unjournal's evaluation process (meaning the authors received a full evaluation on the Unjournal platform, which has been publicly posted at unjournal.pubpub.org). The sample includes papers spanning 2017–2025 working papers in development economics, growth, health policy, environmental economics, and related fields that The Unjournal identified as high-impact. Each of these papers has quantitative scores from at least one human evaluator, and many have multiple (2-3) human ratings.\n\n## LLM-based evaluation\n\n### Quantitative ratings and journal-ranking tiers\n\n\nFollowing The Unjournal's [standard guidelines for evaluators](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators) and their [academic evaluation form](https://coda.io/form/Unjournal-Evaluation-form-academic-stream-Coda-updated-version_dGjfMZ1yXME), evaluators are asked to consider each paper along the following dimensions: **claims & evidence**, **methods**, **logic & communication**, **open science**, **global relevance**, and an **overall** assessment. Ratings are interpreted as percentiles relative to serious recent work in the same area. For each metric, evaluators are asked for the midpoint of their beliefs and their 90% credible interval, to communicate their uncertainty.\nFor the journal rankings measure, we ask both \"what journal ranking tier should this work be published in? (0.0-5.0)\" and \"what journal ranking tier will this work be published in? (0.0-5.0)\", with some further explanation.The full prompt can be seen in the code below -- essentially copied from the Unjournal's guidelines page.\n\n<!-- *Internal note: The Python content below must be run externally* -->\n\nWe captured the versions of each paper that was evaluated by The Unjournal's human evaluators, downloading from the links provided in The Unjournal's Coda database. <!-- Add link to this -- is it correct? Are we sure these are the ones evaluated? --> \n\nWe evaluate each paper by passing the PDF directly to the model and requiring a strict, machine‑readable JSON output. This keeps the assessment tied to the document the authors wrote. Direct ingestion preserves tables, figures, equations, and sectioning, which ad‑hoc text scraping can mangle. It also avoids silent trimming or segmentation choices that would bias what the model sees.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nWe enforce a JSON Schema for the results. The model must return one object for each of the named criteria including a midpoint rating and a 90% interval for each rating. This guarantees that every paper is scored on the same fields with the same types and bounds. It makes the analysis reproducible and comparisons clean. \n\nWe request credible intervals (as we do for human evaluators) to allow the model to communicate its uncertainty rather than suggest false precision; these can also be incorporated into our metrics, penalizing a model's inaccuracy more when it's stated with high confidence.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nRelying on GPT-5 Pro, we use a single‑step call with a reasoning model that supports file input. One step avoids hand‑offs and summary loss from a separate \"ingestion\" stage. The model reads the whole PDF and produces the JSON defined above. We do not retrieve external sources or cross‑paper material for these scores; the evaluation is anchored in the manuscript itself.\n\nThe Python pipeline uploads each PDF once and caches the returned file id keyed by path, size, and modification time. We submit one background job per PDF to the OpenAI Responses API with “high” reasoning effort and server‑side JSON‑Schema enforcement. Submissions record the response id, model id, file id, status, and timestamps. \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nA separate script polls job status and, for each completed job, retrieves the raw response, extracts the first balanced top‑level JSON object, and writes both the raw response and parsed outputs to disk.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}