{
  "hash": "98c9f5de81f26f891bd59cbe08774b9c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Results\"\nformat: \n  html: default\n  # pdf: default\nengine: knitr\n--- \n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nHere we present preliminary results, starting with a comparison of the LLM‑generated quantitative ratings (model: `gpt-5-pro`, see the [previous section](methods.qmd) with human evaluations across [the Unjournal's criteria](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#undefined-1). \n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Quantitative comparison: human vs. GPT‑5 Pro\n\nWe first use the earlier GPT‑5 Pro evaluation run that covered all papers in our Unjournal sample with a simpler JSON‑schema prompt. @fig-forest-overall shows the overall percentile ratings from this initial run, averaged across human evaluators and compared to the LLM’s “overall” scores for each paper; @fig-forest-tiers-should the journal tier ratings.\nThis October 2025 run asked the model only for numeric ratings and journal‑tier scores (no diagnostic summary or reasoning trace); the richer reasoning runs described below were conducted later on a subset of papers and may yield different point estimates for the same criteria.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of Human vs LLM overall percentile ratings](results_files/figure-pdf/fig-forest-overall-1.pdf){#fig-forest-overall}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of Human vs LLM journal tier ratings (should be published in)](results_files/figure-pdf/fig-forest-tiers-should-1.pdf){#fig-forest-tiers-should}\n:::\n:::\n\n\n\n@fig-heatmap-human-minus-llm shows a heatmap of the differences between human and LLM mean ratings across all evaluation criteria. Positive values (in green) indicate that humans rated the paper higher than the LLM, while negative values (in orange) indicate the opposite.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Heatmap of Human minus LLM mean ratings across evaluation criteria](results_files/figure-pdf/fig-heatmap-human-minus-llm-1.pdf){#fig-heatmap-human-minus-llm}\n:::\n:::\n\n\n\n\n\n\\clearpage\n\n## Qualitative comparison: detailed GPT‑5 Pro evaluations\n\nTo understand what GPT‑5 Pro is actually responding to, we re‑ran the model on four focal papers [@Adena2024; @Peterman2024; @Green2025; @Williams2024] using a refined prompt (as shown in the [previous section](methods.qmd)).\n\nThis second run keeps the same quantitative metrics but additionally requires a diagnostic summary of about 1,000 words and high‑effort reasoning, with the full reasoning trace returned by the “thinking” model. For each paper we can therefore inspect:\n\n- the LLM’s quantitative scores and journal‑tier predictions,\n- the hidden reasoning steps used to arrive at those scores, and\n- the token usage and approximate API cost of the evaluation.\n\n@tbl-llm-token-cost-summary summarizes the token usage and estimated cost of each of these inferences; the full assessments and reasoning traces are shown in the Appendix.\n\nWe start by examining selected evaluations in detail. In the next step we will juxtapose these LLM assessments with the human evaluators’ written reports.\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-llm-token-cost-summary .cell tbl-cap='Estimated token usage and cost of GPT-5 Pro evaluations'}\n::: {.cell-output-display}\n\n\n|Paper                | Input tokens| Output tokens| Reasoning tokens| Total tokens| Est. cost (USD)|\n|:--------------------|------------:|-------------:|----------------:|------------:|---------------:|\n|Williams et al. 2024 |        28890|          6795|             5056|        35685|            1.86|\n|Green et al. 2025    |        23090|          6938|             5440|        30028|            1.83|\n|Adena and Hager 2024 |        24420|          6211|             4544|        30631|            1.66|\n|Peterman et al. 2025 |        18948|          6091|             4480|        25039|            1.55|\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n### Case study: Williams et al. (2024)\n\n\nGPT‑5 Pro assigns an overall percentile of 86/100, with relatively high scores for claims and evidence (78), methods (74), advancing knowledge (92), logic and communication (86), open science (78), and global relevance (94). It predicts that the paper deserves publication in a high‑tier journal (tier‑should 4.4/5) and that it will, in practice, be published at roughly the same level (tier‑will 4.8/5). The two Unjournal evaluators, by contrast, each give an overall rating of 50/100 and much lower scores for methods and claims (20 and 5 for both criteria, for each evaluator), while still assigning relatively high journal‑tier ratings (around 3.5–4.5/5).\n\nThe qualitative content of the GPT‑5 Pro diagnostic summary overlaps strongly with the human evaluations. In its narrative assessment, the model highlights six main issues: reliance on the Global Forest Change (GFC) regrowth layer and the limited validation of this product; temporal “data leakage” between the multi‑year regrowth label and contemporaneous predictors; incomplete or optimistic treatment of uncertainty around the headline 215 Mha estimate; a broad and permissive definition of land “available for natural regeneration”; limitations of the carbon overlay and permanence assumptions; and only partial openness of code and workflows, which increases barriers to full replication.\n\nThe Unjournal synthesis and individual evaluations describe a similar set of concerns. The joint synthesis report states that “methodological concerns fundamentally challenge the validity and utility of the central 215 Mha estimate,” and points to data leakage from contemporaneous predictors, confounding by socioeconomic factors and predictor choice, and discrepancies between the estimated magnitude and other reference information, together with a lack of historical validation. It also flags the narrow uncertainty intervals, the treatment of “biophysical potential” versus business‑as‑usual regrowth, additionality and opportunity costs in the policy framing, and the short‑term ephemerality of many regrowth events.\n\nIssue by issue, the overlap is quite direct. On the regrowth data, both GPT‑5 Pro and the evaluators stress that the central 215 Mha estimate rests on a regrowth layer with non‑trivial classification errors and incomplete documentation, and that this undermines confidence in the magnitude and spatial pattern of the estimates. On temporal structure, both sides describe leakage from contemporaneous predictors into the regrowth classification and note that this can yield over‑optimistic performance metrics. On uncertainty, both the model’s summary and the human reports state that the published intervals around the 215 Mha and carbon estimates are narrow relative to plausible sources of error and do not fully propagate uncertainty from the regrowth classification, land‑availability masks, carbon scaling, and future land‑use dynamics. The definition of “available for natural regeneration” is described in both as broad or permissive, with potential for misinterpretation when mapped against policy‑relevant areas. Finally, both note that, although the inputs and gridded outputs are open, code availability and the complexity of pre‑processing steps limit full reproducibility.\n\nThere are also points that appear mainly on one side. GPT‑5 Pro devotes more attention to technical implementation details such as the mismatch between coarse predictors and 30‑m outputs, the need for spatial cross‑validation that further reduces spatial leakage, the lack of spatially explicit error budgets for countries and biomes, and the absence of dynamic scenarios that vary climate and human pressures. The human evaluators more strongly emphasise distinctions between business‑as‑usual regrowth and “additional” potential, the implications of additionality and opportunity costs for policy claims, and the short‑term persistence of regrowth for long‑run carbon sequestration.\n\nOverall, the Williams case shows that GPT‑5 Pro’s written assessment and the Unjournal evaluations discuss largely the same cluster of methodological and interpretive issues and often use similar language to describe them, while the numerical ratings for methods, claims, and overall quality differ substantially.\n\n<!-- ```{r} -->\n<!-- #| label: tbl-llm-williams-metrics -->\n<!-- #| tbl-cap: \"GPT‑5 Pro percentile ratings for Williams et al. (2024)\" -->\n<!-- #| results: 'asis' -->\n\n<!-- williams_metrics |> -->\n<!-- mutate(across(c(Midpoint, Lower_90, Upper_90), round, 1)) |> -->\n<!-- knitr::kable( -->\n<!-- col.names = c(\"Criterion\", \"Midpoint\", \"Lower 90%\", \"Upper 90%\"), -->\n<!-- align = c(\"l\",\"c\",\"c\",\"c\") -->\n<!-- ) |> -->\n<!-- kableExtra::kable_styling(full_width = FALSE) -->\n\n<!-- ``` -->\n\n\n<!-- ```{r} -->\n<!-- #| label: tbl-llm-williams-tiers -->\n<!-- #| tbl-cap: \"GPT‑5 Pro journal tier ratings for Williams et al. (2024)\" -->\n<!-- #| results: 'asis' -->\n\n<!-- williams_tiers |> -->\n<!--   mutate(across(c(Score, Lower_90, Upper_90), round, 2)) |> -->\n<!--   knitr::kable( -->\n<!--     col.names = c(\"Measure\", \"Score\", \"Lower 90%\", \"Upper 90%\"), -->\n<!--     align = c(\"l\",\"c\",\"c\",\"c\") -->\n<!--   ) |> -->\n<!--   kableExtra::kable_styling(full_width = FALSE) -->\n\n\n\n<!-- ``` -->\n\n### Further case studies\n\nBeyond @Williams2024, the refined GPT‑5 Pro run produces similarly detailed assessments for the other three focal papers (summaries in Appendix). For @Adena2024 (published later as @Adena2025), the model highlights the strength of the nationwide geo‑randomized Facebook experiment and the use of total donations across channels, while flagging several internal threats: sparse and heterogeneous exposure that is only analysed as an ITT effect, potential spillovers and spatial correlation across postal codes, the absence of a clean treatment‑on‑the‑treated estimate, and the sensitivity of return‑on‑investment calculations to assumptions about donor lifetime value and unobserved costs. It also notes that code and data are not fully open, limiting replication despite preregistration and extensive appendices. \n\nFor @Green2025, the model emphasises the conceptual and open‑science strengths of the meta‑analysis (restriction to RCTs with behavioural outcomes, robust‑variance estimation, careful publication‑bias checks, reproducible R code and containers) but raises concerns about several technical choices: post‑hoc inclusion decisions, single‑coder data extraction, the ad‑hoc imputation of “unspecified null” results as a small positive effect, reliance on Glass’s Δ rather than Hedges’ g, incomplete description of how binary outcomes and cluster‑randomized trials are converted to standardized mean differences, and limited reporting of assumptions in the RVE and selection‑model steps. \n\nFor @Peterman2024, GPT‑5 Pro again recognises a large, preregistered synthesis using RVE on more than 1,300 effects from 93 RCTs of social safety nets, but stresses that heterogeneity is very high and that pooled averages are reported without prediction intervals. It notes that diverse constructs are collapsed into a single Hedges’ g scale, that dependence structures and RVE correlation assumptions are only briefly described, that large multi‑arm studies may be overweighted, and that some internal inconsistencies (e.g. repeated but differing summary statistics) and the lack of a clear risk‑of‑bias synthesis or public extraction template limit interpretability and reproducibility. \n\nTaken together, these three cases suggest that GPT‑5 Pro is capable of reading complex applied work, reconstructing the main design and estimation strategy, and surfacing quite specific methodological and reporting issues. In the next stage of the project we plan to compare these model‑generated critiques more systematically to the corresponding Unjournal evaluations, including issue‑level adjudication by independent experts, to assess which problems are shared, which are missed by one side, and how often the model’s concerns are judged substantively correct.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}