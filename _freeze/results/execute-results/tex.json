{
  "hash": "baf94faf1122ccadad31066e348f5ae6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Results\"\nformat: \n  html: default\n  # pdf: default\nengine: knitr\n--- \n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nHere we present preliminary results, starting with a comparison of the LLM‑generated quantitative ratings (model: `gpt-5-pro`, see the[(previous section](methods.qmd)) with human evaluations across [the Unjournal's criteria](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#undefined-1). \n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Quantitative comparison: human vs. GPT‑5 Pro\n\nWe first use the earlier GPT‑5 Pro evaluation run that covered all papers in our Unjournal sample with a simpler JSON‑schema prompt. @fig-forest-overall shows the overall percentile ratings from this initial run, averaged across human evaluators and compared to the LLM’s “overall” scores for each paper.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of Human vs LLM overall percentile ratings](results_files/figure-pdf/fig-forest-overall-1.pdf){#fig-forest-overall}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of Human vs LLM journal tier ratings (should be published in)](results_files/figure-pdf/fig-forest-tiers-should-1.pdf){#fig-forest-tiers-should}\n:::\n:::\n\n\n\n@fig-heatmap-human-minus-llm shows a heatmap of the differences between human and LLM mean ratings across all evaluation criteria. Positive values (in green) indicate that humans rated the paper higher than the LLM, while negative values (in orange) indicate the opposite.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Heatmap of Human minus LLM mean ratings across evaluation criteria](results_files/figure-pdf/fig-heatmap-human-minus-llm-1.pdf){#fig-heatmap-human-minus-llm}\n:::\n:::\n\n\n\n\n\n\\clearpage\n\n## Qualitative comparison: detailed GPT‑5 Pro evaluations\n\nTo understand what GPT‑5 Pro is actually responding to, we re‑ran the model on four focal papers [@Adena2024; @Peterman2024; @Green2025; @Williams2024] using a refined prompt.\n\nThis second run keeps the same quantitative metrics but additionally requires a diagnostic summary of about 1,000 words and high‑effort reasoning, with the full reasoning trace returned by the “thinking” model. For each paper we can therefore inspect:\n\n- the LLM’s quantitative scores and journal‑tier predictions,\n- the hidden reasoning steps used to arrive at those scores, and\n- the token usage and approximate API cost of the evaluation.\n\n@tbl-llm-token-cost-summary summarizes the token usage and estimated cost of each of these inferences.\n\nWe start by examining the Williams et al. (2024) evaluation in detail and then show the analogous summaries for the other four focal papers. In the next step we will juxtapose these LLM assessments with the human evaluators’ written reports.\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-llm-token-cost-summary .cell tbl-cap='Estimated token usage and cost of GPT-5 Pro evaluations'}\n::: {.cell-output-display}\n\n\n|Paper                | Input tokens| Output tokens| Reasoning tokens| Total tokens| Est. cost (USD)|\n|:--------------------|------------:|-------------:|----------------:|------------:|---------------:|\n|Williams et al. 2024 |        28890|          6795|             5056|        35685|            1.86|\n|Green et al. 2025    |        23090|          6938|             5440|        30028|            1.83|\n|Adena and Hager 2024 |        24420|          6211|             4544|        30631|            1.66|\n|Peterman et al. 2025 |        18948|          6091|             4480|        25039|            1.55|\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n### Qualitative comparison: Williams et al. (2024)\n\nIn the refined run, GPT‑5 Pro reads about \n28,704 input tokens and produces \n6,327 visible output tokens plus \n5,120 reasoning tokens.  \nAt current API prices this evaluation costs roughly \n$1.80.\n\nThe table below shows the model’s percentile ratings and 90% credible intervals for the Unjournal criteria.\n\n\n\n\n\n::: {#tbl-llm-williams-metrics .cell tbl-cap='GPT-5 Pro percentile ratings for Williams et al. (2024)'}\n\n\\begin{longtable}[t]{lccc}\n\\toprule\nCriterion & Midpoint & Lower 90\\% & Upper 90\\%\\\\\n\\midrule\nOverall assessment & 86 & 86 & 86\\\\\nClaims \\& evidence & 78 & 78 & 78\\\\\nMethods & 74 & 74 & 74\\\\\nAdvancing knowledge and practice & 92 & 92 & 92\\\\\nLogic and communication & 84 & 84 & 84\\\\\n\\addlinespace\nOpen, collaborative, replicable science & 63 & 63 & 63\\\\\nRelevance to global priorities & 94 & 94 & 94\\\\\n\\bottomrule\n\\end{longtable}\n:::\n\n::: {#tbl-llm-williams-tiers .cell tbl-cap='GPT-5 Pro journal tier ratings for Williams et al. (2024)'}\n\n\\begin{longtable}[t]{lccc}\n\\toprule\nMeasure & Score & Lower 90\\% & Upper 90\\%\\\\\n\\midrule\nDeserved journal tier (should) & 4.4 & 4.4 & 4.4\\\\\nPredicted journal tier (will) & 4.8 & 4.8 & 4.8\\\\\n\\bottomrule\n\\end{longtable}\n:::\n\n\n\n\nIn its narrative assessment, GPT‑5 Pro highlights several concrete limitations. It emphasises that the dependent variable comes from a regrowth map with low producer accuracy and substantial omission error in humid biomes, so the model is trained on a systematically biased labeling of true regrowth. It notes that the random‑forest probabilities are learned under class balancing and are not calibrated to true prevalence, yet are treated as probabilities when computing the 215 Mha regeneration estimate and associated confidence intervals. It warns that validation relies on spatially random sampling rather than block cross‑validation, likely inflating reported accuracy. The model also points to conceptual slippage between “biophysical potential” and “realistic” potential under socio‑economic constraints, incomplete treatment of uncertainty in the area and carbon estimates, and only partially open code despite open data and maps.\n\nThe Unjournal synthesis report and individual human evaluations flag a very similar constellation of problems. The synthesis describes methodological issues that make the 215 Mha headline estimate difficult to reconcile with the underlying regrowth data and with historical benchmarks, stressing concerns about contemporaneous predictors (“data leakage”), confounding between biophysical and socio‑economic factors, and the lack of a hindcast or other validation against historical totals. It also criticises the way accuracy is assessed (including the use of a confusion matrix that mixes past regrowth labels with future‑oriented predictions), the narrow reported confidence intervals, the ambiguous definition of “biophysical potential” from a policy perspective, and the treatment of short‑lived regrowth in long‑run carbon claims.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}