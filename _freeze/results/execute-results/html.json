{
  "hash": "4d7f72325c9b393d61d77c2707ef48c3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Results\"\nformat: \n  html: default\n  # pdf: default\nengine: knitr\n--- \n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Include global setup and parameters\"}\nsource(\"setup_params.R\")\n```\n:::\n\n\n\n\n\nHere we present preliminary results, starting with a comparison of the LLM‑generated quantitative ratings (model: `gpt-5-pro`, see the[(previous section](methods.qmd)) with human evaluations across [the Unjournal's criteria](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#undefined-1). \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\nlibrary(\"janitor\")\nlibrary(\"stringr\")\nlibrary(\"lubridate\")\nlibrary(\"here\") \nlibrary(\"knitr\")\nlibrary(\"kableExtra\")\nlibrary(\"ggforce\") \nlibrary(\"ggrepel\")\nlibrary(\"glue\")\n# library(\"ggalluvial\")\nlibrary(\"scales\")\n# library(\"ggbreak\")\nlibrary(\"jsonlite\")\nlibrary(\"purrr\")\nlibrary(\"tibble\")\n\n\n\nUJ_ORANGE <- \"#f19e4b\"   # LLM\nUJ_GREEN  <- \"#99bb66\"   # Human\nUJ_BLUE <- \"#4e79a7\"   # GPT‑5 (legacy)\n\ntheme_uj <- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      legend.position = \"bottom\"\n    )\n}\n\n# theme_uj <- function(base_size = 11) {\n#   theme_minimal(base_size = base_size) +\n#     theme(\n#       panel.background = element_rect(fill = \"#050608\", colour = NA),\n#       plot.background  = element_rect(fill = \"#050608\", colour = NA),\n#       panel.grid.minor = element_blank(),\n#       panel.grid.major = element_line(colour = \"grey25\"),\n#       axis.text        = element_text(colour = \"grey85\"),\n#       axis.title       = element_text(colour = \"grey95\"),\n#       plot.title       = element_text(colour = \"grey98\"),\n#       legend.background= element_rect(fill = \"#050608\", colour = NA),\n#       legend.key       = element_rect(fill = \"#050608\", colour = NA),\n#       legend.text      = element_text(colour = \"grey90\"),\n#       legend.title     = element_text(colour = \"grey95\"),\n#       plot.title.position = \"plot\",\n#       legend.position  = \"bottom\"\n#     )\n# }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# paper_authors <- read_delim(\"data/paper_authors.csv\", delim = \",\")\n\n# Mapping paper keys - short titles\nUJmap <- read_delim(\"data/UJ_map.csv\", delim = \";\") |>\n  mutate(label_paper_title = research,\n         label_paper = paper) |>\n  select(c(\"label_paper_title\", \"label_paper\"))\n\n\n# Unjournal ratings\nrsx <- read_csv(\"data/rsx_evalr_rating.csv\", show_col_types = FALSE) |> \n  clean_names()  |>\n  mutate(label_paper_title = research) |>\n  select(-c(\"research\"))\n\n\n# UJ evaluated research\nresearch <- read_csv(\"data/research.csv\", show_col_types = FALSE) |>\n  clean_names() |>\n  filter(status == \"50_published evaluations (on PubPub, by Unjournal)\") |>  \n  left_join(UJmap, by = c(\"label_paper_title\")) |>\n  mutate(doi = str_trim(doi)) |>\n  mutate(label_paper = if_else(doi == \"https://doi.org/10.3386/w31162\", \"Walker et al. 2023\", label_paper, missing = label_paper)) |>\n  mutate(label_paper = if_else(doi == \"doi.org/10.3386/w32728\", \"Hahn et al. 2025\", label_paper, missing = label_paper))  |>\n  mutate(label_paper = if_else(doi == \"https://doi.org/10.3386/w30011\", \"Bhat et al. 2022\", label_paper, missing = label_paper))  |>\n  mutate(label_paper = if_else(doi == \"10.1093/wbro/lkae010\", \"Crawfurd et al. 2023\", label_paper, missing = label_paper))  |>\n  left_join(rsx, by = c(\"label_paper_title\"))\n \n\njtiers_llm <- read_csv(\"data/tiers_long.csv\", show_col_types = FALSE) |>\n  mutate(middle_rating = score,\n         lower_ci = lo,\n         upper_ci = hi,\n         criteria = if_else(tier_kind == \"tier_will\", \"journal_predict\", \"merits_journal\"),\n         evaluator = \"gpt-5\",\n         label_paper = paper\n         ) |>\n  select(c(\"label_paper\", \"evaluator\", \"middle_rating\", \"lower_ci\", \"upper_ci\" , \"criteria\", \"rationale\"))\n\n\njtiers_uj <- research |>\n  filter(criteria== \"merits_journal\" | criteria == \"journal_predict\") |>\n  mutate(paper = label_paper,\n         rationale = \"\")  |>\n  select(c(\"label_paper\", \"evaluator\", \"middle_rating\", \"lower_ci\", \"upper_ci\" , \"criteria\", \"rationale\"))\n\n\njtiers <- jtiers_uj |>\n  rbind(jtiers_llm) |>\n  mutate(human = if_else(evaluator == \"o3\", \"Human\", \"o3\"),\n         lower_ci = if_else(lower_ci > 10, lower_ci/10, lower_ci))\n\n\n# write_csv(all_ratings, \"data/all_jtiers.csv\")\nwrite_rds(\n  jtiers,\n  \"data/all_jtiers.rds\",\n  compress = \"none\"\n  )\n\n# utilities + clean columns for plotting\n\nlane_offsets_center <- function(m, gap = 0.18) {\n  if (m <= 0) return(numeric(0))\n  if (m == 1) return(0)\n  k <- floor((m - 1)/2)\n  offs <- sort(c(-seq_len(k), 0, seq_len(k))) * gap\n  offs[seq_len(m)]\n}\nlane_offsets_skip0 <- function(m, gap = 0.18) {\n  if (m <= 0) return(numeric(0))\n  if (m == 1) return(gap)\n  k <- ceiling(m/2)\n  offs <- sort(c(-seq_len(k), seq_len(k))) * gap\n  offs[seq_len(m)]\n}\n\njt_use <- jtiers |>\n  transmute(\n    label_paper,\n    criteria,\n    who = if_else(evaluator == \"gpt-5\", \"LLM\", \"Human\"),\n    mid = as.numeric(middle_rating),\n    lo  = suppressWarnings(as.numeric(lower_ci)),\n    hi  = suppressWarnings(as.numeric(upper_ci))\n  ) |>\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(1, pmin(5, lo)), NA_real_),\n    hi = ifelse(is.finite(hi), pmax(1, pmin(5, hi)), NA_real_)\n  )\n\ntier_metric <- \"merits_journal\"  # change \"merits_journal\" to \"journal_predict\" for the other slide\n\n\n\n\ncanon_metric <- function(x) dplyr::recode(\n  x,\n  \"advancing_knowledge\" = \"adv_knowledge\",\n  \"open_science\"        = \"open_sci\",\n  \"logic_communication\" = \"logic_comms\",\n  \"global_relevance\"    = \"gp_relevance\",\n  \"claims_evidence\"     = \"claims\",\n  .default = x\n)\n\n# LLM metrics \nmetrics_llm_full <- readr::read_csv(here(\"data\",\"metrics_long.csv\"), show_col_types = FALSE) |>\n  janitor::clean_names() |>\n  mutate(\n    label_paper = stringr::str_replace(paper, \"et al \", \"et al. \"),\n    criteria    = canon_metric(metric),\n    mid = as.numeric(midpoint),\n    lo  = suppressWarnings(as.numeric(lower_bound)),\n    hi  = suppressWarnings(as.numeric(upper_bound)),\n    who = \"LLM\",\n    evaluator = \"gpt-5\"\n  )\n\n# LLM metrics (for plotting)\nmetrics_llm <- metrics_llm_full |>\n  transmute(label_paper, criteria, who, mid, lo, hi) |>\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(0, pmin(100, lo)), NA_real_),\n    hi = ifelse(is.finite(hi), pmax(0, pmin(100, hi)), NA_real_)\n  )\n\n\n# 1) Build a 1-row-per-title key from `research`\nkey_map <- research %>%\n  transmute(label_paper_title = stringr::str_trim(label_paper_title),\n            label_paper      = label_paper) %>%\n  filter(!is.na(label_paper_title)) %>%\n  distinct(label_paper_title, label_paper) %>%   # remove exact dups\n  group_by(label_paper_title) %>%\n  slice(1) %>%\n  ungroup()\n\n# Optional sanity check: any titles map to >1 label?\n# research %>% distinct(label_paper_title, label_paper) %>%\n#   count(label_paper_title) %>% filter(n > 1)\n\n# 2) Join with relationship assertion\nrsx_research <- rsx %>%\n  mutate(label_paper_title = stringr::str_trim(label_paper_title)) %>%\n  dplyr::left_join(key_map, by = \"label_paper_title\", relationship = \"many-to-one\")\n\n\nmetrics_human <- rsx_research |>\n  mutate(criteria = canon_metric(criteria)) |>\n  filter(criteria %in% c(\"overall\",\"claims\",\"methods\",\"adv_knowledge\",\"logic_comms\",\"open_sci\",\"gp_relevance\")) |>\n  transmute(\n    label_paper,\n    criteria,\n    evaluator,\n    who = \"Human\",\n    mid = as.numeric(middle_rating),\n    lo  = suppressWarnings(as.numeric(lower_ci)),\n    hi  = suppressWarnings(as.numeric(upper_ci))\n  ) |>\n  filter(!is.na(label_paper), !is.na(mid)) |>\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(0, pmin(100, lo)), NA_real_),\n    hi = ifelse(is.finite(hi), pmax(0, pmin(100, hi)), NA_real_)\n  ) %>% \n  # remove exact duplicates (protect against tiny float diffs by rounding first) -- but wait, what if human raters rated identically for a category?\n  mutate(across(c(mid, lo, hi), ~ round(.x, 4))) |>\n  distinct(label_paper, criteria, who, evaluator, mid, lo, hi, .keep_all = FALSE)\n\n\nmetrics_use <- bind_rows(metrics_human, metrics_llm)\n\nmatched_overall <- intersect(\n  metrics_use %>% filter(who==\"LLM\", criteria==\"overall\") %>% pull(label_paper),\n  metrics_use %>% filter(who==\"Human\", criteria==\"overall\") %>% pull(label_paper)\n)\nmetrics_use <- metrics_use %>% mutate(is_matched_overall = label_paper %in% matched_overall)\n```\n:::\n\n\n\n\n\n## Quantitative comparison: human vs. GPT‑5 Pro\n\nWe first use the earlier GPT‑5 Pro evaluation run that covered all papers in our Unjournal sample with a simpler JSON‑schema prompt. @fig-forest-overall shows the overall percentile ratings from this initial run, averaged across human evaluators and compared to the LLM’s “overall” scores for each paper.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhighlight_paper <- \"Kremer et al. 2022 XXX\" # Select here to annotate\n\nmatched <- intersect(\n  metrics_use %>% filter(who==\"LLM\",   criteria==\"overall\") %>% pull(label_paper),\n  metrics_use %>% filter(who==\"Human\", criteria==\"overall\") %>% pull(label_paper)\n)\n\nH_ind <- metrics_human %>%\n  filter(criteria==\"overall\", label_paper %in% matched) %>%\n  mutate(lo = ifelse(is.finite(lo), pmax(0,lo), NA_real_),\n         hi = ifelse(is.finite(hi), pmin(100,hi), NA_real_))\n\nord <- H_ind %>%\n  group_by(label_paper) %>%\n  summarise(h_mean = mean(mid, na.rm=TRUE), .groups=\"drop\") %>%\n  arrange(desc(h_mean)) %>% mutate(pos = row_number())\n\nH_plot <- H_ind %>%\n  inner_join(ord, by=\"label_paper\") %>%\n  group_by(label_paper) %>%\n  mutate(off = (row_number() - (n()+1)/2) * 0.18,\n         x   = pos + off) %>% ungroup()\n\n# per-paper human mean and LLM summary\nH_pp <- H_ind %>% group_by(label_paper) %>% summarise(h_mean = mean(mid), .groups=\"drop\")\nL_c <- metrics_llm %>%\n  filter(criteria==\"overall\", label_paper %in% matched) %>%\n  group_by(label_paper) %>%\n  summarise(mid = mean(mid, na.rm=TRUE),\n            lo  = suppressWarnings(min(coalesce(lo, mid), na.rm=TRUE)),\n            hi  = suppressWarnings(max(coalesce(hi, mid), na.rm=TRUE)),\n            .groups=\"drop\") %>%\n  inner_join(ord, by=\"label_paper\") %>%\n  mutate(x = pos)\n\nlab <- L_c %>% filter(label_paper == highlight_paper)\n\n# overall means to show as horizontal reference lines\nhbar <- mean(H_pp$h_mean, na.rm=TRUE)\nlbar <- mean(L_c$mid,     na.rm=TRUE)\n\nggplot() +\n  geom_vline(data = ord, aes(xintercept = pos), color=\"grey92\", linewidth=0.3) +\n  # mean lines\n  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = \"dotted\", linewidth = 0.8) +\n  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = \"dotted\", linewidth = 0.8) +\n  # humans\n  geom_errorbar(data = subset(H_plot, is.finite(lo)&is.finite(hi)),\n                aes(x=x, ymin=lo, ymax=hi),\n                width=0, linewidth=1, alpha=0.5, color=UJ_GREEN) +\n  geom_point(data = H_plot, aes(x=x, y=mid), size=3.0, alpha=0.9, color=UJ_GREEN) +\n  # LLM\n  geom_errorbar(data = subset(L_c, is.finite(lo)&is.finite(hi)),\n                aes(x=x, ymin=lo, ymax=hi),\n                width=0, linewidth=1.0, color=UJ_ORANGE) +\n  geom_point(data = L_c, aes(x=x, y=mid), size=3.6, shape=18, color=UJ_ORANGE) +\n  # label highlight\n  geom_label_repel(data = lab,\n                   aes(x = x, y = mid, label = label_paper),\n                   min.segment.length = 0, seed = 1, size = 3.2,\n                   fill = \"white\", label.r = unit(0.1,\"lines\")) +\n  # x-axis paper labels\n  scale_x_continuous(breaks = ord$pos, labels = ord$label_paper, expand = expansion(mult = c(0.01, 0.03))) +\n  coord_cartesian(ylim = c(0,100), clip = \"off\") +\n  labs(x=NULL, y=\"Percentile (0–100)\") +\n  theme_uj() +  \n  annotate(\"text\", x = 4, y = 40,\n           label = sprintf(\"Means — Human: %.1f   LLM: %.1f\", hbar, lbar),\n           hjust = 0, size = 4) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),\n        panel.grid.major.x=element_blank(),\n        plot.margin = margin(5, 40, 5, 5))\n```\n\n::: {.cell-output-display}\n![Comparison of Human vs LLM overall percentile ratings](results_files/figure-html/fig-forest-overall-1.png){#fig-forest-overall width=1344}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Optional: set a paper to annotate; leave NA for none\n\nhighlight_paper <- NA_character_\n\nHH <- jt_use %>% filter(criteria == tier_metric, who == \"Human\")\nLL <- jt_use %>% filter(criteria == tier_metric, who == \"LLM\")\nmatched <- intersect(unique(HH$label_paper), unique(LL$label_paper))\n\nH_t <- HH %>%\n  filter(label_paper %in% matched) %>%\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(1, lo), NA_real_),\n    hi = ifelse(is.finite(hi), pmin(5, hi), NA_real_)\n  )\n\nord_t <- H_t %>%\n  group_by(label_paper) %>%\n  summarise(h_mean = mean(mid, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(desc(h_mean)) %>%\n  mutate(pos = row_number())\n\nH_tplot <- H_t %>%\n  inner_join(ord_t, by = \"label_paper\") %>%\n  group_by(label_paper) %>%\n  mutate(off = (row_number() - (n() + 1) / 2) * 0.18,\n         x   = pos + off) %>%\n  ungroup()\n\nL_t <- LL %>%\n  filter(label_paper %in% matched) %>%\n  group_by(label_paper) %>%\n  summarise(\n    mid = mean(mid, na.rm = TRUE),\n    lo  = suppressWarnings(min(coalesce(lo, mid), na.rm = TRUE)),\n    hi  = suppressWarnings(max(coalesce(hi, mid), na.rm = TRUE)),\n    .groups = \"drop\"\n  ) %>%\n  inner_join(ord_t, by = \"label_paper\") %>%\n  mutate(x = pos)\n\nH_pp <- H_t %>% group_by(label_paper) %>% summarise(h_mean = mean(mid), .groups = \"drop\")\nhbar <- mean(H_pp$h_mean, na.rm = TRUE)\nlbar <- mean(L_t$mid,     na.rm = TRUE)\n\nlab <- L_t %>% filter(label_paper == highlight_paper)\nx_ann <- if (nrow(ord_t)) min(ord_t$pos) + 0.3 else 0\n\nggplot() +\n  geom_vline(data = ord_t, aes(xintercept = pos), color = \"grey92\", linewidth = 0.3) +\n  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = \"dashed\", linewidth = 0.8) +\n  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = \"dotted\", linewidth = 0.8) +\n  \n  # Humans\n  \n  geom_errorbar(data = subset(H_tplot, is.finite(lo) & is.finite(hi)),\n                aes(x = x, ymin = lo, ymax = hi),\n                width = 0, linewidth = 1, alpha = 0.5, color = UJ_GREEN) +\n  geom_point(data = H_tplot, aes(x = x, y = mid),\n             size = 3.0, alpha = 0.9, color = UJ_GREEN) +\n  \n  # LLM\n  \n  geom_errorbar(data = subset(L_t, is.finite(lo) & is.finite(hi)),\n                aes(x = x, ymin = lo, ymax = hi),\n                width = 0, linewidth = 1.0, color = UJ_ORANGE) +\n  geom_point(data = L_t, aes(x = x, y = mid),\n             size = 3.6, shape = 18, color = UJ_ORANGE) +\n  geom_label_repel(data = lab,\n                   aes(x = x, y = mid, label = label_paper),\n                   min.segment.length = 0, seed = 1, size = 3.2,\n                   fill = \"white\", label.r = unit(0.1, \"lines\")) +\n  scale_x_continuous(breaks = ord_t$pos, labels = ord_t$label_paper,\n                     expand = expansion(mult = c(0.01, 0.03))) +\n  coord_cartesian(ylim = c(1, 5), clip = \"off\") +\n  labs(x = NULL, y = \"Journal tier (1–5)\") +\n  theme_uj() +\n  annotate(\"text\", x = x_ann, y = 1.4,\n           label = sprintf(\"Means — Human: %.2f   LLM: %.2f\", hbar, lbar),\n           hjust = 0, size = 4) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),\n        panel.grid.major.x = element_blank(),\n        plot.margin = margin(5, 40, 5, 5))\n```\n\n::: {.cell-output-display}\n![Comparison of Human vs LLM journal tier ratings (should be published in)](results_files/figure-html/fig-forest-tiers-should-1.png){#fig-forest-tiers-should width=1344}\n:::\n:::\n\n\n@fig-heatmap-human-minus-llm shows a heatmap of the differences between human and LLM mean ratings across all evaluation criteria. Positive values (in green) indicate that humans rated the paper higher than the LLM, while negative values (in orange) indicate the opposite.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetric_order <- c(\"overall\",\"claims\",\"methods\",\"adv_knowledge\",\"logic_comms\",\"open_sci\",\"gp_relevance\")\nmetric_lab <- c(\n  overall=\"Overall\",\n  claims=\"Claims & evidence\",\n  methods=\"Methods\",\n  adv_knowledge=\"Adv. knowledge\",\n  logic_comms=\"Logic & comms\",\n  open_sci=\"Open science\",\n  gp_relevance=\"Global relevance\"\n)\n\nH_mean <- metrics_use %>%\n  filter(who==\"Human\", criteria %in% metric_order) %>%\n  group_by(label_paper, criteria) %>%\n  summarise(h = mean(mid, na.rm=TRUE), .groups=\"drop\")\n\nL_mean <- metrics_use %>%\n  filter(who==\"LLM\", criteria %in% metric_order) %>%\n  group_by(label_paper, criteria) %>%\n  summarise(l = mean(mid, na.rm=TRUE), .groups=\"drop\")\n\nDdiff <- inner_join(H_mean, L_mean, by=c(\"label_paper\",\"criteria\")) %>%\n  mutate(diff = h - l) %>%  # positive = Human higher\n  mutate(crit = factor(criteria, levels = metric_order, labels = metric_lab[metric_order]))\n\n# order papers by overall difference (Human−LLM)\nord_p <- Ddiff %>%\n  filter(criteria==\"overall\") %>%\n  arrange(desc(diff)) %>%\n  pull(label_paper)\n\nggplot(Ddiff, aes(x = factor(label_paper, levels = ord_p), y = crit, fill = diff)) +\n  geom_tile(color = \"white\", linewidth = 0.25) +\n  scale_fill_gradient2(low = UJ_ORANGE, mid = \"grey95\", high = UJ_GREEN, midpoint = 0,\n                       name = \"Human − LLM\") +\n  labs(x = NULL, y = NULL) +\n  theme_uj() +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8), \n        axis.text.y = element_text(size = 12),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Heatmap of Human minus LLM mean ratings across evaluation criteria](results_files/figure-html/fig-heatmap-human-minus-llm-1.png){#fig-heatmap-human-minus-llm width=1344}\n:::\n:::\n\n\n\n\n::: {.columns}\n\n::: {.column width=\"75%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- metrics_use %>%\n  filter(criteria==\"overall\") %>%\n  group_by(who, label_paper) %>%\n  summarise(mid = mean(mid, na.rm=TRUE), .groups=\"drop\") %>%\n  tidyr::pivot_wider(names_from = who, values_from = mid) %>%\n  filter(is.finite(Human), is.finite(LLM))   # all matched papers\n\n# stats\nr    <- suppressWarnings(cor(D$Human, D$LLM, method=\"pearson\"))\nrho  <- suppressWarnings(cor(D$Human, D$LLM, method=\"spearman\"))\nMAE  <- mean(abs(D$LLM - D$Human))\nalpha_overall <- tryCatch({\n  if (requireNamespace(\"irr\", quietly = TRUE)) {\n    M <- rbind(D$Human, D$LLM); irr::kripp.alpha(M, method = \"interval\")$value\n  } else NA_real_\n}, error = function(e) NA_real_)\nn <- nrow(D)\n\nggplot(D, aes(x = Human, y = LLM)) +\n  geom_abline(slope=1, intercept=0, linetype=\"dashed\", linewidth=0.8, color=\"grey60\") +\n  geom_point(color=UJ_GREEN, size=4, alpha=0.9) +\n  stat_smooth(method=\"lm\", se=FALSE, linewidth=0.5, color=UJ_ORANGE) +\n  coord_equal(xlim=c(25,100), ylim=c(25,100), expand=FALSE) +\n  # annotate(\"text\", x = 4, y = 98,\n  #          label = sprintf(\"n=%d   r=%.2f   rho=%.2f   α=%.2f   MAE=%.1f\", n, r, rho, alpha_overall, MAE),\n  #          hjust = 0, size = 3.5) +\n  labs(x=\"Human overall (0–100)\", y=\"LLM overall (0–100)\") +\n  theme_uj()\n```\n\n::: {.cell-output-display}\n![](results_files/figure-html/scatter-overall-static-stats-1.png){width=480}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"25%\"}\n\n\n- Matched papers: 47\n- Pearson r: 0.37\n- Spearman rho: 0.50\n- Krippendorff α (interval): 0.02\n- MAE (points): 13.2\n\n:::\n\n\n:::\n\n\n\\clearpage\n\n## Qualitative comparison: detailed GPT‑5 Pro evaluations\n\nTo understand what GPT‑5 Pro is actually responding to, we re‑ran the model on four focal papers [@Adena2024; @Peterman2024; @Green2025; @Williams2024] using a refined prompt.\n\nThis second run keeps the same quantitative metrics but additionally requires a diagnostic summary of about 1,000 words and high‑effort reasoning, with the full reasoning trace returned by the “thinking” model. For each paper we can therefore inspect:\n\n- the LLM’s quantitative scores and journal‑tier predictions,\n- the hidden reasoning steps used to arrive at those scores, and\n- the token usage and approximate API cost of the evaluation.\n\nWe start by examining the Williams et al. (2024) evaluation in detail and then show the analogous summaries for the other four focal papers. In the next step we will juxtapose these LLM assessments with the human evaluators’ written reports.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jsonlite)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(here)\n\n\ngpt5pro_price_input <- 15 # input per 1M tokens ()\ngpt5pro_price_output <- 120 # output per 1M tokens ()\n\nprice_in_per_token <- gpt5pro_price_input / 1e6\nprice_out_per_token <- gpt5pro_price_output / 1e6\n\n\n\n`%||%` <- function(x, y) if (!is.null(x)) x else y\n\njson_dir <- here(\"results\", \"json\")\njson_files <- list.files(json_dir, pattern = \"\\\\.response\\\\.json$\", full.names = TRUE)\n\nparse_one_response <- function(path) {\n  r <- jsonlite::fromJSON(path, simplifyVector = FALSE)\n  paper <- basename(path) |> str_replace(\"\\\\.response\\\\.json$\", \"\")\n  \n  # usage / tokens\n  \n  usage      <- r$usage %||% list()\n  out_detail <- usage$output_tokens_details %||% list()\n  \n  # split reasoning vs message\n  \n  out_list       <- r$output %||% list()\n  reasoning_obj  <- purrr::detect(out_list, ~ .x$type == \"reasoning\", .default = NULL)\n  message_obj    <- purrr::detect(out_list, ~ .x$type == \"message\",   .default = NULL)\n  \n  reasoning_id    <- NA_character_\n  reasoning_steps <- character()\n  if (!is.null(reasoning_obj)) {\n    reasoning_id <- reasoning_obj$id %||% NA_character_\n    if (!is.null(reasoning_obj$summary) && is.list(reasoning_obj$summary)) {\n      reasoning_steps <- purrr::map_chr(reasoning_obj$summary, \"text\")\n    }\n  }\n  reasoning_full <- if (length(reasoning_steps)) paste(reasoning_steps, collapse = \"\\n\\n\") else NA_character_\n  \n  # final JSON payload with assessment + metrics\n  \n  assessment_summary <- NA_character_\n  metrics_json       <- NULL\n  if (!is.null(message_obj) && length(message_obj$content) > 0) {\n    txt    <- message_obj$content[[1]]$text\n    parsed <- jsonlite::fromJSON(txt, simplifyVector = TRUE)\n    assessment_summary <- parsed$assessment_summary\n    metrics_json       <- parsed$metrics\n  }\n  \n  metrics_long <- tibble()\n  tiers_long   <- tibble()\n  if (!is.null(metrics_json)) {\n    for (nm in names(metrics_json)) {\n      if (nm %in% c(\"tier_should\", \"tier_will\")) {\n        tiers_long <- bind_rows(\n          tiers_long,\n          tibble(\n            paper    = paper,\n            tier_kind = nm,\n            score    = metrics_json[[nm]]$score,\n            ci_lower = metrics_json[[nm]]$ci_lower,\n            ci_upper = metrics_json[[nm]]$ci_upper\n          )\n        )\n      } else {\n        metrics_long <- bind_rows(\n          metrics_long,\n          tibble(\n            paper       = paper,\n            metric      = nm,\n            midpoint    = metrics_json[[nm]]$midpoint,\n            lower_bound = metrics_json[[nm]]$lower_bound,\n            upper_bound = metrics_json[[nm]]$upper_bound\n          )\n        )\n      }\n    }\n  }\n  \n  master_row <- tibble(\n    paper            = paper,\n    model            = r$model,\n    response_id      = r$id,\n    created_at       = as.POSIXct(r$created_at, origin = \"1970-01-01\", tz = \"UTC\"),\n    input_tokens     = usage$input_tokens %||% NA_integer_,\n    output_tokens    = usage$output_tokens %||% NA_integer_,\n    reasoning_tokens = out_detail$reasoning_tokens %||% NA_integer_,\n    assessment_summary = assessment_summary,\n    reasoning_full     = reasoning_full,\n    reasoning_id       = reasoning_id\n  )\n  \n  list(\n    master         = master_row,\n    metrics        = metrics_long,\n    tiers          = tiers_long,\n    reasoning_steps = tibble(\n      paper = paper,\n      step  = seq_along(reasoning_steps),\n      text  = reasoning_steps\n    )\n  )\n}\n\nparsed <- purrr::map(json_files, parse_one_response)\n\nllm_master          <- bind_rows(purrr::map(parsed, \"master\"))\nllm_metrics_long    <- bind_rows(purrr::map(parsed, \"metrics\"))\nllm_tiers_long      <- bind_rows(purrr::map(parsed, \"tiers\"))\nllm_reasoning_steps <- bind_rows(purrr::map(parsed, \"reasoning_steps\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntoken_cost_summary <- llm_master |>\n  transmute(\n    paper,\n    input_tokens,\n    output_tokens,\n    reasoning_tokens,\n    total_tokens    = input_tokens + output_tokens,\n    est_cost_usd    = input_tokens  * price_in_per_token +\n      (output_tokens + reasoning_tokens) * price_out_per_token\n  ) |>\n  arrange(desc(est_cost_usd))\n\ntoken_cost_summary_display <- token_cost_summary |>\n  mutate(\n    est_cost_usd = round(est_cost_usd, 2)\n  )\n\nknitr::kable(\n  token_cost_summary_display,\n  col.names = c(\n    \"Paper\", \"Input tokens\", \"Output tokens\", \"Reasoning tokens\",\n    \"Total tokens\", \"Est. cost (USD)\"\n  ),\n  align = c(\"l\",\"r\",\"r\",\"r\",\"r\",\"r\")\n)\n```\n\n::: {.cell-output-display}\n\n\n|Paper                | Input tokens| Output tokens| Reasoning tokens| Total tokens| Est. cost (USD)|\n|:--------------------|------------:|-------------:|----------------:|------------:|---------------:|\n|Peterman et al. 2025 |        18762|          7617|             6208|        26379|            1.94|\n|Adena and Hager 2024 |        24234|          7019|             5312|        31253|            1.84|\n|Williams et al. 2024 |        28704|          6327|             5120|        35031|            1.80|\n|Kudymowa et al. 2023 |        65096|          4147|             2432|        69243|            1.77|\n|Green et al. 2025    |        22904|          5884|             3904|        28788|            1.52|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# pricing for GPT‑5 Pro (USD / 1M tokens, from OpenAI API docs)\n\ngpt5pro_price_input  <- 15    # input per 1M tokens\ngpt5pro_price_output <- 120   # output per 1M tokens\n\nprice_in_per_token  <- gpt5pro_price_input  / 1e6\nprice_out_per_token <- gpt5pro_price_output / 1e6\n\n# Williams row\n\nwilliams_row <- llm_master |>\n  filter(paper == \"Williams et al. 2024\") |>\n  slice(1)\n\n# tokens + cost\n\nwilliams_tokens <- williams_row |>\n  transmute(\n    input_tokens,\n    output_tokens,\n    reasoning_tokens,\n    total_tokens    = input_tokens + output_tokens,\n    est_cost_usd    = input_tokens  * price_in_per_token +\n      (output_tokens + reasoning_tokens) * price_out_per_token\n  )\n\n# 0–100 metrics\n\nmetric_labels <- c(\n  overall              = \"Overall assessment\",\n  claims_evidence      = \"Claims & evidence\",\n  methods              = \"Methods\",\n  advancing_knowledge  = \"Advancing knowledge and practice\",\n  logic_communication  = \"Logic and communication\",\n  open_science         = \"Open, collaborative, replicable science\",\n  global_relevance     = \"Relevance to global priorities\"\n)\n\nmetric_order <- names(metric_labels)\n\nwilliams_metrics <- llm_metrics_long |>\n  filter(paper == \"Williams et al. 2024\",\n         metric %in% metric_order) |>\n  mutate(\n    metric_label = metric_labels[metric],\n    metric_label = factor(metric_label, levels = unname(metric_labels))\n  ) |>\n  arrange(metric_label) |>\n  transmute(\n    Criterion = metric_label,\n    Midpoint  = midpoint,\n    Lower_90  = lower_bound,\n    Upper_90  = upper_bound\n  )\n\n# journal tiers\n\nwilliams_tiers <- llm_tiers_long |>\n  filter(paper == \"Williams et al. 2024\") |>\n  mutate(\n    Measure = recode(\n      tier_kind,\n      tier_should = \"Deserved journal tier (should)\",\n      tier_will   = \"Predicted journal tier (will)\"\n    )\n  ) |>\n  transmute(\n    Measure,\n    Score    = score,\n    Lower_90 = ci_lower,\n    Upper_90 = ci_upper\n  )\n\n# text blobs from JSON\n\nwilliams_assessment       <- williams_row$assessment_summary\nwilliams_reasoning_full   <- williams_row$reasoning_full\nwilliams_reasoning_steps  <- llm_reasoning_steps |>\n  filter(paper == \"Williams et al. 2024\")\n```\n:::\n\n\n\n\n### Qualitative comparison: Williams et al. (2024)\n\nIn the refined run, GPT‑5 Pro reads about \n28,704 input tokens and produces \n6,327 visible output tokens plus \n5,120 reasoning tokens.  \nAt current API prices this evaluation costs roughly \n$1.80.\n\nThe table below shows the model’s percentile ratings and 90% credible intervals for the Unjournal criteria.\n\n\n\n\n::: {#tbl-llm-williams-metrics .cell tbl-cap='GPT-5 Pro percentile ratings for Williams et al. (2024)'}\n\n```{.r .cell-code}\nwilliams_metrics |>\nmutate(across(c(Midpoint, Lower_90, Upper_90), round, 1)) |>\nknitr::kable(\ncol.names = c(\"Criterion\", \"Midpoint\", \"Lower 90%\", \"Upper 90%\"),\nalign = c(\"l\",\"c\",\"c\",\"c\")\n) |>\nkableExtra::kable_styling(full_width = FALSE)\n```\n\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Criterion </th>\n   <th style=\"text-align:center;\"> Midpoint </th>\n   <th style=\"text-align:center;\"> Lower 90% </th>\n   <th style=\"text-align:center;\"> Upper 90% </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Overall assessment </td>\n   <td style=\"text-align:center;\"> 86 </td>\n   <td style=\"text-align:center;\"> 86 </td>\n   <td style=\"text-align:center;\"> 86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Claims &amp; evidence </td>\n   <td style=\"text-align:center;\"> 78 </td>\n   <td style=\"text-align:center;\"> 78 </td>\n   <td style=\"text-align:center;\"> 78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Methods </td>\n   <td style=\"text-align:center;\"> 74 </td>\n   <td style=\"text-align:center;\"> 74 </td>\n   <td style=\"text-align:center;\"> 74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Advancing knowledge and practice </td>\n   <td style=\"text-align:center;\"> 92 </td>\n   <td style=\"text-align:center;\"> 92 </td>\n   <td style=\"text-align:center;\"> 92 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Logic and communication </td>\n   <td style=\"text-align:center;\"> 84 </td>\n   <td style=\"text-align:center;\"> 84 </td>\n   <td style=\"text-align:center;\"> 84 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Open, collaborative, replicable science </td>\n   <td style=\"text-align:center;\"> 63 </td>\n   <td style=\"text-align:center;\"> 63 </td>\n   <td style=\"text-align:center;\"> 63 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Relevance to global priorities </td>\n   <td style=\"text-align:center;\"> 94 </td>\n   <td style=\"text-align:center;\"> 94 </td>\n   <td style=\"text-align:center;\"> 94 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {#tbl-llm-williams-tiers .cell tbl-cap='GPT-5 Pro journal tier ratings for Williams et al. (2024)'}\n\n```{.r .cell-code}\nwilliams_tiers |>\n  mutate(across(c(Score, Lower_90, Upper_90), round, 2)) |>\n  knitr::kable(\n    col.names = c(\"Measure\", \"Score\", \"Lower 90%\", \"Upper 90%\"),\n    align = c(\"l\",\"c\",\"c\",\"c\")\n  ) |>\n  kableExtra::kable_styling(full_width = FALSE)\n```\n\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Measure </th>\n   <th style=\"text-align:center;\"> Score </th>\n   <th style=\"text-align:center;\"> Lower 90% </th>\n   <th style=\"text-align:center;\"> Upper 90% </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Deserved journal tier (should) </td>\n   <td style=\"text-align:center;\"> 4.4 </td>\n   <td style=\"text-align:center;\"> 4.4 </td>\n   <td style=\"text-align:center;\"> 4.4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Predicted journal tier (will) </td>\n   <td style=\"text-align:center;\"> 4.8 </td>\n   <td style=\"text-align:center;\"> 4.8 </td>\n   <td style=\"text-align:center;\"> 4.8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n\nFor Williams et al. (2024), GPT‑5 Pro assigns a high overall percentile score (86/100), with particularly strong ratings for advancing knowledge (92) and global relevance (94), and a relatively favourable view of methods (74) and claims and evidence (78). It judges the paper as deserving publication in a high‑tier journal (tier‑should 4.4/5, tier‑will 4.8/5). \n\nIn its diagnostic summary and reasoning trace (printed below), the model identifies many of the same issues highlighted by the human evaluators: heavy reliance on a regrowth dataset with low producer accuracy and substantial omission error; temporal leakage from contemporaneous predictors; uncalibrated random‑forest probabilities used to derive the 215 Mha estimate; unrealistically narrow confidence intervals; coarse predictors driving 30‑m predictions; a liberal definition of land “available for restoration”; and incomplete uncertainty quantification for the carbon overlay and permanence. It also notes that code is not fully open, limiting replication despite open data and published maps.\n\nBy contrast, both human evaluators at The Unjournal assign much lower overall ratings (50/100) and are substantially more critical of methods and claims: methods scores of 20 and 5, and claims scores of 20 and 5, respectively. They explicitly argue that methodological problems “fundamentally challenge the validity and utility of the central 215 Mha estimate of regeneration potential,” even while rating the paper’s contribution to knowledge and global relevance as high and considering it suitable for high‑tier journals conditional on major revisions.\n\nQualitatively, the LLM and human experts are strikingly aligned on what the main problems are: they converge on concerns about biased input data, temporal leakage, incomplete probability calibration, under‑stated uncertainty, domain definition, and the gap between “biophysical potential” and realistic policy use. However, they diverge sharply on how serious these problems are. The human evaluators treat them as sufficient to render the headline estimates low‑credibility and heavily qualified for policy use; GPT‑5 Pro instead regards these issues as important but ultimately compatible with a high overall rating, robust methods, and near top‑journal quality.\n\nThis case suggests that, at least in this configuration, the LLM can reproduce sophisticated methodological critiques and uncertainty language but tends to under‑penalise these shortcomings in its quantitative ratings, especially for high‑profile, high‑impact work.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}