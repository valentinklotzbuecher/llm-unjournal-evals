{
  "hash": "a15dd75d0db1c3d9c00750b6c4a42e2f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Results\"\nformat: \n  html: default\n  # pdf: default\nengine: knitr\n--- \n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Include global setup and parameters\"}\nsource(\"setup_params.R\")\n```\n:::\n\n\n\n\n\nHere we present preliminary results, starting with a comparison of the LLM‑generated quantitative ratings (model: `gpt-5-pro`, see the[(previous section](methods.qmd)) with human evaluations across [the Unjournal's criteria](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#undefined-1). \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\nlibrary(\"janitor\")\nlibrary(\"stringr\")\nlibrary(\"lubridate\")\nlibrary(\"here\") \nlibrary(\"knitr\")\nlibrary(\"kableExtra\")\nlibrary(\"ggforce\") \nlibrary(\"ggrepel\")\nlibrary(\"glue\")\n# library(\"ggalluvial\")\nlibrary(\"scales\")\n# library(\"ggbreak\")\nlibrary(\"jsonlite\")\nlibrary(\"purrr\")\nlibrary(\"tibble\")\n\n\n\nUJ_ORANGE <- \"#f19e4b\"   # LLM\nUJ_GREEN  <- \"#99bb66\"   # Human\nUJ_BLUE <- \"#4e79a7\"   # GPT‑5 (legacy)\n\ntheme_uj <- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title.position = \"plot\",\n      legend.position = \"bottom\"\n    )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# paper_authors <- read_delim(\"data/paper_authors.csv\", delim = \",\")\n\n# Mapping paper keys - short titles\nUJmap <- read_delim(\"data/UJ_map.csv\", delim = \";\") |>\n  mutate(label_paper_title = research,\n         label_paper = paper) |>\n  select(c(\"label_paper_title\", \"label_paper\"))\n\n\n# Unjournal ratings\nrsx <- read_csv(\"data/rsx_evalr_rating.csv\", show_col_types = FALSE) |> \n  clean_names()  |>\n  mutate(label_paper_title = research) |>\n  select(-c(\"research\"))\n\n\n# UJ evaluated research\nresearch <- read_csv(\"data/research.csv\", show_col_types = FALSE) |>\n  clean_names() |>\n  filter(status == \"50_published evaluations (on PubPub, by Unjournal)\") |>  \n  left_join(UJmap, by = c(\"label_paper_title\")) |>\n  mutate(doi = str_trim(doi)) |>\n  mutate(label_paper = if_else(doi == \"https://doi.org/10.3386/w31162\", \"Walker et al. 2023\", label_paper, missing = label_paper)) |>\n  mutate(label_paper = if_else(doi == \"doi.org/10.3386/w32728\", \"Hahn et al. 2025\", label_paper, missing = label_paper))  |>\n  mutate(label_paper = if_else(doi == \"https://doi.org/10.3386/w30011\", \"Bhat et al. 2022\", label_paper, missing = label_paper))  |>\n  mutate(label_paper = if_else(doi == \"10.1093/wbro/lkae010\", \"Crawfurd et al. 2023\", label_paper, missing = label_paper))  |>\n  left_join(rsx, by = c(\"label_paper_title\"))\n \n\njtiers_llm <- read_csv(\"data/tiers_long.csv\", show_col_types = FALSE) |>\n  mutate(middle_rating = score,\n         lower_ci = lo,\n         upper_ci = hi,\n         criteria = if_else(tier_kind == \"tier_will\", \"journal_predict\", \"merits_journal\"),\n         evaluator = \"gpt-5\",\n         label_paper = paper\n         ) |>\n  select(c(\"label_paper\", \"evaluator\", \"middle_rating\", \"lower_ci\", \"upper_ci\" , \"criteria\", \"rationale\"))\n\n\njtiers_uj <- research |>\n  filter(criteria== \"merits_journal\" | criteria == \"journal_predict\") |>\n  mutate(paper = label_paper,\n         rationale = \"\")  |>\n  select(c(\"label_paper\", \"evaluator\", \"middle_rating\", \"lower_ci\", \"upper_ci\" , \"criteria\", \"rationale\"))\n\n\njtiers <- jtiers_uj |>\n  rbind(jtiers_llm) |>\n  mutate(human = if_else(evaluator == \"o3\", \"Human\", \"o3\"),\n         lower_ci = if_else(lower_ci > 10, lower_ci/10, lower_ci))\n\n\n# write_csv(all_ratings, \"data/all_jtiers.csv\")\nwrite_rds(\n  jtiers,\n  \"data/all_jtiers.rds\",\n  compress = \"none\"\n  )\n\n# utilities + clean columns for plotting\n\nlane_offsets_center <- function(m, gap = 0.18) {\n  if (m <= 0) return(numeric(0))\n  if (m == 1) return(0)\n  k <- floor((m - 1)/2)\n  offs <- sort(c(-seq_len(k), 0, seq_len(k))) * gap\n  offs[seq_len(m)]\n}\nlane_offsets_skip0 <- function(m, gap = 0.18) {\n  if (m <= 0) return(numeric(0))\n  if (m == 1) return(gap)\n  k <- ceiling(m/2)\n  offs <- sort(c(-seq_len(k), seq_len(k))) * gap\n  offs[seq_len(m)]\n}\n\njt_use <- jtiers |>\n  transmute(\n    label_paper,\n    criteria,\n    who = if_else(evaluator == \"gpt-5\", \"LLM\", \"Human\"),\n    mid = as.numeric(middle_rating),\n    lo  = suppressWarnings(as.numeric(lower_ci)),\n    hi  = suppressWarnings(as.numeric(upper_ci))\n  ) |>\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(1, pmin(5, lo)), NA_real_),\n    hi = ifelse(is.finite(hi), pmax(1, pmin(5, hi)), NA_real_)\n  )\n\ntier_metric <- \"merits_journal\"  # change \"merits_journal\" to \"journal_predict\" for the other slide\n\n\n\n\ncanon_metric <- function(x) dplyr::recode(\n  x,\n  \"advancing_knowledge\" = \"adv_knowledge\",\n  \"open_science\"        = \"open_sci\",\n  \"logic_communication\" = \"logic_comms\",\n  \"global_relevance\"    = \"gp_relevance\",\n  \"claims_evidence\"     = \"claims\",\n  .default = x\n)\n\n# LLM metrics \nmetrics_llm_full <- readr::read_csv(here(\"data\",\"metrics_long.csv\"), show_col_types = FALSE) |>\n  janitor::clean_names() |>\n  mutate(\n    label_paper = stringr::str_replace(paper, \"et al \", \"et al. \"),\n    criteria    = canon_metric(metric),\n    mid = as.numeric(midpoint),\n    lo  = suppressWarnings(as.numeric(lower_bound)),\n    hi  = suppressWarnings(as.numeric(upper_bound)),\n    who = \"LLM\",\n    evaluator = \"gpt-5\"\n  )\n\n# LLM metrics (for plotting)\nmetrics_llm <- metrics_llm_full |>\n  transmute(label_paper, criteria, who, mid, lo, hi) |>\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(0, pmin(100, lo)), NA_real_),\n    hi = ifelse(is.finite(hi), pmax(0, pmin(100, hi)), NA_real_)\n  )\n\n\n# 1) Build a 1-row-per-title key from `research`\nkey_map <- research %>%\n  transmute(label_paper_title = stringr::str_trim(label_paper_title),\n            label_paper      = label_paper) %>%\n  filter(!is.na(label_paper_title)) %>%\n  distinct(label_paper_title, label_paper) %>%   # remove exact dups\n  group_by(label_paper_title) %>%\n  slice(1) %>%\n  ungroup()\n\n# Optional sanity check: any titles map to >1 label?\n# research %>% distinct(label_paper_title, label_paper) %>%\n#   count(label_paper_title) %>% filter(n > 1)\n\n# 2) Join with relationship assertion\nrsx_research <- rsx %>%\n  mutate(label_paper_title = stringr::str_trim(label_paper_title)) %>%\n  dplyr::left_join(key_map, by = \"label_paper_title\", relationship = \"many-to-one\")\n\n\nmetrics_human <- rsx_research |>\n  mutate(criteria = canon_metric(criteria)) |>\n  filter(criteria %in% c(\"overall\",\"claims\",\"methods\",\"adv_knowledge\",\"logic_comms\",\"open_sci\",\"gp_relevance\")) |>\n  transmute(\n    label_paper,\n    criteria,\n    evaluator,\n    who = \"Human\",\n    mid = as.numeric(middle_rating),\n    lo  = suppressWarnings(as.numeric(lower_ci)),\n    hi  = suppressWarnings(as.numeric(upper_ci))\n  ) |>\n  filter(!is.na(label_paper), !is.na(mid)) |>\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(0, pmin(100, lo)), NA_real_),\n    hi = ifelse(is.finite(hi), pmax(0, pmin(100, hi)), NA_real_)\n  ) %>% \n  # remove exact duplicates (protect against tiny float diffs by rounding first) -- but wait, what if human raters rated identically for a category?\n  mutate(across(c(mid, lo, hi), ~ round(.x, 4))) |>\n  distinct(label_paper, criteria, who, evaluator, mid, lo, hi, .keep_all = FALSE)\n\n\nmetrics_use <- bind_rows(metrics_human, metrics_llm)\n\nmatched_overall <- intersect(\n  metrics_use %>% filter(who==\"LLM\", criteria==\"overall\") %>% pull(label_paper),\n  metrics_use %>% filter(who==\"Human\", criteria==\"overall\") %>% pull(label_paper)\n)\nmetrics_use <- metrics_use %>% mutate(is_matched_overall = label_paper %in% matched_overall)\n```\n:::\n\n\n\n\n\n## Quantitative comparison: human vs. GPT‑5 Pro (initial run)\n\nWe first use the earlier GPT‑5 Pro evaluation run that covered all papers in our Unjournal sample with a simpler JSON‑schema prompt. @fig-forest-overall shows the overall percentile ratings from this initial run, averaged across human evaluators and compared to the LLM’s “overall” scores for each paper.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhighlight_paper <- \"Kremer et al. 2022 XXX\" # Select here to annotate\n\nmatched <- intersect(\n  metrics_use %>% filter(who==\"LLM\",   criteria==\"overall\") %>% pull(label_paper),\n  metrics_use %>% filter(who==\"Human\", criteria==\"overall\") %>% pull(label_paper)\n)\n\nH_ind <- metrics_human %>%\n  filter(criteria==\"overall\", label_paper %in% matched) %>%\n  mutate(lo = ifelse(is.finite(lo), pmax(0,lo), NA_real_),\n         hi = ifelse(is.finite(hi), pmin(100,hi), NA_real_))\n\nord <- H_ind %>%\n  group_by(label_paper) %>%\n  summarise(h_mean = mean(mid, na.rm=TRUE), .groups=\"drop\") %>%\n  arrange(desc(h_mean)) %>% mutate(pos = row_number())\n\nH_plot <- H_ind %>%\n  inner_join(ord, by=\"label_paper\") %>%\n  group_by(label_paper) %>%\n  mutate(off = (row_number() - (n()+1)/2) * 0.18,\n         x   = pos + off) %>% ungroup()\n\n# per-paper human mean and LLM summary\nH_pp <- H_ind %>% group_by(label_paper) %>% summarise(h_mean = mean(mid), .groups=\"drop\")\nL_c <- metrics_llm %>%\n  filter(criteria==\"overall\", label_paper %in% matched) %>%\n  group_by(label_paper) %>%\n  summarise(mid = mean(mid, na.rm=TRUE),\n            lo  = suppressWarnings(min(coalesce(lo, mid), na.rm=TRUE)),\n            hi  = suppressWarnings(max(coalesce(hi, mid), na.rm=TRUE)),\n            .groups=\"drop\") %>%\n  inner_join(ord, by=\"label_paper\") %>%\n  mutate(x = pos)\n\nlab <- L_c %>% filter(label_paper == highlight_paper)\n\n# overall means to show as horizontal reference lines\nhbar <- mean(H_pp$h_mean, na.rm=TRUE)\nlbar <- mean(L_c$mid,     na.rm=TRUE)\n\nggplot() +\n  geom_vline(data = ord, aes(xintercept = pos), color=\"grey92\", linewidth=0.3) +\n  # mean lines\n  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = \"dotted\", linewidth = 0.8) +\n  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = \"dotted\", linewidth = 0.8) +\n  # humans\n  geom_errorbar(data = subset(H_plot, is.finite(lo)&is.finite(hi)),\n                aes(x=x, ymin=lo, ymax=hi),\n                width=0, linewidth=1, alpha=0.5, color=UJ_GREEN) +\n  geom_point(data = H_plot, aes(x=x, y=mid), size=3.0, alpha=0.9, color=UJ_GREEN) +\n  # LLM\n  geom_errorbar(data = subset(L_c, is.finite(lo)&is.finite(hi)),\n                aes(x=x, ymin=lo, ymax=hi),\n                width=0, linewidth=1.0, color=UJ_ORANGE) +\n  geom_point(data = L_c, aes(x=x, y=mid), size=3.6, shape=18, color=UJ_ORANGE) +\n  # label highlight\n  geom_label_repel(data = lab,\n                   aes(x = x, y = mid, label = label_paper),\n                   min.segment.length = 0, seed = 1, size = 3.2,\n                   fill = \"white\", label.r = unit(0.1,\"lines\")) +\n  # x-axis paper labels\n  scale_x_continuous(breaks = ord$pos, labels = ord$label_paper, expand = expansion(mult = c(0.01, 0.03))) +\n  coord_cartesian(ylim = c(0,100), clip = \"off\") +\n  labs(x=NULL, y=\"Percentile (0–100)\") +\n  theme_uj() +  \n  annotate(\"text\", x = 4, y = 40,\n           label = sprintf(\"Means — Human: %.1f   LLM: %.1f\", hbar, lbar),\n           hjust = 0, size = 4) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),\n        panel.grid.major.x=element_blank(),\n        plot.margin = margin(5, 40, 5, 5))\n```\n\n::: {.cell-output-display}\n![Comparison of Human vs LLM overall percentile ratings](results_files/figure-html/fig-forest-overall-1.png){#fig-forest-overall width=1344}\n:::\n:::\n\n\n\n@fig-heatmap-human-minus-llmshows a heatmap of the differences between human and LLM mean ratings across all evaluation criteria. Positive values (in green) indicate that humans rated the paper higher than the LLM, while negative values (in orange) indicate the opposite.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetric_order <- c(\"overall\",\"claims\",\"methods\",\"adv_knowledge\",\"logic_comms\",\"open_sci\",\"gp_relevance\")\nmetric_lab <- c(\n  overall=\"Overall\",\n  claims=\"Claims & evidence\",\n  methods=\"Methods\",\n  adv_knowledge=\"Adv. knowledge\",\n  logic_comms=\"Logic & comms\",\n  open_sci=\"Open science\",\n  gp_relevance=\"Global relevance\"\n)\n\nH_mean <- metrics_use %>%\n  filter(who==\"Human\", criteria %in% metric_order) %>%\n  group_by(label_paper, criteria) %>%\n  summarise(h = mean(mid, na.rm=TRUE), .groups=\"drop\")\n\nL_mean <- metrics_use %>%\n  filter(who==\"LLM\", criteria %in% metric_order) %>%\n  group_by(label_paper, criteria) %>%\n  summarise(l = mean(mid, na.rm=TRUE), .groups=\"drop\")\n\nDdiff <- inner_join(H_mean, L_mean, by=c(\"label_paper\",\"criteria\")) %>%\n  mutate(diff = h - l) %>%  # positive = Human higher\n  mutate(crit = factor(criteria, levels = metric_order, labels = metric_lab[metric_order]))\n\n# order papers by overall difference (Human−LLM)\nord_p <- Ddiff %>%\n  filter(criteria==\"overall\") %>%\n  arrange(desc(diff)) %>%\n  pull(label_paper)\n\nggplot(Ddiff, aes(x = factor(label_paper, levels = ord_p), y = crit, fill = diff)) +\n  geom_tile(color = \"white\", linewidth = 0.25) +\n  scale_fill_gradient2(low = UJ_ORANGE, mid = \"grey95\", high = UJ_GREEN, midpoint = 0,\n                       name = \"Human − LLM\") +\n  labs(x = NULL, y = NULL) +\n  theme_uj() +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8), \n        axis.text.y = element_text(size = 12),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Heatmap of Human minus LLM mean ratings across evaluation criteria](results_files/figure-html/fig-heatmap-human-minus-llm-1.png){#fig-heatmap-human-minus-llm width=1344}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Optional: set a paper to annotate; leave NA for none\n\nhighlight_paper <- NA_character_\n\nHH <- jt_use %>% filter(criteria == tier_metric, who == \"Human\")\nLL <- jt_use %>% filter(criteria == tier_metric, who == \"LLM\")\nmatched <- intersect(unique(HH$label_paper), unique(LL$label_paper))\n\nH_t <- HH %>%\n  filter(label_paper %in% matched) %>%\n  mutate(\n    lo = ifelse(is.finite(lo), pmax(1, lo), NA_real_),\n    hi = ifelse(is.finite(hi), pmin(5, hi), NA_real_)\n  )\n\nord_t <- H_t %>%\n  group_by(label_paper) %>%\n  summarise(h_mean = mean(mid, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(desc(h_mean)) %>%\n  mutate(pos = row_number())\n\nH_tplot <- H_t %>%\n  inner_join(ord_t, by = \"label_paper\") %>%\n  group_by(label_paper) %>%\n  mutate(off = (row_number() - (n() + 1) / 2) * 0.18,\n         x   = pos + off) %>%\n  ungroup()\n\nL_t <- LL %>%\n  filter(label_paper %in% matched) %>%\n  group_by(label_paper) %>%\n  summarise(\n    mid = mean(mid, na.rm = TRUE),\n    lo  = suppressWarnings(min(coalesce(lo, mid), na.rm = TRUE)),\n    hi  = suppressWarnings(max(coalesce(hi, mid), na.rm = TRUE)),\n    .groups = \"drop\"\n  ) %>%\n  inner_join(ord_t, by = \"label_paper\") %>%\n  mutate(x = pos)\n\nH_pp <- H_t %>% group_by(label_paper) %>% summarise(h_mean = mean(mid), .groups = \"drop\")\nhbar <- mean(H_pp$h_mean, na.rm = TRUE)\nlbar <- mean(L_t$mid,     na.rm = TRUE)\n\nlab <- L_t %>% filter(label_paper == highlight_paper)\nx_ann <- if (nrow(ord_t)) min(ord_t$pos) + 0.3 else 0\n\nggplot() +\n  geom_vline(data = ord_t, aes(xintercept = pos), color = \"grey92\", linewidth = 0.3) +\n  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = \"dashed\", linewidth = 0.8) +\n  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = \"dotted\", linewidth = 0.8) +\n  \n  # Humans\n  \n  geom_errorbar(data = subset(H_tplot, is.finite(lo) & is.finite(hi)),\n                aes(x = x, ymin = lo, ymax = hi),\n                width = 0, linewidth = 1, alpha = 0.5, color = UJ_GREEN) +\n  geom_point(data = H_tplot, aes(x = x, y = mid),\n             size = 3.0, alpha = 0.9, color = UJ_GREEN) +\n  \n  # LLM\n  \n  geom_errorbar(data = subset(L_t, is.finite(lo) & is.finite(hi)),\n                aes(x = x, ymin = lo, ymax = hi),\n                width = 0, linewidth = 1.0, color = UJ_ORANGE) +\n  geom_point(data = L_t, aes(x = x, y = mid),\n             size = 3.6, shape = 18, color = UJ_ORANGE) +\n  geom_label_repel(data = lab,\n                   aes(x = x, y = mid, label = label_paper),\n                   min.segment.length = 0, seed = 1, size = 3.2,\n                   fill = \"white\", label.r = unit(0.1, \"lines\")) +\n  scale_x_continuous(breaks = ord_t$pos, labels = ord_t$label_paper,\n                     expand = expansion(mult = c(0.01, 0.03))) +\n  coord_cartesian(ylim = c(1, 5), clip = \"off\") +\n  labs(x = NULL, y = \"Journal tier (1–5)\") +\n  theme_uj() +\n  annotate(\"text\", x = x_ann, y = 1.4,\n           label = sprintf(\"Means — Human: %.2f   LLM: %.2f\", hbar, lbar),\n           hjust = 0, size = 4) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),\n        panel.grid.major.x = element_blank(),\n        plot.margin = margin(5, 40, 5, 5))\n```\n\n::: {.cell-output-display}\n![Comparison of Human vs LLM journal tier ratings (should be published in)](results_files/figure-html/fig-forest-tiers-should-1.png){#fig-forest-tiers-should width=1344}\n:::\n:::\n\n\n\n\n\n\n## Qualitative comparison: detailed GPT‑5 Pro evaluations\n\nTo understand what GPT‑5 Pro is actually responding to, we re‑ran the model on five focal papers (Adena and Hager 2024; Kudymowa et al. 2023; Peterman et al. 2025; Green et al. 2025; Williams et al. 2024) using a refined prompt.\n\nThis second run keeps the same quantitative metrics but additionally requires a diagnostic summary of about 1,000 words and high‑effort reasoning, with the full reasoning trace returned by the “thinking” model. For each paper we can therefore inspect:\n\n- the LLM’s quantitative scores and journal‑tier predictions,\n- the hidden reasoning steps used to arrive at those scores, and\n- the token usage and approximate API cost of the evaluation.\n\nWe start by examining the Williams et al. (2024) evaluation in detail and then show the analogous summaries for the other four focal papers. In the next step we will juxtapose these LLM assessments with the human evaluators’ written reports.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n`%||%` <- function(x, y) if (!is.null(x)) x else y\n\njson_dir <- here(\"results\", \"json\")\njson_files <- list.files(\n  json_dir,\n  pattern = \"\\\\.response\\\\.json$\",\n  full.names = TRUE\n)\n\nparse_one_response <- function(path) {\n  r <- jsonlite::fromJSON(path, simplifyVector = FALSE)\n  paper <- basename(path) |> str_replace(\"\\\\.response\\\\.json$\", \"\")\n  \n  # usage / tokens\n  \n  usage      <- r$usage %||% list()\n  out_detail <- usage$output_tokens_details %||% list()\n  \n  # split reasoning vs message\n  \n  out_list       <- r$output %||% list()\n  reasoning_obj  <- purrr::detect(out_list, ~ .x$type == \"reasoning\", .default = NULL)\n  message_obj    <- purrr::detect(out_list, ~ .x$type == \"message\",   .default = NULL)\n  \n  reasoning_id    <- NA_character_\n  reasoning_steps <- character()\n  if (!is.null(reasoning_obj)) {\n    reasoning_id <- reasoning_obj$id %||% NA_character_\n    if (!is.null(reasoning_obj$summary) && is.list(reasoning_obj$summary)) {\n      reasoning_steps <- purrr::map_chr(reasoning_obj$summary, \"text\")\n    }\n  }\n  reasoning_full <- if (length(reasoning_steps)) paste(reasoning_steps, collapse = \"\\n\\n\") else NA_character_\n  \n  # final JSON payload with assessment + metrics\n  \n  assessment_summary <- NA_character_\n  metrics_json       <- NULL\n  if (!is.null(message_obj) && length(message_obj$content) > 0) {\n    txt    <- message_obj$content[[1]]$text\n    parsed <- jsonlite::fromJSON(txt, simplifyVector = TRUE)\n    assessment_summary <- parsed$assessment_summary\n    metrics_json       <- parsed$metrics\n  }\n  \n  metrics_long <- tibble()\n  tiers_long   <- tibble()\n  if (!is.null(metrics_json)) {\n    for (nm in names(metrics_json)) {\n      if (nm %in% c(\"tier_should\", \"tier_will\")) {\n        tiers_long <- bind_rows(\n          tiers_long,\n          tibble(\n            paper    = paper,\n            tier_kind = nm,\n            score    = metrics_json[[nm]]$score,\n            ci_lower = metrics_json[[nm]]$ci_lower,\n            ci_upper = metrics_json[[nm]]$ci_upper\n          )\n        )\n      } else {\n        metrics_long <- bind_rows(\n          metrics_long,\n          tibble(\n            paper       = paper,\n            metric      = nm,\n            midpoint    = metrics_json[[nm]]$midpoint,\n            lower_bound = metrics_json[[nm]]$lower_bound,\n            upper_bound = metrics_json[[nm]]$upper_bound\n          )\n        )\n      }\n    }\n  }\n  \n  master_row <- tibble(\n    paper            = paper,\n    model            = r$model,\n    response_id      = r$id,\n    created_at       = as.POSIXct(r$created_at, origin = \"1970-01-01\", tz = \"UTC\"),\n    input_tokens     = usage$input_tokens %||% NA_integer_,\n    output_tokens    = usage$output_tokens %||% NA_integer_,\n    reasoning_tokens = out_detail$reasoning_tokens %||% NA_integer_,\n    assessment_summary = assessment_summary,\n    reasoning_full     = reasoning_full,\n    reasoning_id       = reasoning_id\n  )\n  \n  list(\n    master         = master_row,\n    metrics        = metrics_long,\n    tiers          = tiers_long,\n    reasoning_steps = tibble(\n      paper = paper,\n      step  = seq_along(reasoning_steps),\n      text  = reasoning_steps\n    )\n  )\n}\n\nparsed <- purrr::map(json_files, parse_one_response)\n\nllm_master          <- bind_rows(purrr::map(parsed, \"master\"))\nllm_metrics_long    <- bind_rows(purrr::map(parsed, \"metrics\"))\nllm_tiers_long      <- bind_rows(purrr::map(parsed, \"tiers\"))\nllm_reasoning_steps <- bind_rows(purrr::map(parsed, \"reasoning_steps\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# pricing for GPT‑5 Pro (USD / 1M tokens, from OpenAI API docs)\n\ngpt5pro_price_input  <- 15    # input per 1M tokens\ngpt5pro_price_output <- 120   # output per 1M tokens\n\nprice_in_per_token  <- gpt5pro_price_input  / 1e6\nprice_out_per_token <- gpt5pro_price_output / 1e6\n\n# Williams row\n\nwilliams_row <- llm_master |>\n  filter(paper == \"Williams et al. 2024\") |>\n  slice(1)\n\n# tokens + cost\n\nwilliams_tokens <- williams_row |>\n  transmute(\n    input_tokens,\n    output_tokens,\n    reasoning_tokens,\n    total_tokens    = input_tokens + output_tokens,\n    est_cost_usd    = input_tokens  * price_in_per_token +\n      (output_tokens + reasoning_tokens) * price_out_per_token\n  )\n\n# 0–100 metrics\n\nmetric_labels <- c(\n  overall              = \"Overall assessment\",\n  claims_evidence      = \"Claims & evidence\",\n  methods              = \"Methods\",\n  advancing_knowledge  = \"Advancing knowledge and practice\",\n  logic_communication  = \"Logic and communication\",\n  open_science         = \"Open, collaborative, replicable science\",\n  global_relevance     = \"Relevance to global priorities\"\n)\n\nmetric_order <- names(metric_labels)\n\nwilliams_metrics <- llm_metrics_long |>\n  filter(paper == \"Williams et al. 2024\",\n         metric %in% metric_order) |>\n  mutate(\n    metric_label = metric_labels[metric],\n    metric_label = factor(metric_label, levels = unname(metric_labels))\n  ) |>\n  arrange(metric_label) |>\n  transmute(\n    Criterion = metric_label,\n    Midpoint  = midpoint,\n    Lower_90  = lower_bound,\n    Upper_90  = upper_bound\n  )\n\n# journal tiers\n\nwilliams_tiers <- llm_tiers_long |>\n  filter(paper == \"Williams et al. 2024\") |>\n  mutate(\n    Measure = recode(\n      tier_kind,\n      tier_should = \"Deserved journal tier (should)\",\n      tier_will   = \"Predicted journal tier (will)\"\n    )\n  ) |>\n  transmute(\n    Measure,\n    Score    = score,\n    Lower_90 = ci_lower,\n    Upper_90 = ci_upper\n  )\n\n# text blobs from JSON\n\nwilliams_assessment       <- williams_row$assessment_summary\nwilliams_reasoning_full   <- williams_row$reasoning_full\nwilliams_reasoning_steps  <- llm_reasoning_steps |>\n  filter(paper == \"Williams et al. 2024\")\n```\n:::\n\n\n\n\n### Qualitative comparison: Williams et al. (2024)\n\nIn the refined run, GPT‑5 Pro reads about \n28,704 input tokens and produces \n6,327 visible output tokens plus \n5,120 reasoning tokens.  \nAt current API prices this evaluation costs roughly \n$1.80.\n\nThe table below shows the model’s percentile ratings and 90% credible intervals for the Unjournal criteria.\n\n\n\n\n\n```{.r .cell-code}\nwilliams_metrics |>\nmutate(across(c(Midpoint, Lower_90, Upper_90), round, 1)) |>\nknitr::kable(\ncol.names = c(\"Criterion\", \"Midpoint\", \"Lower 90%\", \"Upper 90%\"),\nalign = c(\"l\",\"c\",\"c\",\"c\")\n) |>\nkableExtra::kable_styling(full_width = FALSE)\n```\n\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Criterion </th>\n   <th style=\"text-align:center;\"> Midpoint </th>\n   <th style=\"text-align:center;\"> Lower 90% </th>\n   <th style=\"text-align:center;\"> Upper 90% </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Overall assessment </td>\n   <td style=\"text-align:center;\"> 86 </td>\n   <td style=\"text-align:center;\"> 86 </td>\n   <td style=\"text-align:center;\"> 86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Claims &amp; evidence </td>\n   <td style=\"text-align:center;\"> 78 </td>\n   <td style=\"text-align:center;\"> 78 </td>\n   <td style=\"text-align:center;\"> 78 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Methods </td>\n   <td style=\"text-align:center;\"> 74 </td>\n   <td style=\"text-align:center;\"> 74 </td>\n   <td style=\"text-align:center;\"> 74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Advancing knowledge and practice </td>\n   <td style=\"text-align:center;\"> 92 </td>\n   <td style=\"text-align:center;\"> 92 </td>\n   <td style=\"text-align:center;\"> 92 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Logic and communication </td>\n   <td style=\"text-align:center;\"> 84 </td>\n   <td style=\"text-align:center;\"> 84 </td>\n   <td style=\"text-align:center;\"> 84 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Open, collaborative, replicable science </td>\n   <td style=\"text-align:center;\"> 63 </td>\n   <td style=\"text-align:center;\"> 63 </td>\n   <td style=\"text-align:center;\"> 63 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Relevance to global priorities </td>\n   <td style=\"text-align:center;\"> 94 </td>\n   <td style=\"text-align:center;\"> 94 </td>\n   <td style=\"text-align:center;\"> 94 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n\n\n```{.r .cell-code}\nwilliams_tiers |>\n  mutate(across(c(Score, Lower_90, Upper_90), round, 2)) |>\n  knitr::kable(\n    col.names = c(\"Measure\", \"Score\", \"Lower 90%\", \"Upper 90%\"),\n    align = c(\"l\",\"c\",\"c\",\"c\")\n  ) |>\n  kableExtra::kable_styling(full_width = FALSE)\n```\n\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Measure </th>\n   <th style=\"text-align:center;\"> Score </th>\n   <th style=\"text-align:center;\"> Lower 90% </th>\n   <th style=\"text-align:center;\"> Upper 90% </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Deserved journal tier (should) </td>\n   <td style=\"text-align:center;\"> 4.4 </td>\n   <td style=\"text-align:center;\"> 4.4 </td>\n   <td style=\"text-align:center;\"> 4.4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Predicted journal tier (will) </td>\n   <td style=\"text-align:center;\"> 4.8 </td>\n   <td style=\"text-align:center;\"> 4.8 </td>\n   <td style=\"text-align:center;\"> 4.8 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n\n\n\n\n::: {.callout-tip}\n#### Model reasoning trace – Williams et al. 2024\n\n::: {.small}\n**Analyzing model limitations**\n\nI see another limitation in the model's assumptions. They assumed stationarity from 2000-2016 when predicting for 2015 to 2030. They included present conditions like 2018 tree cover but didn't factor in future climate or socio-economic changes. By excluding socio-economic predictors, they reduced potential model improvements slightly, but this could bias predictions in areas where socio-economic factors play a significant role, potentially leading to unrealistic outcomes in \"biophysical potential.\" They clarify it’s about modeling potential, not feasibility.\n\n**Examining dataset limitations**\n\nI notice another issue with the primary regrowth dataset used, which came from Global Forest Watch to map regrowth over 5 m. In humid tropics, the producer's accuracy is low at just 18.7% because of omission errors, although user accuracy is high at 85.1%. The detection biases may skew the training sample and affect model calibration. Their conservative approach to mapping regrowth likely favors more accessible and larger areas, and many small regrowth patches are excluded. It’s unclear if the training sample is representative across different biomes without proper stratification.\n\n**Clarifying domain definitions**\n\nI need to define the domain used in the study. They included all parts of forest biomes within ±25° but excluded water, bare land, urban areas, and so on, while purposely opting for a liberal domain. They used 2018 tree cover to exclude forests but may misclassify certain savannas or drylands as part of forest biomes. While they mention not including savannas, their definition could still capture mosaic-savanna zones. Additionally, there's a scale mismatch since predictions at a finer 30 m resolution are computed alongside coarser predictors, which could impact accuracy.\n\n**Assessing the weighted area approach**\n\nI see another issue with the weighted area approach, where the probability of regrowth times pixel area is calculated. However, the random forest classification is trained on presence/absence from 2000-2016, and the output probabilities aren't properly calibrated. Calibration techniques like Platt scaling or isotonic regression are necessary to interpret results as probabilities. They also didn't present AUC, only accuracy, which may mislead since balanced sampling artificially sets prevalence to 0.5. Without proper probability calibration, the estimate of expected area could be questionable and not reflect actual conditions.\n\n**Reviewing definitions and calibration**\n\nThey used specific definitions, claiming that \"the continuous potential\" can be seen as the probability of natural regeneration per pixel, with the weighted-area value representing the expected area for regeneration. However, they didn't perform any calibration on this. Their accuracy and out-of-bag accuracy are reported as 0.878, but they failed to discuss probability calibration, which is important. Regarding carbon sequestration potentials, they calculated totals and ranges from previous datasets, but it’s unclear where the range comes from—possibly from Monte Carlo methods or carbon dataset quartile variation, yet they didn't show how they computed uncertainty.\n\n**Identifying dataset uncertainties**\n\nThe dataset presents uncertainties by grid cell, but their method multiplies 30-year totals without mentioning how uncertainty is handled. The \"range\" they provide likely comes from minimum and maximum values across these uncertainties, which is unclear. They need to clarify how uncertainty propagates and address independence and omitted below-ground carbon, since they reference adding 22–28% from literature. Additionally, they didn't consider factors like degradation baseline, leakage, or permanence. While they frame the \"biophysical potential,\" the abstract could mislead policy implications with its stated above-ground carbon sequestration potential of 23.4 Gt C over 30 years.\n\n**Checking carbon sequestration claims**\n\nI noticed they mentioned that \"new carbon sinks could increase global sequestration by 14.3% per year\" but also stated \"mitigating current pantropical forest carbon losses by 90.5% per year,\" which might be inconsistent. They claim assisted natural regeneration can sequester significant carbon but didn't show how they calculated this. It seems to derive from 23.4 Gt over 30 years, equating to 0.78 Gt C per year. With a net carbon loss of about 0.86 Gt C per year, I need to verify Baccini 2017's values, which reported a net carbon source of 1.1 Gt C per year. My memory on this is a bit fuzzy!\n\n**Evaluating carbon loss mitigation claims**\n\nThey state that \"mitigating current pantropical forest carbon losses by approximately 90.5% per year,\" referencing Baccini, raises some questions. If regeneration can offset about 0.8 Gt per year, that suggests total losses of around 0.9 Gt per year. While this might be plausible, there are logical leaps involved since their potential benefits might not actually be realized. It's a bit tricky to interpret, yet they present it confidently as potential. I need to consider these assumptions carefully!\n\n**Analyzing dataset issues**\n\nThey wisely exclude plantation regrowth from their analysis, but it seems their training dataset might still contain misclassified plantations. They reported a previous algorithm with 90.6% accuracy; however, an area-based producer accuracy of just 18.7% in humid tropics suggests significant under-detection. This non-random classification error could undermine generalizability. While they argue for the robustness of the detected patches, the validation accuracy of 87.9% relies on potentially biased training dataset presence points. Field validation is unlikely, and poor accuracy in Southeastern Asia raises another red flag.\n\n**Evaluating modeling assumptions**\n\nThe modeling approach assumes that predicted 'potential' is valid from 2015 to 2030 without considering changes, which is questionable since the climate is constantly shifting. Additionally, the \"distance to forest\" will likely change as regrowth occurs, yet they didn't model this dynamic feedback. They mentioned underestimating impacts like positive feedback and the exclusion of savanna/dryland. Climate change could also hinder regrowth, leading to ambiguity. They present a figure showing \"available for restoration\" area by country, but they don't clarify how this was calculated, leaving some mystery around the methodology used.\n\n**Analyzing definitions and reproducibility**\n\nThey define the study's domain as all tropical forest biomes, excluding areas like water, urban, and existing forests, while including factors like 'distance to water' as covariates. It seems that \"available for restoration\" refers to the total area of this domain, but their disclaimers indicate some harvest/regrowth classifications might be included. Although they mention using a forestry dataset, they didn’t clearly define \"available for restoration.\" Additionally, requesting code via email isn’t ideal; it should be publicly accessible. While they use accessible datasets, the lack of code hampers reproducibility, and the confidence intervals aren’t credible.\n\n**Evaluating accuracy and methodology**\n\nThey mention the model overcame potential overfitting with an out-of-bag (OOB) accuracy of 87.8%. They used a balanced sample and omitted socioeconomic features due to similar accuracy, but I wonder if the limited role of socioeconomic data in predicting presence versus absence, as indicated by remote sensing, could affect results. They also disclaim that the maps are only one input, which is good. Since this is published in Nature, it seems high quality, but I believe we should look beyond that. We need to propose credible metrics.\n\n**Ranking evaluation criteria**\n\nBased on the content, I would rank high—maybe around the 85th percentile overall.\n\nFor claims and evidence, they articulate strong claims and provide robust mapping and a predictive model. However, there are issues like reliance on one training dataset with major omissions and unrealistic confidence intervals. I’d place this at about 75.\n\nFor methods, they've implemented random forest modeling and accounted for autocorrelation, yet there’s heavy reliance on coarse data. This leads me to perhaps a 70.\n\nRegarding advancing knowledge, their maps on natural regeneration potential are valuable, globally relevant for restoration planning, so I’d mark this as a 90.\n\nOn logic and communication, while the writing is clear and disclaimers are present, there are leaps in concluding about carbon losses and some misuses of \"CI.\" I’d score this around 82.\n\nFor open science, while the data is available on Zenodo and the input data is open, the code is only available on request, which limits replicability. I’d give a score of 65.\n\nFinally, in terms of global relevance, this work ties in well with global priorities and quantifies information by country, so I’d rate this a solid 92.\n\n**Assigning tier ratings**\n\nI need to assign \"tier_should\" and \"tier_will\" ratings from 0 to 5. Based on quality, I think \"should\" deserves a high score, perhaps around 4.5 or 4.7, due to some identified issues. For \"will,\" since it’s already published in Nature, I’d predict around 4.6 to 4.8. However, they want continuous numbers, so I’ll set \"should\" at 4.4 and \"will\" at 4.7. \n\nIf I consider possible underestimation due to method-related issues, perhaps \"should\" could be 4.2 or 4.4, while \"will\" could be 4.8 because of strong interest. Still, the dataset not being open-coded is a consideration, but it’s published in Nature, so I want to remain fair. \n\nNow, I need to craft a summary, aiming for around 1000 words. I’ll start by summarizing the approach and identifying major issues.\n\n**Highlighting dataset issues**\n\nI’m noticing that the training dataset has biases, especially regarding natural regrowth mapping, which shows significant omissions in humid biomes. This can lead to a non-random sample of presences, and the model might learn from incomplete detections, distorting variable relationships and potentially mispredicting areas.  \n\nNext, regarding probability calibration and the \"expected area\" method, they treat random forest outputs as probabilities, but without proper calibration and baseline prevalence, the resulting estimates of 215 million hectares could be inaccurate. They present highly precise confidence intervals that don't seem credible because they didn’t account for uncertainty across inputs.  \n\nLastly, concerning spatial autocorrelation, while they tried to evaluate it, accuracy tends to decline with distance, so their optimism might not be justified. They didn’t apply block cross-validation and reported overall accuracy, avoiding metrics like AUC. The balanced classification gives an unrealistic sense of accuracy, and there are no confusion matrices or biome-specific metrics presented beyond overall accuracy.\n\n**Addressing scale and domain issues**\n\nI see that there’s a scale mismatch with the 30 m predictions derived from 300 m to 1 km predictors, which could cause smoothing and artifacts. They’ve acknowledged this with visual evidence in Fig. 1, suggesting it could impact local planning as they consider these results to be preliminary.  \n\nThe domain definition appears overly broad, including areas where natural regeneration may be undesirable—like non-forest ecosystems, productive farmland, or communities—which could potentially mislead their \"available for restoration\" metrics.  \n\nLastly, they assume conditions from 2000 to 2016 without modeling climate change or socioeconomic factors, which could really undermine their 2030 predictions. Also, the carbon accumulation overlay from the Cook-Patton dataset has a scale mismatch issue, with 1 km data being downscaled to 30 m without considering edges and heterogeneity.\n\n**Evaluating carbon accounting and code accessibility**\n\nThey don't initially account for below-ground carbon, indicating an additional 22–28% should be included, and they don't consider the risk of reclearance. Their discussions on leakage and additionality seem inadequate since their yearly rates are used to support claims of significant offsetting of losses, which likely exaggerates realized mitigation.\n\nWhile they exclude plantations, algorithmic misclassification might still exist, and they haven’t incorporated a classification probability step. \n\nRegarding code accessibility, it's not publicly available, making replication difficult. Although they provide predicted maps on Zenodo, the training and modeling code needing to be requested further hampers replicability. Additionally, using 2018 tree cover for distance predictions while training on 2000 cover could lead to variable shifts, potentially degrading the predictions since they haven't cross-validated changes.\n\n**Observing assumptions and suggestions for improvement**\n\nThe weighted area assumption sums across all pixels to determine \"expected area,\" but it overlooks spatial dependence and positive feedback, which are mentioned but not integrated. The country-level summarization is good, although it uses administrative boundaries without addressing ecoregion differences, and there's a lack of error budget for these country-level sums.\n\nThey note that predictions may degrade in Southeastern Asia, but don't confirm this for later years. For improvements, I should suggest block cross-validation, probability calibration via isotonic regression, and incorporating socioeconomic variables. \n\nOverall, I thought the score should be around 85, but I'm reconsidering an 86.\n\n**Finalizing ranking and summary considerations**\n\nI'm setting the overall score to 86, considering it's intended to reflect percentiles within [0, 100]. For research in remote sensing global mapping, I'd lean toward an 88, but due to certain issues, I might adjust this to 82. However, advancing knowledge could be rated higher at 90, leading me to average around 86, so choosing 87 seems fair. \n\nI also need to fill out the JSON carefully, ensuring that the assessment summary is a concise paragraph of about 800-1100 words and omitting any author names or external knowledge outside the provided PDF. The confidence intervals will match the midpoint of 86. For claims and evidence, I'm thinking a score of 79?\n\n**Setting scores and producing JSON details**\n\nI’ve decided to set the score of 78 for overall assessment, with specifics for other categories: 73 for methods, 92 for advancing knowledge, 84 for logic communication, 62 for open science, and 94 for global relevance. For tier should, I’ll use a score of 4.4, and for tier will, I’ll set it at 4.8, all while keeping consistency in mind with overall scores at 86, advancing at 92, and global relevance at 94. \n\nThe JSON needs to include an assessment summary of around 1,000 words and metrics for each score calculated with midpoint, lower, and upper bounds. I’ll also avoid naming authors or their institutions, but I can reference previous analyses without using specific names or citation numbers.\n\n**Crafting the summary points**\n\nI'm going to paraphrase the training dataset description, ensuring I accurately reference the analysis of natural regrowth from 2000 to 2016 without including citation numbers. I should say, “The study builds on an existing pantropical remote sensing analysis that mapped natural regrowth and distinguished it from plantations.”  \n\nNext, I want to finalize the summary by outlining key points. For data, I’ll mention potential mapping across tropical forests at a 30 m resolution, calculating weighted areas for 215 Mha, and noting variations across ecoregions.  \n\nFor methods, I’ll detail the balanced random forest approach, variable selection, the training dataset derived from regrowth mapping, and the overall accuracy. I’ll also highlight potential issues like omissions in the training dataset and domain definition relative to thresholds.\n\n**Assessing model limitations**\n\nI need to go over some aspects of the model's predictive capabilities. First, I see that there's a lack of probability calibration in the balanced training; the predicted probabilities aren’t calibrated, which might lead to bias in the weighted area approach and unrealistic confidence intervals. \n\nNext, there’s the issue of carbon overlay and uncertainty propagation; downscaling seems to be done naively. The coarser covariate resolution could degrade the 30 m predictions, causing blockiness. Socio-economic variables are omitted in terms of feasibility versus potential, and there’s a failure to integrate dynamic processes like climate change impacts.  \n\nI should also note that accuracy in Southeastern Asia is poor, and the code's availability is limited, impacting replicability due to the heavy dataset. \n\nLastly, I’ll mention that the additional risk of reclearance isn’t accounted for in the carbon measurements. I need to finalize this while including suggestions and discussing uncertainties qualitatively as we maintain strict JSON formatting.\n:::\n:::\n\n::: {.callout-note}\n#### Diagnostic assessment – Williams et al. 2024\n\n::: {.small}\nThis paper delivers a high-resolution, pantropical map of where deforested lands have biophysical potential to undergo natural forest regeneration, and it aggregates this to country and biome summaries and an overlay-based carbon removal potential. The approach is transparent in intent—predicting potential, not feasibility—but several methodological choices and data limitations affect inference strength and precision. The dependent variable (presence/absence of natural regeneration during 2000–2016) is derived from a prior global remote-sensing classification that conservatively emphasized omission errors in humid regions and required a minimum patch size; area-based producer accuracy for natural regrowth in the humid biome was low while user accuracy for detected patches was high. This means the positive training class is a non-random subset of true regrowth, with systematic under-detection likely concentrated in particular forest types, geographies, and patch sizes. The predictive model then inherits these detection and sampling biases, potentially distorting learned relationships and shifting mapped potential away from contexts where regrowth was common but poorly detected. Model fitting uses random forests on millions of stratified points, with a sensible variable-selection procedure; predictions are ultimately based on biophysical predictors alone because a combined socioecological model yielded very similar accuracy. Omitting socioeconomic predictors makes the maps more temporally stable and widely available, but it also reframes outputs as “biophysical potential” and risks overpredicting potential in places where human pressure or governance constraints are binding. Validation emphasizes overall accuracy around 88% using a balanced validation set; however, accuracy declines at moderate distances from training locations, indicating residual spatial autocorrelation and some optimism in headline accuracy. The use of accuracy rather than threshold-free metrics, and the balancing of classes (prevalence set to 50%) during training and validation, further complicates interpretation of both accuracy and the calibration of predicted probabilities. The paper interprets the continuous model output as a probability and converts this to “expected area” by multiplying by pixel area and summing, yielding 215 Mha. Yet the calibration of random-forest scores to true probabilities is not established; without calibration (e.g., isotonic or Platt scaling) under the real-world prevalence, the expected-area identity may be biased. The extraordinarily narrow “confidence intervals” reported for area (±0.22 Mha at global scale) are not credible statistical uncertainty; they reflect computational or rounding artifacts, not propagation of uncertainty from training data errors, cross-validated prediction error, input-layer uncertainty, or probability calibration. A threshold (>0.5) sensitivity shows a substantially larger area, highlighting dependence on interpretation choices. Many predictors are coarse (250 m–1 km) while predictions are at 30 m; this scale mismatch introduces blockiness and may overstate local precision. The domain of “available for restoration” is defined liberally by excluding only forests, water, urban, bare, and forestry areas; while appropriate for modeling, its presentation alongside country totals risks being misread as areas that are socially or legally available for reforestation. Temporal assumptions also matter: the model projects to 2015/2030 by assuming that 2000–2016 relationships hold and by updating some forest-context layers with 2018 data; it does not incorporate climate change trajectories, shifting fire regimes, or evolving socioeconomics that strongly affect regeneration and permanence. Carbon estimates are calculated by downscaling a 1 km accumulation dataset to 30 m, multiplying by modeled potential, and summing; the stated range lacks a clearly defined uncertainty model and appears not to propagate classification, calibration, or permanence risks. The paper is commendably explicit that realized sequestration will be lower due to leakage, reclearance, and policy constraints, yet some statements comparing potential annual removals to current pantropical losses could be interpreted too strongly given these omitted risks and uncertainties. Spatial performance is uneven, with lower accuracy in parts of Southeast Asia; country and biome summaries lack uncertainty decomposition, which would be useful for decision makers. On replicability, large gridded outputs are publicly archived and inputs are open, but code is available only upon request; combined with the volume of preprocessing, this raises barriers to full reproduction, code review, and extension. Strengths include a clearly articulated objective focused on actionable planning, a very large training sample, careful variable selection, explicit treatment of autocorrelation effects (even if not using full spatial block cross-validation), and extensive caveats about use and governance. The resulting layers will be highly valuable as a first-pass screen for assisted natural regeneration opportunities and for national restoration dialogues. The main improvements needed relate to uncertainty characterization (probability calibration, spatially explicit error propagation, and country-level error budgets), spatial cross-validation that reduces spatial leakage, clearer separation of statistical precision from conceptual ranges, dynamic scenario analysis including plausible climate and human-pressure changes, and full open release of code. Overall, this is an influential, practically useful mapping contribution with limitations that mainly concern probabilistic interpretation, uncertainty, and temporal and socioecological external validity.\n:::\n:::\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoken_cost_summary <- llm_master |>\n  transmute(\n    paper,\n    input_tokens,\n    output_tokens,\n    reasoning_tokens,\n    total_tokens    = input_tokens + output_tokens,\n    est_cost_usd    = input_tokens  * price_in_per_token +\n      (output_tokens + reasoning_tokens) * price_out_per_token\n  ) |>\n  arrange(desc(est_cost_usd))\n\ntoken_cost_summary_display <- token_cost_summary |>\n  mutate(\n    est_cost_usd = round(est_cost_usd, 2)\n  )\n\nknitr::kable(\n  token_cost_summary_display,\n  col.names = c(\n    \"Paper\", \"Input tokens\", \"Output tokens\", \"Reasoning tokens\",\n    \"Total tokens\", \"Est. cost (USD)\"\n  ),\n  align = c(\"l\",\"r\",\"r\",\"r\",\"r\",\"r\")\n)\n```\n\n::: {.cell-output-display}\n\n\n|Paper                | Input tokens| Output tokens| Reasoning tokens| Total tokens| Est. cost (USD)|\n|:--------------------|------------:|-------------:|----------------:|------------:|---------------:|\n|Peterman et al. 2025 |        18762|          7617|             6208|        26379|            1.94|\n|Adena and Hager 2024 |        24234|          7019|             5312|        31253|            1.84|\n|Williams et al. 2024 |        28704|          6327|             5120|        35031|            1.80|\n|Kudymowa et al. 2023 |        65096|          4147|             2432|        69243|            1.77|\n|Green et al. 2025    |        22904|          5884|             3904|        28788|            1.52|\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}