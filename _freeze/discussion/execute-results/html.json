{
  "hash": "fd384eb715553ea74813caf915d438d6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Discussion\"\nformat: html\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Include global setup and parameters\"}\nsource(\"setup_params.R\")\n```\n:::\n\n\n\n## Preliminary conclusions\n\nTaken together, our first results suggest that GPT‑5 Pro can mimic parts of the structure of expert research evaluation but is not yet a drop‑in replacement for human reviewers.\n\nQuantitatively, the model’s overall ratings correlate positively with the Unjournal evaluators’ mean scores, with substantial residual disagreement. The LLM also exhibits systematic score inflation: it rarely assigns low percentiles, compresses most papers into the upper part of the scale, and produces higher mean overall ratings (86.5 vs 74.4 for humans).\n\nAcross criteria, differences vary by paper and dimension. In several cases humans rate methodological rigour and strength of evidence substantially lower than the model, while the LLM is closer to human scores on global relevance and advancing knowledge. Credible intervals are often narrower for the model than for humans, especially on methods and claims, suggesting that GPT‑5 Pro expresses more confidence than human evaluators in areas where it may not reliably detect all issues.\n\nQualitatively, the @Williams2024 case study illustrates both strengths and limitations. The model’s diagnostic summary and reasoning trace surface many of the same concerns as the two Unjournal reviewers: biased and incomplete regrowth data, temporal leakage from contemporaneous predictors, uncalibrated random‑forest outputs driving the key 215 Mha estimate, under‑stated uncertainty, coarse predictors at 30‑m resolution, and a liberal definition of land “available for restoration”, as well as barriers to full replication. Yet GPT‑5 Pro still assigns high quantitative ratings, whereas human evaluators give much lower methods and claims scores and describe the headline estimate as low‑credibility without major revisions.\n\nIn this configuration, the model therefore seems capable of reproducing detailed, technically informed critiques and uncertainty language while under‑penalising those issues in its numeric scoring and expressed uncertainty. This gap between qualitative diagnosis and quantitative judgment is an important target for further work.\n\n\n## Limitations\n\nOur findings are preliminary and subject to several limitations.\n\nFirst, the sample is small and highly selected. We currently analyse around 40–50 Unjournal evaluation packages plus four focal papers for which we ran the richer “reasoning + summary” configuration. All are social‑science papers that The Unjournal had already judged to be promising or high‑impact, and they are mostly empirical and policy‑relevant. This is not a random sample of research and our results may not generalise to other disciplines, to less polished work, or to submissions outside The Unjournal’s remit. \n\nSecond, our “ground truth” is itself noisy. Unjournal evaluations typically involve only one to three expert reviewers per paper, who often disagree substantially on both the numerical ratings and the narrative assessment. We aggregate across evaluators and treat the resulting averages as a baseline, but this is only a moving, human‑defined target rather than a definitive measure of quality. When the model diverges from the human scores, we cannot yet say whether it is “wrong”, picking up real issues that humans missed, or simply expressing a different but defensible judgment. \n\nThird, we study a single model and a small number of prompts. Most quantitative results come from an earlier October 2025 run that only elicited 0–100 ratings and credible intervals via a compact JSON schema. The detailed qualitative comparisons and token‑cost analyses draw on a later run that uses a longer diagnostic prompt and high‑effort reasoning, for only four focal papers. Scores are therefore conditional on one model family (GPT‑5 Pro), one set of prompt choices, and single draws per paper; we have not yet explored other models, alternative prompting strategies, or multi‑run aggregation, all of which could change both level and calibration of the ratings. \n\nFourth, we cannot fully rule out training‑data contamination. For the papers analysed so far, the model may have encountered the manuscripts, related discussions, or even fragments of Unjournal evaluations during pre‑training or fine‑tuning. Note, however, that many papers and evaluations were published after the current models knowledge cutoff at 30 September 2024.\n\nFifth, our numerical metrics provide only a coarse view of alignment. The current analysis focuses on correlations, mean absolute error, Krippendorff’s alpha, and simple differences between human and model midpoints. These statistics treat a 5–10 point gap as a failure even when the underlying qualitative judgments are very similar, and they do not yet exploit the full information in the stated 90% intervals (for example via proper scoring rules or coverage checks). We also see that GPT‑5 Pro tends to report narrow credible intervals, often narrower than human evaluators’ ranges, suggesting over‑confidence that we have not yet systematically quantified. \n\nSixth, the qualitative comparisons are based on a tiny number of cases and informal coding. For Williams et al. (2024) we compare a single LLM assessment to two human reports and read them side‑by‑side to identify overlapping and unique critiques. This shows substantial convergence in the issues raised, but it remains an anecdotal comparison. We have not yet run blinded human adjudication on LLM‑identified problems, nor systematically coded “missed issues” or hallucinated concerns across a larger set of papers. \n\nFinally, we have only begun to probe bias and high‑stakes implications. Prior work suggests that LLMs can exhibit systematic favouritism towards prestigious authors and institutions; our current experiments do not anonymize papers or vary author information, so we cannot yet say whether similar effects arise in this setting. Likewise, we have not evaluated how an LLM‑assisted reviewing workflow would affect downstream publication, funding, or policy decisions, or whether users would actually trust and act on these evaluations. Those questions will require additional experimental designs and collaboration with editors, funders, and practitioners.\n\n## Next steps\n\nThis paper is intended as a first step: a proof‑of‑concept benchmark linking frontier LLM evaluations to an existing human review pipeline. Several extensions are already in progress or planned.\n\nFirst, we will broaden coverage. The current analysis uses 47 completed Unjournal evaluation packages and a small set of focal papers with full reasoning traces. We plan to add more papers (including new evaluations as they are completed), expand beyond economics and social policy, and incorporate additional LLM families and model sizes. This will let us separate model‑class effects from idiosyncrasies of a single system.\n\nSecond, we will sharpen the evaluation metrics. Beyond simple correlations and mean absolute errors, we plan to use proper scoring rules for interval forecasts and journal‑outcome predictions, examine stability across repeated runs and prompt variants, and quantify how often models detect the most serious issues identified by humans versus raising concerns that humans did not mention. For the focal papers we will explicitly code both human reports and model summaries into issue categories to compare recall and precision.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}