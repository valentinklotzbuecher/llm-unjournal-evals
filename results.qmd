---
title: "Results"
format: 
  html: default
  # pdf: default
engine: knitr
--- 

```{r} 
#| label: setup call
#| code-summary: "Include global setup and parameters"

source("setup_params.R")

```




Here we present preliminary results, starting with a comparison of the LLM‑generated quantitative ratings (model: `gpt-5-pro`, see the[(previous section](methods.qmd)) with human evaluations across [the Unjournal's criteria](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#undefined-1). 






```{r}
#| label: setup-libs

library("tidyverse")
library("janitor")
library("stringr")
library("lubridate")
library("here") 
library("knitr")
library("kableExtra")
library("ggforce") 
library("ggrepel")
library("glue")
# library("ggalluvial")
library("scales")
# library("ggbreak")
library("jsonlite")
library("purrr")
library("tibble")



UJ_ORANGE <- "#f19e4b"   # LLM
UJ_GREEN  <- "#99bb66"   # Human
UJ_BLUE <- "#4e79a7"   # GPT‑5 (legacy)

theme_uj <- function(base_size = 11) {
  theme_minimal(base_size = base_size) +
    theme(
      panel.grid.minor = element_blank(),
      plot.title.position = "plot",
      legend.position = "bottom"
    )
}

# theme_uj <- function(base_size = 11) {
#   theme_minimal(base_size = base_size) +
#     theme(
#       panel.background = element_rect(fill = "#050608", colour = NA),
#       plot.background  = element_rect(fill = "#050608", colour = NA),
#       panel.grid.minor = element_blank(),
#       panel.grid.major = element_line(colour = "grey25"),
#       axis.text        = element_text(colour = "grey85"),
#       axis.title       = element_text(colour = "grey95"),
#       plot.title       = element_text(colour = "grey98"),
#       legend.background= element_rect(fill = "#050608", colour = NA),
#       legend.key       = element_rect(fill = "#050608", colour = NA),
#       legend.text      = element_text(colour = "grey90"),
#       legend.title     = element_text(colour = "grey95"),
#       plot.title.position = "plot",
#       legend.position  = "bottom"
#     )
# }


 
```


```{r}
#| label: load-data


# paper_authors <- read_delim("data/paper_authors.csv", delim = ",")

# Mapping paper keys - short titles
UJmap <- read_delim("data/UJ_map.csv", delim = ";") |>
  mutate(label_paper_title = research,
         label_paper = paper) |>
  select(c("label_paper_title", "label_paper"))


# Unjournal ratings
rsx <- read_csv("data/rsx_evalr_rating.csv", show_col_types = FALSE) |> 
  clean_names()  |>
  mutate(label_paper_title = research) |>
  select(-c("research"))


# UJ evaluated research
research <- read_csv("data/research.csv", show_col_types = FALSE) |>
  clean_names() |>
  filter(status == "50_published evaluations (on PubPub, by Unjournal)") |>  
  left_join(UJmap, by = c("label_paper_title")) |>
  mutate(doi = str_trim(doi)) |>
  mutate(label_paper = if_else(doi == "https://doi.org/10.3386/w31162", "Walker et al. 2023", label_paper, missing = label_paper)) |>
  mutate(label_paper = if_else(doi == "doi.org/10.3386/w32728", "Hahn et al. 2025", label_paper, missing = label_paper))  |>
  mutate(label_paper = if_else(doi == "https://doi.org/10.3386/w30011", "Bhat et al. 2022", label_paper, missing = label_paper))  |>
  mutate(label_paper = if_else(doi == "10.1093/wbro/lkae010", "Crawfurd et al. 2023", label_paper, missing = label_paper))  |>
  left_join(rsx, by = c("label_paper_title"))
 

jtiers_llm <- read_csv("data/tiers_long.csv", show_col_types = FALSE) |>
  mutate(middle_rating = score,
         lower_ci = lo,
         upper_ci = hi,
         criteria = if_else(tier_kind == "tier_will", "journal_predict", "merits_journal"),
         evaluator = "gpt-5",
         label_paper = paper
         ) |>
  select(c("label_paper", "evaluator", "middle_rating", "lower_ci", "upper_ci" , "criteria", "rationale"))


jtiers_uj <- research |>
  filter(criteria== "merits_journal" | criteria == "journal_predict") |>
  mutate(paper = label_paper,
         rationale = "")  |>
  select(c("label_paper", "evaluator", "middle_rating", "lower_ci", "upper_ci" , "criteria", "rationale"))


jtiers <- jtiers_uj |>
  rbind(jtiers_llm) |>
  mutate(human = if_else(evaluator == "o3", "Human", "o3"),
         lower_ci = if_else(lower_ci > 10, lower_ci/10, lower_ci))


# write_csv(all_ratings, "data/all_jtiers.csv")
write_rds(
  jtiers,
  "data/all_jtiers.rds",
  compress = "none"
  )

# utilities + clean columns for plotting

lane_offsets_center <- function(m, gap = 0.18) {
  if (m <= 0) return(numeric(0))
  if (m == 1) return(0)
  k <- floor((m - 1)/2)
  offs <- sort(c(-seq_len(k), 0, seq_len(k))) * gap
  offs[seq_len(m)]
}
lane_offsets_skip0 <- function(m, gap = 0.18) {
  if (m <= 0) return(numeric(0))
  if (m == 1) return(gap)
  k <- ceiling(m/2)
  offs <- sort(c(-seq_len(k), seq_len(k))) * gap
  offs[seq_len(m)]
}

jt_use <- jtiers |>
  transmute(
    label_paper,
    criteria,
    who = if_else(evaluator == "gpt-5", "LLM", "Human"),
    mid = as.numeric(middle_rating),
    lo  = suppressWarnings(as.numeric(lower_ci)),
    hi  = suppressWarnings(as.numeric(upper_ci))
  ) |>
  mutate(
    lo = ifelse(is.finite(lo), pmax(1, pmin(5, lo)), NA_real_),
    hi = ifelse(is.finite(hi), pmax(1, pmin(5, hi)), NA_real_)
  )

tier_metric <- "merits_journal"  # change "merits_journal" to "journal_predict" for the other slide




canon_metric <- function(x) dplyr::recode(
  x,
  "advancing_knowledge" = "adv_knowledge",
  "open_science"        = "open_sci",
  "logic_communication" = "logic_comms",
  "global_relevance"    = "gp_relevance",
  "claims_evidence"     = "claims",
  .default = x
)

# LLM metrics 
metrics_llm_full <- readr::read_csv(here("data","metrics_long.csv"), show_col_types = FALSE) |>
  janitor::clean_names() |>
  mutate(
    label_paper = stringr::str_replace(paper, "et al ", "et al. "),
    criteria    = canon_metric(metric),
    mid = as.numeric(midpoint),
    lo  = suppressWarnings(as.numeric(lower_bound)),
    hi  = suppressWarnings(as.numeric(upper_bound)),
    who = "LLM",
    evaluator = "gpt-5"
  )

# LLM metrics (for plotting)
metrics_llm <- metrics_llm_full |>
  transmute(label_paper, criteria, who, mid, lo, hi) |>
  mutate(
    lo = ifelse(is.finite(lo), pmax(0, pmin(100, lo)), NA_real_),
    hi = ifelse(is.finite(hi), pmax(0, pmin(100, hi)), NA_real_)
  )


# 1) Build a 1-row-per-title key from `research`
key_map <- research %>%
  transmute(label_paper_title = stringr::str_trim(label_paper_title),
            label_paper      = label_paper) %>%
  filter(!is.na(label_paper_title)) %>%
  distinct(label_paper_title, label_paper) %>%   # remove exact dups
  group_by(label_paper_title) %>%
  slice(1) %>%
  ungroup()

# Optional sanity check: any titles map to >1 label?
# research %>% distinct(label_paper_title, label_paper) %>%
#   count(label_paper_title) %>% filter(n > 1)

# 2) Join with relationship assertion
rsx_research <- rsx %>%
  mutate(label_paper_title = stringr::str_trim(label_paper_title)) %>%
  dplyr::left_join(key_map, by = "label_paper_title", relationship = "many-to-one")


metrics_human <- rsx_research |>
  mutate(criteria = canon_metric(criteria)) |>
  filter(criteria %in% c("overall","claims","methods","adv_knowledge","logic_comms","open_sci","gp_relevance")) |>
  transmute(
    label_paper,
    criteria,
    evaluator,
    who = "Human",
    mid = as.numeric(middle_rating),
    lo  = suppressWarnings(as.numeric(lower_ci)),
    hi  = suppressWarnings(as.numeric(upper_ci))
  ) |>
  filter(!is.na(label_paper), !is.na(mid)) |>
  mutate(
    lo = ifelse(is.finite(lo), pmax(0, pmin(100, lo)), NA_real_),
    hi = ifelse(is.finite(hi), pmax(0, pmin(100, hi)), NA_real_)
  ) %>% 
  # remove exact duplicates (protect against tiny float diffs by rounding first) -- but wait, what if human raters rated identically for a category?
  mutate(across(c(mid, lo, hi), ~ round(.x, 4))) |>
  distinct(label_paper, criteria, who, evaluator, mid, lo, hi, .keep_all = FALSE)


metrics_use <- bind_rows(metrics_human, metrics_llm)

matched_overall <- intersect(
  metrics_use %>% filter(who=="LLM", criteria=="overall") %>% pull(label_paper),
  metrics_use %>% filter(who=="Human", criteria=="overall") %>% pull(label_paper)
)
metrics_use <- metrics_use %>% mutate(is_matched_overall = label_paper %in% matched_overall)


```




## Quantitative comparison: human vs. GPT‑5 Pro

We first use the earlier GPT‑5 Pro evaluation run that covered all papers in our Unjournal sample with a simpler JSON‑schema prompt. @fig-forest-overall shows the overall percentile ratings from this initial run, averaged across human evaluators and compared to the LLM’s “overall” scores for each paper.


```{r}
#| label: fig-forest-overall
#| fig-cap: "Comparison of Human vs LLM overall percentile ratings"
#| fig-width: 14
#| fig-height: 6


highlight_paper <- "Kremer et al. 2022 XXX" # Select here to annotate

matched <- intersect(
  metrics_use %>% filter(who=="LLM",   criteria=="overall") %>% pull(label_paper),
  metrics_use %>% filter(who=="Human", criteria=="overall") %>% pull(label_paper)
)

H_ind <- metrics_human %>%
  filter(criteria=="overall", label_paper %in% matched) %>%
  mutate(lo = ifelse(is.finite(lo), pmax(0,lo), NA_real_),
         hi = ifelse(is.finite(hi), pmin(100,hi), NA_real_))

ord <- H_ind %>%
  group_by(label_paper) %>%
  summarise(h_mean = mean(mid, na.rm=TRUE), .groups="drop") %>%
  arrange(desc(h_mean)) %>% mutate(pos = row_number())

H_plot <- H_ind %>%
  inner_join(ord, by="label_paper") %>%
  group_by(label_paper) %>%
  mutate(off = (row_number() - (n()+1)/2) * 0.18,
         x   = pos + off) %>% ungroup()

# per-paper human mean and LLM summary
H_pp <- H_ind %>% group_by(label_paper) %>% summarise(h_mean = mean(mid), .groups="drop")
L_c <- metrics_llm %>%
  filter(criteria=="overall", label_paper %in% matched) %>%
  group_by(label_paper) %>%
  summarise(mid = mean(mid, na.rm=TRUE),
            lo  = suppressWarnings(min(coalesce(lo, mid), na.rm=TRUE)),
            hi  = suppressWarnings(max(coalesce(hi, mid), na.rm=TRUE)),
            .groups="drop") %>%
  inner_join(ord, by="label_paper") %>%
  mutate(x = pos)

lab <- L_c %>% filter(label_paper == highlight_paper)

# overall means to show as horizontal reference lines
hbar <- mean(H_pp$h_mean, na.rm=TRUE)
lbar <- mean(L_c$mid,     na.rm=TRUE)

ggplot() +
  geom_vline(data = ord, aes(xintercept = pos), color="grey92", linewidth=0.3) +
  # mean lines
  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = "dotted", linewidth = 0.8) +
  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = "dotted", linewidth = 0.8) +
  # humans
  geom_errorbar(data = subset(H_plot, is.finite(lo)&is.finite(hi)),
                aes(x=x, ymin=lo, ymax=hi),
                width=0, linewidth=1, alpha=0.5, color=UJ_GREEN) +
  geom_point(data = H_plot, aes(x=x, y=mid), size=3.0, alpha=0.9, color=UJ_GREEN) +
  # LLM
  geom_errorbar(data = subset(L_c, is.finite(lo)&is.finite(hi)),
                aes(x=x, ymin=lo, ymax=hi),
                width=0, linewidth=1.0, color=UJ_ORANGE) +
  geom_point(data = L_c, aes(x=x, y=mid), size=3.6, shape=18, color=UJ_ORANGE) +
  # label highlight
  geom_label_repel(data = lab,
                   aes(x = x, y = mid, label = label_paper),
                   min.segment.length = 0, seed = 1, size = 3.2,
                   fill = "white", label.r = unit(0.1,"lines")) +
  # x-axis paper labels
  scale_x_continuous(breaks = ord$pos, labels = ord$label_paper, expand = expansion(mult = c(0.01, 0.03))) +
  coord_cartesian(ylim = c(0,100), clip = "off") +
  labs(x=NULL, y="Percentile (0–100)") +
  theme_uj() +  
  annotate("text", x = 4, y = 40,
           label = sprintf("Means — Human: %.1f   LLM: %.1f", hbar, lbar),
           hjust = 0, size = 4) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),
        panel.grid.major.x=element_blank(),
        plot.margin = margin(5, 40, 5, 5))

```


```{r}
#| label: fig-forest-tiers-should
#| fig-cap: "Comparison of Human vs LLM journal tier ratings (should be published in)"
#| fig-width: 14
#| fig-height: 6

# Optional: set a paper to annotate; leave NA for none

highlight_paper <- NA_character_

HH <- jt_use %>% filter(criteria == tier_metric, who == "Human")
LL <- jt_use %>% filter(criteria == tier_metric, who == "LLM")
matched <- intersect(unique(HH$label_paper), unique(LL$label_paper))

H_t <- HH %>%
  filter(label_paper %in% matched) %>%
  mutate(
    lo = ifelse(is.finite(lo), pmax(1, lo), NA_real_),
    hi = ifelse(is.finite(hi), pmin(5, hi), NA_real_)
  )

ord_t <- H_t %>%
  group_by(label_paper) %>%
  summarise(h_mean = mean(mid, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(h_mean)) %>%
  mutate(pos = row_number())

H_tplot <- H_t %>%
  inner_join(ord_t, by = "label_paper") %>%
  group_by(label_paper) %>%
  mutate(off = (row_number() - (n() + 1) / 2) * 0.18,
         x   = pos + off) %>%
  ungroup()

L_t <- LL %>%
  filter(label_paper %in% matched) %>%
  group_by(label_paper) %>%
  summarise(
    mid = mean(mid, na.rm = TRUE),
    lo  = suppressWarnings(min(coalesce(lo, mid), na.rm = TRUE)),
    hi  = suppressWarnings(max(coalesce(hi, mid), na.rm = TRUE)),
    .groups = "drop"
  ) %>%
  inner_join(ord_t, by = "label_paper") %>%
  mutate(x = pos)

H_pp <- H_t %>% group_by(label_paper) %>% summarise(h_mean = mean(mid), .groups = "drop")
hbar <- mean(H_pp$h_mean, na.rm = TRUE)
lbar <- mean(L_t$mid,     na.rm = TRUE)

lab <- L_t %>% filter(label_paper == highlight_paper)
x_ann <- if (nrow(ord_t)) min(ord_t$pos) + 0.3 else 0

ggplot() +
  geom_vline(data = ord_t, aes(xintercept = pos), color = "grey92", linewidth = 0.3) +
  geom_hline(yintercept = hbar, color = UJ_GREEN,  linetype = "dashed", linewidth = 0.8) +
  geom_hline(yintercept = lbar, color = UJ_ORANGE, linetype = "dotted", linewidth = 0.8) +
  
  # Humans
  
  geom_errorbar(data = subset(H_tplot, is.finite(lo) & is.finite(hi)),
                aes(x = x, ymin = lo, ymax = hi),
                width = 0, linewidth = 1, alpha = 0.5, color = UJ_GREEN) +
  geom_point(data = H_tplot, aes(x = x, y = mid),
             size = 3.0, alpha = 0.9, color = UJ_GREEN) +
  
  # LLM
  
  geom_errorbar(data = subset(L_t, is.finite(lo) & is.finite(hi)),
                aes(x = x, ymin = lo, ymax = hi),
                width = 0, linewidth = 1.0, color = UJ_ORANGE) +
  geom_point(data = L_t, aes(x = x, y = mid),
             size = 3.6, shape = 18, color = UJ_ORANGE) +
  geom_label_repel(data = lab,
                   aes(x = x, y = mid, label = label_paper),
                   min.segment.length = 0, seed = 1, size = 3.2,
                   fill = "white", label.r = unit(0.1, "lines")) +
  scale_x_continuous(breaks = ord_t$pos, labels = ord_t$label_paper,
                     expand = expansion(mult = c(0.01, 0.03))) +
  coord_cartesian(ylim = c(1, 5), clip = "off") +
  labs(x = NULL, y = "Journal tier (1–5)") +
  theme_uj() +
  annotate("text", x = x_ann, y = 1.4,
           label = sprintf("Means — Human: %.2f   LLM: %.2f", hbar, lbar),
           hjust = 0, size = 4) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8),
        panel.grid.major.x = element_blank(),
        plot.margin = margin(5, 40, 5, 5))


```

@fig-heatmap-human-minus-llm shows a heatmap of the differences between human and LLM mean ratings across all evaluation criteria. Positive values (in green) indicate that humans rated the paper higher than the LLM, while negative values (in orange) indicate the opposite.

```{r}
#| label: fig-heatmap-human-minus-llm
#| fig-cap: "Heatmap of Human minus LLM mean ratings across evaluation criteria"
#| fig-width: 14
#| fig-height: 7


metric_order <- c("overall","claims","methods","adv_knowledge","logic_comms","open_sci","gp_relevance")
metric_lab <- c(
  overall="Overall",
  claims="Claims & evidence",
  methods="Methods",
  adv_knowledge="Adv. knowledge",
  logic_comms="Logic & comms",
  open_sci="Open science",
  gp_relevance="Global relevance"
)

H_mean <- metrics_use %>%
  filter(who=="Human", criteria %in% metric_order) %>%
  group_by(label_paper, criteria) %>%
  summarise(h = mean(mid, na.rm=TRUE), .groups="drop")

L_mean <- metrics_use %>%
  filter(who=="LLM", criteria %in% metric_order) %>%
  group_by(label_paper, criteria) %>%
  summarise(l = mean(mid, na.rm=TRUE), .groups="drop")

Ddiff <- inner_join(H_mean, L_mean, by=c("label_paper","criteria")) %>%
  mutate(diff = h - l) %>%  # positive = Human higher
  mutate(crit = factor(criteria, levels = metric_order, labels = metric_lab[metric_order]))

# order papers by overall difference (Human−LLM)
ord_p <- Ddiff %>%
  filter(criteria=="overall") %>%
  arrange(desc(diff)) %>%
  pull(label_paper)

ggplot(Ddiff, aes(x = factor(label_paper, levels = ord_p), y = crit, fill = diff)) +
  geom_tile(color = "white", linewidth = 0.25) +
  scale_fill_gradient2(low = UJ_ORANGE, mid = "grey95", high = UJ_GREEN, midpoint = 0,
                       name = "Human − LLM") +
  labs(x = NULL, y = NULL) +
  theme_uj() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1, vjust = 1, size = 8), 
        axis.text.y = element_text(size = 12),
        panel.grid = element_blank())

```



\clearpage

## Qualitative comparison: detailed GPT‑5 Pro evaluations

To understand what GPT‑5 Pro is actually responding to, we re‑ran the model on four focal papers [@Adena2024; @Peterman2024; @Green2025; @Williams2024] using a refined prompt.

This second run keeps the same quantitative metrics but additionally requires a diagnostic summary of about 1,000 words and high‑effort reasoning, with the full reasoning trace returned by the “thinking” model. For each paper we can therefore inspect:

- the LLM’s quantitative scores and journal‑tier predictions,
- the hidden reasoning steps used to arrive at those scores, and
- the token usage and approximate API cost of the evaluation.

@tbl-llm-token-cost-summary summarizes the token usage and estimated cost of each of these inferences.

We start by examining the Williams et al. (2024) evaluation in detail and then show the analogous summaries for the other four focal papers. In the next step we will juxtapose these LLM assessments with the human evaluators’ written reports.


```{r}
#| label: llm-json-from-responses

library(jsonlite)
library(purrr)
library(dplyr)
library(tidyr)
library(stringr)
library(tibble)
library(here)


gpt5pro_price_input <- 15 # input per 1M tokens ()
gpt5pro_price_output <- 120 # output per 1M tokens ()

price_in_per_token <- gpt5pro_price_input / 1e6
price_out_per_token <- gpt5pro_price_output / 1e6



`%||%` <- function(x, y) if (!is.null(x)) x else y

json_dir <- here("results", "json")
json_files <- list.files(json_dir, pattern = "\\.response\\.json$", full.names = TRUE)

parse_one_response <- function(path) {
  r <- jsonlite::fromJSON(path, simplifyVector = FALSE)
  paper <- basename(path) |> str_replace("\\.response\\.json$", "")
  
  # usage / tokens
  
  usage      <- r$usage %||% list()
  out_detail <- usage$output_tokens_details %||% list()
  
  # split reasoning vs message
  
  out_list       <- r$output %||% list()
  reasoning_obj  <- purrr::detect(out_list, ~ .x$type == "reasoning", .default = NULL)
  message_obj    <- purrr::detect(out_list, ~ .x$type == "message",   .default = NULL)
  
  reasoning_id    <- NA_character_
  reasoning_steps <- character()
  if (!is.null(reasoning_obj)) {
    reasoning_id <- reasoning_obj$id %||% NA_character_
    if (!is.null(reasoning_obj$summary) && is.list(reasoning_obj$summary)) {
      reasoning_steps <- purrr::map_chr(reasoning_obj$summary, "text")
    }
  }
  reasoning_full <- if (length(reasoning_steps)) paste(reasoning_steps, collapse = "\n\n") else NA_character_
  
  # final JSON payload with assessment + metrics
  
  assessment_summary <- NA_character_
  metrics_json       <- NULL
  if (!is.null(message_obj) && length(message_obj$content) > 0) {
    txt    <- message_obj$content[[1]]$text
    parsed <- jsonlite::fromJSON(txt, simplifyVector = TRUE)
    assessment_summary <- parsed$assessment_summary
    metrics_json       <- parsed$metrics
  }
  
  metrics_long <- tibble()
  tiers_long   <- tibble()
  if (!is.null(metrics_json)) {
    for (nm in names(metrics_json)) {
      if (nm %in% c("tier_should", "tier_will")) {
        tiers_long <- bind_rows(
          tiers_long,
          tibble(
            paper    = paper,
            tier_kind = nm,
            score    = metrics_json[[nm]]$score,
            ci_lower = metrics_json[[nm]]$ci_lower,
            ci_upper = metrics_json[[nm]]$ci_upper
          )
        )
      } else {
        metrics_long <- bind_rows(
          metrics_long,
          tibble(
            paper       = paper,
            metric      = nm,
            midpoint    = metrics_json[[nm]]$midpoint,
            lower_bound = metrics_json[[nm]]$lower_bound,
            upper_bound = metrics_json[[nm]]$upper_bound
          )
        )
      }
    }
  }
  
  master_row <- tibble(
    paper            = paper,
    model            = r$model,
    response_id      = r$id,
    created_at       = as.POSIXct(r$created_at, origin = "1970-01-01", tz = "UTC"),
    input_tokens     = usage$input_tokens %||% NA_integer_,
    output_tokens    = usage$output_tokens %||% NA_integer_,
    reasoning_tokens = out_detail$reasoning_tokens %||% NA_integer_,
    assessment_summary = assessment_summary,
    reasoning_full     = reasoning_full,
    reasoning_id       = reasoning_id
  )
  
  list(
    master         = master_row,
    metrics        = metrics_long,
    tiers          = tiers_long,
    reasoning_steps = tibble(
      paper = paper,
      step  = seq_along(reasoning_steps),
      text  = reasoning_steps
    )
  )
}

parsed <- purrr::map(json_files, parse_one_response)

llm_master          <- bind_rows(purrr::map(parsed, "master"))
llm_metrics_long    <- bind_rows(purrr::map(parsed, "metrics"))
llm_tiers_long      <- bind_rows(purrr::map(parsed, "tiers"))
llm_reasoning_steps <- bind_rows(purrr::map(parsed, "reasoning_steps"))

```


```{r}
#| label: tbl-llm-token-cost-summary
#| tbl-cap: "Estimated token usage and cost of GPT‑5 Pro evaluations"

token_cost_summary <- llm_master |>
  transmute(
    paper,
    input_tokens,
    output_tokens,
    reasoning_tokens,
    total_tokens    = input_tokens + output_tokens,
    est_cost_usd    = input_tokens  * price_in_per_token +
      (output_tokens + reasoning_tokens) * price_out_per_token
  ) |>
  arrange(desc(est_cost_usd))

token_cost_summary_display <- token_cost_summary |>
  mutate(
    est_cost_usd = round(est_cost_usd, 2)
  )

knitr::kable(
  token_cost_summary_display,
  col.names = c(
    "Paper", "Input tokens", "Output tokens", "Reasoning tokens",
    "Total tokens", "Est. cost (USD)"
  ),
  align = c("l","r","r","r","r","r")
)

```

```{r}
#| label: llm-williams-from-json

# pricing for GPT‑5 Pro (USD / 1M tokens, from OpenAI API docs)

gpt5pro_price_input  <- 15    # input per 1M tokens
gpt5pro_price_output <- 120   # output per 1M tokens

price_in_per_token  <- gpt5pro_price_input  / 1e6
price_out_per_token <- gpt5pro_price_output / 1e6

# Williams row

williams_row <- llm_master |>
  filter(paper == "Williams et al. 2024") |>
  slice(1)

# tokens + cost

williams_tokens <- williams_row |>
  transmute(
    input_tokens,
    output_tokens,
    reasoning_tokens,
    total_tokens    = input_tokens + output_tokens,
    est_cost_usd    = input_tokens  * price_in_per_token +
      (output_tokens + reasoning_tokens) * price_out_per_token
  )

# 0–100 metrics

metric_labels <- c(
  overall              = "Overall assessment",
  claims_evidence      = "Claims & evidence",
  methods              = "Methods",
  advancing_knowledge  = "Advancing knowledge and practice",
  logic_communication  = "Logic and communication",
  open_science         = "Open, collaborative, replicable science",
  global_relevance     = "Relevance to global priorities"
)

metric_order <- names(metric_labels)

williams_metrics <- llm_metrics_long |>
  filter(paper == "Williams et al. 2024",
         metric %in% metric_order) |>
  mutate(
    metric_label = metric_labels[metric],
    metric_label = factor(metric_label, levels = unname(metric_labels))
  ) |>
  arrange(metric_label) |>
  transmute(
    Criterion = metric_label,
    Midpoint  = midpoint,
    Lower_90  = lower_bound,
    Upper_90  = upper_bound
  )

# journal tiers

williams_tiers <- llm_tiers_long |>
  filter(paper == "Williams et al. 2024") |>
  mutate(
    Measure = recode(
      tier_kind,
      tier_should = "Deserved journal tier (should)",
      tier_will   = "Predicted journal tier (will)"
    )
  ) |>
  transmute(
    Measure,
    Score    = score,
    Lower_90 = ci_lower,
    Upper_90 = ci_upper
  )

# text blobs from JSON

williams_assessment       <- williams_row$assessment_summary
williams_reasoning_full   <- williams_row$reasoning_full
williams_reasoning_steps  <- llm_reasoning_steps |>
  filter(paper == "Williams et al. 2024")



```



### Qualitative comparison: Williams et al. (2024)

In the refined run, GPT‑5 Pro reads about 
`r scales::comma(williams_tokens$input_tokens)` input tokens and produces 
`r scales::comma(williams_tokens$output_tokens)` visible output tokens plus 
`r scales::comma(williams_tokens$reasoning_tokens)` reasoning tokens.  
At current API prices this evaluation costs roughly 
`r scales::dollar(williams_tokens$est_cost_usd, accuracy = 0.01)`.

The table below shows the model’s percentile ratings and 90% credible intervals for the Unjournal criteria.



```{r}
#| label: tbl-llm-williams-metrics
#| tbl-cap: "GPT‑5 Pro percentile ratings for Williams et al. (2024)"
#| results: 'asis'

williams_metrics |>
mutate(across(c(Midpoint, Lower_90, Upper_90), round, 1)) |>
knitr::kable(
col.names = c("Criterion", "Midpoint", "Lower 90%", "Upper 90%"),
align = c("l","c","c","c")
) |>
kableExtra::kable_styling(full_width = FALSE)

```


```{r}
#| label: tbl-llm-williams-tiers
#| tbl-cap: "GPT‑5 Pro journal tier ratings for Williams et al. (2024)"
#| results: 'asis'

williams_tiers |>
  mutate(across(c(Score, Lower_90, Upper_90), round, 2)) |>
  knitr::kable(
    col.names = c("Measure", "Score", "Lower 90%", "Upper 90%"),
    align = c("l","c","c","c")
  ) |>
  kableExtra::kable_styling(full_width = FALSE)



```




