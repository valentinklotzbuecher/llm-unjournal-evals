project:
  type: book
  output-dir: _book

book:
  title: "Comparing LLM and human reviews of social science research using data from Unjournal.org"
  author:
    - name: "Valentin Klotzbücher"
      orcid: 0000-0001-9382-6757
      email: valentin.klotzbuecher@usb.ch
      affiliation: "University of Basel & University Hospital Basel"
    # - name: "Lorenzo Pacchiardi"
    #   affiliation: "University of Cambridge"
    #   orcid: 0000-0003-4760-7638
    - name: "David Reinstein"
      affiliation: "The Unjournal"
      email: davidreinstein@unjournal.org
      orcid: 0000-0002-0470-4991
    - name: "Tianmai Michael Zhang"
      orcid: 0000-0001-5108-9250
      # email: tianmai@uw.edu
      affiliation: "University of Washington"
    # - name: "Casey Wimsatt"
  chapters:
    - index.qmd
    - methods.qmd #Puts papers data together, calls the LLM runs, has python code
    - results.qmd #Matches LLM output to Unjournal rating data, analysis of differences and performance, R code
    # - questions_answers.qmd
    - discussion.qmd
    - references.qmd
  appendices:
      - appendix_llm_traces.qmd
  #   - paper_response_analysis.qmd
  date: last-modified
  abstract: |
    We study how a frontier language model evaluates social‑science research compared to expert human reviewers in The Unjournal.
    Using the same structured rubric as human evaluators, we ask GPT‑5 Pro to rate papers on overall quality, methods, evidence, communication, openness, and global relevance, and to produce a narrative assessment anchored in the PDF of each paper.
    We first compare its quantitative ratings to aggregated human scores across The Unjournal’s existing evaluations, then take a closer qualitative look at a small set of focal papers, including a high‑profile mapping study of natural regeneration. For these focal cases, we examine where the model’s written review overlaps with and diverges from the human reports, and how both sides describe the main strengths, weaknesses, and policy relevance of the work. So far, the model reliably identifies many of the same methodological and interpretive issues that human experts emphasize, but it tends to translate these into more generous numerical ratings and narrower uncertainty intervals. We view this as an initial, work‑in‑progress probe of LLM‑based peer review in a high‑stakes, policy‑relevant domain, and as the first step toward a broader benchmark and set of tools for comparing and combining human and AI research evaluations.
  repo-url: https://github.com/valentinklotzbuecher/llm-unjournal-eval
  downloads: [pdf]

bibliography:
  - "references.bib"
  # - "grateful-refs.bib"
csl: "the-quarterly-journal-of-economics.csl"
link-citations: true


format:
  html:
    theme: [journal, theme-light.scss]
    # googlefonts:
    #   - Source Sans Pro:400,600
    highlight-style: atom-one
    toc: true
    number-sections: false
    citations-hover: true
    footnotes-hover: true
    embed-resources: true
    smooth-scroll: true
    code-fold: true
    code-tools: true
    code-summary: "Show code"
    code-overflow: wrap
    fig-cap-location: bottom
    fig-align: left
    lightbox: true
    df-print: paged
    execute:
      warning: false
      message: false
      freeze: auto
      cache: true
      python: "python3"
    # code-links:
    #   - icon: github
    #     href: https://github.com/valentinklotzbuecher/llm-unjournal-eval
    other-links:
      - icon: rocket
        href: https://www.unjournal.org/
        text: "The Unjournal"
  pdf:
    documentclass: scrreprt
    fontsize: 11pt
    geometry: margin=1in
    toc: true
    number-sections: true
    link-citations: true
    keep-tex: true
    fig-cap-location: top
    fig-align: left
    df-print: kable
    latex-engine: xelatex
    include-in-header: preamble.tex
    execute:
      warning: false
      message: false
      freeze: auto
      cache: true
      prefer-html: true
      echo: false
  # docx:
  #   toc: true
  #   number-sections: true
  #   link-citations: true
  #   execute:
  #     warning: false
  #     message: false
  #     freeze: auto
  #     cache: true
  #     prefer-html: true
  #     echo: false
